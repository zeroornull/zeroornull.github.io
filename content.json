{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"contact","text":"","link":"/contact/index.html"},{"title":"404","text":"","link":"/404/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Vuex学习","text":"Vuex学习","link":"/2021/04/02/Vuex%E5%AD%A6%E4%B9%A0/"},{"title":"改造eladmin为前后端一起部署","text":"前言 改造eladmin为前后端一起部署 2021-06-25 15:07:08 用了很多方法，不介绍失败的方法了，直接上成功的 首先改造前端1.更改.env.production 12VUE_APP_BASE_API = 'http://47.99.209.106:18061'VUE_APP_WS_API = 'ws://47.99.209.106:18061' 改成对应的后端接口 2.更改vue.config.js 1publicPath: process.env.NODE_ENV === 'development' ? '/' : './', 3.更改router.js 1mode: 'hash', 然后就是后端1、添加META-INF/resource/ 将前端打包好的dist放在下面 2、给SpringSecurityConfig添加图上四个 暂不确定是否都必须 注意部署后的项目web加载很慢 很奇怪","link":"/2021/06/25/%E6%94%B9%E9%80%A0eladmin%E4%B8%BA%E5%89%8D%E5%90%8E%E7%AB%AF%E4%B8%80%E8%B5%B7%E9%83%A8%E7%BD%B2/"},{"title":"使用cloudflare免费加速github page","text":"前言 使用cloudflare免费加速github page ​ 2021-07-26 14:00:16 使用cloudflare免费加速github page前言github page 在国内访问速度非常慢，而且近期 github.io 的域名经常被干扰解析成127.0.0.1，迫于无奈在网上找到了一个能白嫖加速 github page 的办法，就是套一层 cloudflare CDN，虽然它在国内没有 CDN 节点，但是整体效果是完爆 github.io，不过要注意的是免费版本是有请求次数限制的，每天 10W 次，当然这足够我的小博客使用了，这里记录一下操作步骤。 准备准备域名随便到哪买一个 国内好像得备案 设置 github page保存之后 github 会自动的在仓库根目录里生成一个CNAME文件，里面存储着域名配置信息 设置域名解析可以使用这个 https://zijian.aliyun.com/?spm=a2c1d.8251892.content.11.7c5c5b76F5cVb1#/domainDetect 通过域名提供商，修改刚刚的域名解析，通过 A 记录分别解析到以下 4 个 IP： 添加到自己的 域名解析那里 当记录全部解析生效时，就可以通过你自己的设置的域名访问到博客了，这个时候再开启HTTPS，示例图： 然后 github 会自动签发提供给你自己的设置的域名域名使用的 SSL 证书，等待一段时间后，就可以通过HTTPS访问博客了。 使用 cloudflare CDN上面的步骤全部就绪之后，就可以开始白嫖之路了 先通过https://dash.cloudflare.com/sign-up链接进行注册 添加站点，把对应的域名填写进去： 提交之后会自动扫描域名对应的解析记录： 查看 cloudfalre 对应的 NS 记录 通过域名的运营商修改对应的 NS 记录，这里每个运营商的修改方式都不一样，我这里是用的阿里云的： 这样就设置完毕了 可以看到 dns 解析的 ip 已经变了，已经被 cloudflare 接管了，然后清除下浏览器 DNS 缓存，chrome 浏览器输入chrome://net-internals/#dns进入清除页： 再次访问你自己的设置的域名，F12 打开网络面板可以看到已经用上了 CDN 了： 后记一直白嫖一直爽，但是cloudflare不一定一直会提供免费版的，如果有一天它挂了，只需要把 DNS 的 NS 解析记录再还原回去就行了。","link":"/2021/07/26/%E4%BD%BF%E7%94%A8cloudflare%E5%8A%A0%E9%80%9FgithubPage/"},{"title":"部署nginx二级目录","text":"前言 最近的nginx笔记 2021-06-25 13:07:08 项目为eladmin 前端 vue 主要的问题是前端更改配置 及部分nginx修改 参考链接: https://blog.csdn.net/qq_38319289/article/details/111867185 1.修改 router/router.js添加一行 1base: 'xinfuwu', 2、然后修改 vue.config.js更改一行 1publicPath: '/xinfuwu/', 3、部署时，通过NGINX的反向代理首先，给需要部署的项目定义一个 NGINX 的 server 1234567891011server { listen 4071; location / { #vue h5 history mode 时配置 try_files $uri $uri/ /xinfuwu_web/index.html; root html/xinfuwu_web; index index.html index.htm; } } 再到配置域名的主配置server上做反向代理 1234567891011121314location /xinfuwu/api/ { client_max_body_size 40M; proxy_pass http://127.0.0.1:10088/; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; } location ^~/xinfuwu/ { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:4071/; } 4.env.production也需要修改 baseapi修改为 ‘/xinfuwu/api’","link":"/2021/06/25/%E9%83%A8%E7%BD%B2nginx%E4%BA%8C%E7%BA%A7%E7%9B%AE%E5%BD%95/"},{"title":"linux韩顺平2021笔记","text":"前言 最近的linux学习笔记 linux韩顺平2021p1 课程内容基础篇Linux入门 vm和linux的安装 Linux目录结构 实际操作篇远程登陆（Xshell XFtp） 实用指令 进程管理 用户管理 Vi和Vim管理 定时任务调度 RPM和YUM 开机，重启和用户登陆注销 磁盘分区，挂载 网络配置 2021高级篇日志管理 Linux内核源码&amp;内核升级 定制自己的Linux Linux备份和恢复 Linux可视化管理webmin和bt运维工具 Linux入侵检测&amp;权限划分&amp;系统优化 Linux面试题（腾讯，百度，美团，滴滴 ） p2 应用领域p3 概述p4 Linux和Unixp5 vmware15.5安装下载链接：https://www.nocmd.com/windows/740.html p6 centOS7.6安装下载链接：http://mirrors.163.com/centos/7.6.1810/isos/x86_64/CentOS-7-x86_64-DVD-1810.iso linux分区：3个区 boot 引导分区 1g swap 交换分区 和内存大小一致2g 可以临时充当内存 根分区 17g p7 网络连接的三种方式桥接模式 会直接占用网段，会造成256个不够用 虚拟系统可以和外部系统通讯，但是容易造成IP冲突 NAT模式 网络地址转换模式 虚拟系统可以和外部系统通讯，而且不造成IP冲突 主机模式 不和外部通讯 svn checkout https://192.168.0.144:8443/svn/ntbyrck/ –username=0027xxp –password=0027xxp D:\\byrc p8 虚拟机克隆用于快速构建集群 方式一 直接拷贝一份安装好的虚拟机文件 方式二 使用vmware的克隆操作（需要先关闭linux系统） ​ 克隆方法 ​ 1.创建链接克隆（这只是引用） ​ 2.创建完整克隆（这个是拷贝） p9 虚拟机快照在进行一些不确定的操作时，用于恢复原先的某个状态，也叫快照管理 快照会占用一定空间 p10 虚拟机迁移和删除p11 vmtools安装后，在windows下更容易管理vm虚拟机，可以设置windows和centos的共享文件夹 1.进入centOS 2.点击vm菜单的-&gt;install vmware tools 3.centos会出现一个vm安装包，xx.tar.gz 4.拷贝到/opt 5.使用解压命令tar,得到一个安装文件 cd /opt tar -zxvf ./ 进行安装 可能会出现一些问题 参考链接 6.进入该vm解压的目录，/opt目录下 7.安装./vmware-install.pl 8.全部使用默认设置即可安装成功 9.注意：安装vmtools需要有gcc 主机的共享文件夹需要在vmware中设置 共享文件夹位置在/mnt/hgfs/ p12 第4章 linux目录结构linux采用层级树状结构，最上层根目录/ /root root用户的目录 /home 每创建一个用户都会出现一个用户的主目录 /bin 常用指令 环境设置 之类的文件 Binary /sbin s代表Super user的意思 /etc 系统管理所需要的配置文件和子目录 比如安装了mysql数据库 my.conf /boot 系统启动相关 核心文件，包括一些连接文件以及镜像文件 /dev 设备管理器 linux会把所有的硬件映射成一个文件管理 一切皆文件 /media 自动识别设备挂载到这个目录下 /lib 系统开机所需要的最基本的动态连接共享库，作用类似Windows里的DDL文件。几乎所有的应用程序都需要用到这些共享库 /lost+found 一般是空的，当系统非法关机后，这里就存放了一些文件 /usr 用户很多应用程序和文件都放在这个目录下，类似windows下的program files 目录 /proc 这个目录是一个虚拟的目录，它是系统内存的映射，访问这个目录来获取系统信息 /srv service缩写，存放一些服务启动之后需要提取的数据 /sys linux2.6内核很大的一个变化 。安装了新出现的文件系统 sysfs /tmp 存放临时文件 /mnt 为了让用户临时挂载别的文件系统，我们可以把外部存储挂载在/mnt/上，然后进入该目录就可以查看里面的内容了。d:/myshare /opt 这是主机额外安装软件（约定俗成）所摆放的目录。如安装Oracle数据库就可摆放在该目录下 挂载：例如将myshare文件夹挂载在/mnt/hgfs目录下 /usr/local 额外安装软件所安装的目录，一般通过编译源码的方式安装的程序 /var 这个目录存放着不断扩充着的东西，习惯将经常被修改的目录放在这个目录下。包括各种日志文件 /selinux[security-enhanced linux] SELinux是一种安全子系统，它能控制程序只能访问特定文件；三种工作模式，可以自行设置，需要启用 p13 第5章 远程登录到Linux服务器p14 远程登录xshell6 p15 远程文件传输xftp6 p16 vi和vim编辑器常用三种模式正常模式 插入模式 iIoOaArR 命令行模式 输入”esc” + “:” 或 “/“ 再输入:wq “wq”代表写入并退出 p17 vi和vim快捷键命令行模式输入 :wq(保存退出) :q(退出) :q!(强制退出，不保存) 拷贝当前行 yy 拷贝当前行向下五行 5yy 粘贴 p 删除当前行 dd 删除当前行向下五行 5dd 查找 / + 所需的字段 n键用来切换 :setnu 显示行号 :setnonu 关闭显示行号 文档最末行 G 最首行 gg 这些快捷键在一般模式下使用即可 指定行数 输入行号 + shift +g 撤销操作 一般模式下 按 u p18 vi vim 内容整理p19 第七章 开机、重启和用户注销shutdown -h now 立刻进行关机 shutdown -h 1 “hello,1分钟后会关机了” shutdown -r now 现在重新启动计算机 halt 关机，作用和上面一样 reboot 现在重启 sync 内存同步到磁盘 不论重启还是关闭系统，首先要运行sync指令，同步内存至磁盘 目前的shutdown/reboot/halt命令均已经在关机前进行了sync 建议还是先运行sync命令 p20 登录注销su - 用户名 为切换用户 logout在图形级界面运行级别是无效的 在运行级别3下有效 p21 用户管理添加用户useradd 用户名 1.创建用户成功后，会自动创建和用户名同名的home目录 2.也可以通过useradd -d 指定目录 新的用户名，给新创建的用户指定家目录 指定/修改密码 passwd 用户名（不写用户名会给当前登录的用户更改密码） 显示当前用户 pwd 删除用户但是不删除家目录 userdel 用户名 删除用户以及家目录 userdel -r 用户名 操作慎重 这样删除会把用户家目录所有内容删除 一般情况下建议保留家目录 p22 查询用户信息指令基本语法id 用户名 切换用户su - 切换用户名 权限高的用户切换到权限低的不需要输入密码，反之需要 返回到原来的用户 exit/logout 查看当前用户/登录用户基本语法whoami/Who am I p23 用户组介绍类似于角色，系统可对有共性的多个用户进行统一的管理 新增组指令：groupadd 组名 删除组groupdel 组名 增加用户时直接加上组useradd -g 用户组 用户名 修改用户的组usermod -g 用户组 用户名 用户和组相关文件/etc/password 文件用户的配置文件 用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录shell /etc/shadow 文件口令配置文件 登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间标志 /etc/group 文件 组配置文件 组名:口令:组标志号:组内用户列表 p24 用户管理总结p25 第九章 实用指令指定运行级别基本介绍 0：关机 1：单用户【找回丢失密码】 2：多用户状态没有网络服务 3：多用户状态有网络服务 4：系统未使用保留给用户 5：图形界面 6：系统重启 常用运行级别为3和5，也可以指定默认运行级别 init[0123456] 在centos7之前,/etc/inittab文件中指定 简化为 multi-user.target:analogous to runlevel 3 graphical.target:analagous to runlevel 5 当前运行级别 systemctl get-default systemctl set-default TARGET.target p26 如何找回root密码参考链接 p27 帮助指令man 命令或配置文件 Linux下,隐藏文件以.开头 选项可以组合使用 help 命令 p28 文件目录类pwd 指令显示当前工作目录绝对路径 ls 指令ls 目录或文件 常用选项 -a 所有 -l 列表 应用实例 查看当前目录所有内容信息 cd指令cd ~ 或者cd :回到自己的家目录 cd.. 回到当前目录的上一级目录 p28 文件目录类（2）mkdir指令创建目录 mkdir 要创建的目录 常见选项 -p：创建多级目录 案例一：创建一个目录 /home/dog mkdir /home/dog 案例二：创建一个多级目录 /home/animal/tiger mkdir -p /home/animal/tiger rmdir指令删除空目录 rmdir 要删除的空目录 案例：删除一个目录 /home/dog 细节注意：删除的是空目录，有内容则无法删除 如果要删除非空目录，需要使用 rm-rf 要删除的目录 例：rm -rf /home/animal touch 指令创建空文件 touch 文件名称 案例：创建一个空文件 hello.txt p30 文件目录指令（3）cp指令拷贝文件到指定目录 cp [选项] source dest 常用选项 -r：递归复制整个文件夹 cp hello.txt /home/bbb cp -r /home/bbb /opt/ \\cp 表示强制覆盖不提示 rm指令移除文件或目录 rm [选项] 要删除的文件或目录 常用选项： -r：递归删除整个文件夹 -f：强制删除不提示 案例一：将/home/hello.txt 删除，rm /home/hello.txt 案例二：递归删除整个文件夹 /home/bbb，rm -rf /home/bbb p31 文件目录指令（4）mv指令mv移动文件与目录或重命名 基本语法 mv oldNameFile newNameFile （功能描述：重命名） mv /temp/movefile /targetFolder (功能描述：移动文件) 实例 案例一：将/home/cat.txt 文件 重新命名为pig.txt 案例二：将/home/pig.txt 文件 移动到/root目录下 案例三：移动整个目录 cat指令cat 查看文件内容 基本用法 ​ cat [选项] 要查看的文件 常用选项 ​ -n：显示行号 cat只能浏览文件，而不能修改文件，为了浏览方便，一般会带上 管道命令| more 管道指的是 将前面得到的结果交给后面的指令来完成 more 指令基于vi编辑器的文本过滤器，全屏幕按页显示文本文件内容。more指令中内置了若干快捷键 基本语法： more 要查看的文件 less指令分屏查看文件内容，功能与more类似，但比more更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于大型文件具有更高的效率。 基本语法 less 要查看的文件 应用实例 案例: 采用less 查看一个大文件文件 /opt/杂文.txt less /opt/杂文.txt echo 指令echo 输出内容到控制台 基本语法 echo [选项] [输出内容] 应用实例 案例: 使用echo 指令输出环境变量, 比如输出 $PATH $HOSTNAME, echo $HOSTNAME 案例: 使用echo 指令输出hello,world! head 指令head 用于显示文件的开头部分内容，默认情况下head 指令显示文件的前10 行内容 基本语法 head 文件(功能描述：查看文件头10 行内容) head -n 5 文件(功能描述：查看文件头5 行内容，5 可以是任意行数) 应用实例 案例: 查看/etc/profile 的前面5 行代码 head -n 5 /etc/profile tail 指令tail 用于输出文件中尾部的内容，默认情况下tail 指令显示文件的前10 行内容。 基本语法 tail 文件（功能描述：查看文件尾10 行内容） tail -n 5 文件（功能描述：查看文件尾5 行内容，5 可以是任意行数） tail -f 文件（功能描述：实时追踪该文档的所有更新） 应用实例 案例1: 查看/etc/profile 最后5 行的代码 tail -n 5 /etc/profile 案例2: 实时监控mydate.txt , 看看到文件有变化时，是否看到， 实时的追加hello,world tail -f /home/mydate.txt &gt; 指令和&gt;&gt; 指令 &gt; 输出重定向（覆盖）和&gt;&gt; 追加 基本语法 ls -l &gt;文件（功能描述：列表的内容写入文件a.txt 中（覆盖写）） ls -al &gt;&gt;文件（功能描述：列表的内容追加到文件aa.txt 的末尾） cat 文件1 &gt; 文件2 （功能描述：将文件1 的内容覆盖到文件2） echo “内容”&gt;&gt; 文件(追加) 应用实例 案例1: 将/home 目录下的文件列表写入到/home/info.txt 中, 覆盖写入 ls -l /home &gt; /home/info.txt [如果info.txt 没有，则会创建] 案例2: 将当前日历信息追加到/home/mycal 文件中 指令为： cal &gt;&gt; /home/mycal ln 指令link 软链接也称为符号链接，类似于windows 里的快捷方式，主要存放了链接其他文件的路径 基本语法 ln -s [原文件或目录] [软链接名] （功能描述：给原文件创建一个软链接） 应用实例 案例1: 在/home 目录下创建一个软连接myroot，连接到/root 目录 history 指令查看已经执行过历史命令,也可以执行历史指令 基本语法 history （功能描述：查看已经执行过历史命令） 应用实例 案例1: 显示所有的历史命令 history 案例2: 显示最近使用过的10 个指令。 history 10 案例3：执行历史编号为5 的指令 !5 p34 时间日期类时间日期类date 指令-显示当前日期基本语法 date （功能描述：显示当前时间） date +%Y （功能描述：显示当前年份） date +%m（功能描述：显示当前月份） date +%d （功能描述：显示当前是哪一天） date “+%Y-%m-%d %H:%M:%S”（功能描述：显示年月日时分秒） 应用实例案例1: 显示当前时间信息date案例2: 显示当前时间年月日date “+%Y-%m-%d” 案例3: 显示当前时间年月日时分秒date “+%Y-%m-%d %H:%M:%S” date 指令-设置日期基本语法date -s 字符串时间 应用实例案例1: 设置系统当前时间， 比如设置成2020-11-03 20:02:10date -s “2020-11-03 20:02:10” cal 指令查看日历指令cal 基本语法cal [选项] （功能描述：不加选项，显示本月日历） 应用实例案例1: 显示当前日历cal案例2: 显示2020 年日历: cal 2020 p35 查找指令（1）搜索查找类find指令find 指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端。 基本语法find [搜索范围] [选项] 选项说明 应用实例案例1: 按文件名：根据名称查找/home 目录下的hello.txt 文件find /home -name hello.txt案例2：按拥有者：查找/opt 目录下，用户名称为nobody 的文件find /opt -user nobody案例3：查找整个linux 系统下大于200M 的文件（+n 大于-n 小于n 等于, 单位有k,M,G）find / -size +200M ls -lh h表示大小用k，m之类表示 locate 指令locate 指令可以快速定位文件路径。locate 指令利用事先建立的系统中所有文件名称及路径的locate 数据库实现快速定位给定的文件。Locate 指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate 时刻 基本语法locate 搜索文件 特别说明由于locate 指令基于数据库进行查询，所以第一次运行前，必须使用updatedb 指令创建locate 数据库。 应用实例案例1: 请使用locate 指令快速定位hello.txt 文件所在目录which 指令，可以查看某个指令在哪个目录下，比如ls 指令在哪个目录which ls p36 查找指令（2）grep 指令和管道符号 |grep 过滤查找， 管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理。 基本语法grep [选项] 查找内容源文件 常用选项 应用实例案例1: 请在hello.txt 文件中，查找”yes” 所在行，并且显示行号写法1: cat /home/hello.txt | grep “yes”写法2: grep -n “yes” /home/hello.txt p37 压缩和解压类gzip/gunzip 指令gzip 用于压缩文件， gunzip 用于解压的 基本语法gzip 文件（功能描述：压缩文件，只能将文件压缩为*.gz 文件）gunzip 文件.gz （功能描述：解压缩文件命令） 应用实例案例1: gzip 压缩， 将/home 下的hello.txt 文件进行压缩gzip /home/hello.txt案例2: gunzip 压缩， 将/home 下的hello.txt.gz 文件进行解压缩gunzip /home/hello.txt.gz zip/unzip 指令zip 用于压缩文件， unzip 用于解压的，这个在项目打包发布中很有用的 基本语法zip [选项] XXX.zip 将要压缩的内容（功能描述：压缩文件和目录的命令）unzip [选项] XXX.zip （功能描述：解压缩文件） zip 常用选项-r：递归压缩，即压缩目录 unzip 的常用选项-d&lt;目录&gt; ：指定解压后文件的存放目录 p38 压缩和解压类（2）tar 指令tar 指令是打包指令，最后打包后的文件是.tar.gz 的文件。 基本语法tar [选项] XXX.tar.gz 打包的内容(功能描述：打包目录，压缩后的文件格式.tar.gz) 选项说明 应用实例案例1: 压缩多个文件，将/home/pig.txt 和/home/cat.txt 压缩成pc.tar.gztar -zcvf pc.tar.gz /home/pig.txt /home/cat.txt案例2: 将/home 的文件夹压缩成myhome.tar.gztar -zcvf myhome.tar.gz /home/案例3: 将pc.tar.gz 解压到当前目录tar -zxvf pc.tar.gz案例4: 将myhome.tar.gz 解压到/opt/tmp2 目录下(1) mkdir /opt/tmp2 (2) tar -zxvf /home/myhome.tar.gz -C /opt/tmp2 p39 实用指令小结p40 第十章 Linux组的介绍Linux 组基本介绍在linux 中的每个用户必须属于一个组，不能独立于组外。在linux 中每个文件有所有者、所在组、其它组的概念。 所有者 所在组 其它组 改变用户所在的组 p41 所有者文件/目录所有者一般为文件的创建者,谁创建了该文件，就自然的成为该文件的所有者。 查看文件的所有者指令：ls –ahl应用实例 修改文件所有者（change owner） 指令：chown 用户名 文件名应用案例要求：使用root 创建一个文件apple.txt ，然后将其所有者修改成tomchown tom apple.txt 组的创建基本指令groupadd 组名 应用实例创建一个组, ,monstergroupadd monster 创建一个用户fox ，并放入到monster 组中useradd -g monster fox p42 所在组文件/目录所在组当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组(默认)。 查看文件/目录所在组基本指令ls –ahl应用实例, 使用fox 来创建一个文件，看看该文件属于哪个组? -rw-r–r–. 1 fox monster 0 12月 30 15:08 ok.txt 修改文件/目录所在的组基本指令chgrp 组名文件名 应用实例使用root 用户创建文件orange.txt ,看看当前这个文件属于哪个组，然后将这个文件所在组，修改到fruit 组。 groupadd fruit touch orange.txt 看看当前这个文件属于哪个组-&gt; root 组 chgrp fruit orange.txt p43 修改所在组其它组除文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 改变用户所在组在添加用户时，可以指定将该用户添加到哪个组中，同样的用root 的管理权限可以改变某个用户所在的组。 改变用户所在组usermod –g 新组名用户名usermod –d 目录名用户名改变该用户登陆的初始目录。特别说明：用户需要有进入到新目录的权限。 应用实例将zwj 这个用户从原来所在组，修改到wudang 组usermod -g wudang zwj p44 rwx权限权限的基本介绍ls -l 中显示的内容如下：-rwxrw-r– 1 root root 1213 Feb 2 09:39 abc 0-9 位说明第0 位确定文件类型(d, - , l , c , b)-代表是一个普通文件l 是链接，相当于windows 的快捷方式 linkd 是目录，相当于windows 的文件夹c 是字符设备文件，鼠标，键盘b 是块设备，比如硬盘第1-3 位确定所有者（该文件的所有者）拥有该文件的权限。—User第4-6 位确定所属组（同用户组的）拥有该文件的权限，—Group第7-9 位确定其他用户拥有该文件的权限—Other rwx 权限详解，难点rwx 作用到文件 [ r ]代表可读(read): 可以读取,查看 [ w ]代表可写(write): 可以修改,但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被执行 rwx 作用到目录 [ r ]代表可读(read): 可以读取，ls 查看目录内容 [ w ]代表可写(write): 可以修改, 对目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 p45 权限说明案例ls -l 中显示的内容如下：-rwxrw-r– 1 root root 1213 Feb 2 09:39 abc 10 个字符确定不同用户能对文件干什么第一个字符代表文件类型： - l d c b其余字符每3 个一组(rwx) 读(r) 写(w) 执行(x)第一组rwx : 文件拥有者的权限是读、写和执行第二组rw- : 与文件拥有者同一组的用户的权限是读、写但不能执行第三组r– : 不与文件拥有者同组的其他用户的权限是读不能写和执行 可用数字表示为: r=4,w=2,x=1 因此rwx=4+2+1=7 , 数字可以进行组合 其它说明1 文件：硬连接数或目录：子目录数root 用户root 组1213 文件大小(字节)，如果是文件夹，显示4096 字节Feb 2 09:39 最后修改日期abc 文件名 p46 修改权限修改权限-chmod基本说明：通过chmod 指令，可以修改文件或者目录的权限。 第一种方式：+ 、-、= 变更权限u:所有者g:所有组o:其他人a:所有人(u、g、o 的总和) chmod u=rwx,g=rx,o=x 文件/目录名 chmod o+w 文件/目录名 chmod a-x 文件/目录名 案例演示 给abc 文件的所有者读写执行的权限，给所在组读执行权限，给其它组读执行权限。chmod u=rwx,g=rx,o=rx abc 给abc 文件的所有者除去执行的权限，增加组写的权限chmod u-x,g+w abc 给abc 文件的所有用户添加读的权限chmod a+r abc 第二种方式：通过数字变更权限r=4 w=2 x=1 rwx=4+2+1=7chmod u=rwx,g=rx,o=x 文件目录名相当于chmod 751 文件/目录名 案例演示要求：将/home/abc.txt 文件的权限修改成rwxr-xr-x, 使用给数字的方式实现： chmod 755 /home/abc.txt p47 修改所在组和所有者基本介绍chown newowner 文件/目录改变所有者chown newowner:newgroup 文件/目录改变所有者和所在组 -R 如果是目录则使其下所有子文件或目录递归生效 案例演示 请将/home/abc.txt 文件的所有者修改成tomchown tom /home/abc.txt 请将/home/test 目录下所有的文件和目录的所有者都修改成tomchown -R tom /home/test 修改文件/目录所在组-chgrp基本介绍chgrp newgroup 文件/目录【改变所在组】 案例演示请将/home/abc .txt 文件的所在组修改成shaolin (少林)groupadd shaolinchgrp shaolin /home/abc.txt请将/home/test 目录下所有的文件和目录的所在组都修改成shaolin(少林)chgrp -R shaolin /home/test p48 最佳实践-警察和土匪游戏police ， banditjack, jerry: 警察xh, xq: 土匪 创建组groupadd police ; groupadd bandit 创建用户useradd -g police jack ; useradd -g police jerryuseradd -g bandit xh; useradd -g bandit xq jack 创建一个文件，自己可以读r 写w，本组人可以读，其它组没人任何权限首先jack 登录； vim jack.txt ; chmod 640 jack.txt jack 修改该文件，让其它组人可以读, 本组人可以读写chmod o=r,g=r jack.txt xh 投靠警察，看看是否可以读写.usermod -g police xh 测试，看看xh 是否可以读写，xq 是否可以, 小结论，就是如果要对目录内的文件进行操作，需要要有对该目录的相应权限 p49文件权限管理[课堂练习1] 建立两个组（神仙(sx),妖怪(yg)） 建立四个用户(唐僧,悟空，八戒，沙僧) 设置密码 把悟空，八戒放入妖怪唐僧沙僧在神仙 用悟空建立一个文件（monkey.java 该文件要输出i am monkey） 给八戒一个可以r w 的权限 八戒修改monkey.java 加入一句话( i am pig) 唐僧沙僧对该文件没有权限 把沙僧放入妖怪组 让沙僧修改该文件monkey, 加入一句话(“我是沙僧，我是妖怪!”); 对文件夹rwx 的细节讨论和测试!!!x: 表示可以进入到该目录, 比如cdr: 表示可以ls , 将目录的内容显示w: 表示可以在该目录，删除或者创建文件 示意图 课堂练习2，完成如下操作 用root 登录，建立用户mycentos,自己设定密码 用mycentos 登录，在主目录下建立目录test/t11/t1 在t1 中建立一个文本文件aa,用vi 编辑其内容为ls –al 改变aa 的权限为可执行文件[可以将当前日期追加到一个文件],运行该文件./aa 删除新建立的目录test/t11/t1 删除用户mycentos 及其主目录中的内容 将linux 设置成进入到图形界面的 重新启动linux 或关机 p50 第10章总结p51 第11章 定时任务调度crond 任务调度crontab 进行 定时任务的设置 概述任务调度：是指系统在某个时间执行的特定的命令或程序。任务调度分类：1.系统工作：有些重要的工作必须周而复始地执行。如病毒扫描等个别用户工作：个别用户可能希望执行某些程序，比如对mysql 数据库的备份。示意图 p52 crontab基本语法crontab [选项] 常用选项 快速入门设置任务调度文件：/etc/crontab设置个人任务调度。执行crontab –e 命令。接着输入任务到调度文件如：*/1 * * * * ls –l /etc/ &gt; /tmp/to.txt意思说每小时的每分钟执行ls –l /etc/ &gt; /tmp/to.txt 命令 参数细节说明cron表达式 5 个占位符的说明 p53 crond 时间规则特殊时间执行案例 每天的凌晨4点，每10分钟的时间段为 4-5点之内 p54 crond应用实例案例1：每隔1 分钟，就将当前的日期信息，追加到/tmp/mydate 文件中 */1 * * * * date &gt;&gt; /tmp/mydate 案例2：每隔1 分钟， 将当前日期和日历都追加到/home/mycal 文件中步骤: (1) vim /home/my.sh 写入内容date &gt;&gt; /home/mycal 和cal &gt;&gt; /home/mycal(2) 给my.sh 增加执行权限，chmod u+x /home/my.sh(3) crontab -e 增加*/1 * * * * /home/my.sh crond 相关指令conrtab –r：终止任务调度。其实就是删除crondtab -e 中的任务crontab –l：列出当前有那些任务调度service crond restart [重启任务调度] p55 at定时任务基本介绍 at 命令是一次性定时计划任务，at 的守护进程atd 会以后台模式运行，检查作业队列来运行。 默认情况下，atd 守护进程每60 秒检查作业队列（任务队列），有作业时，会检查作业运行时间，如果时间与当前时间匹配，则运行此作业。 at 命令是一次性定时计划任务，执行完一个任务后不再执行此任务了 在使用at 命令的时候，一定要保证atd 进程的启动, 可以使用相关指令来查看ps -ef | grep atd //可以检测atd 是否在运行 ps -ef 检测现在有哪些进程在运行 | grep 过滤 画一个示意图 at 命令格式at [选项] [时间]Ctrl + D 结束at 命令的输入， 输出两次 at 命令选项 at 时间定义at 指定时间的方法： 接受在当天的hh:mm（小时:分钟）式的时间指定。假如该时间已过去，那么就放在第二天执行。例如：04:00 使用midnight（深夜），noon（中午），teatime（饮茶时间，一般是下午4 点）等比较模糊的词语来指定时间。 采用12 小时计时制，即在时间后面加上AM（上午）或PM（下午）来说明是上午还是下午。例如：12pm 指定命令执行的具体日期，指定格式为month day（月日）或mm/dd/yy（月/日/年）或dd.mm.yy（日.月.年），指定的日期必须跟在指定时间的后面。例如：04:00 2021-03-1 使用相对计时法。指定格式为：now + count time-units ，now 就是当前时间，time-units 是时间单位，这里能够是minutes（分钟）、hours（小时）、days（天）、weeks（星期）。count 是时间的数量，几天，几小时。例如：now + 5 minutes 直接使用today（今天）、tomorrow（明天）来指定完成命令的时间。 p56 at任务调度实例案例1：2 天后的下午5 点执行/bin/ls /home 案例2：atq 命令来查看系统中没有执行的工作任务案例3：明天17 点钟，输出时间到指定文件内比如/root/date100.log 案例4：2 分钟后，输出时间到指定文件内比如/root/date200.log 案例5：删除已经设置的任务, atrm 编号atrm 4 //表示将job 队列，编号为4 的job 删除. 默认删除键变^H，只要按住ctrl键，删除键就可以使用了~p57 任务调度小结p58 磁盘分区机制Linux 分区原理介绍 Linux 来说无论有几个分区，分给哪一目录使用，它归根结底就只有一个根目录，一个独立且唯一的文件结构, Linux中每个分区都是用来组成整个文件系统的一部分。 Linux 采用了一种叫“载入”的处理方法，它的整个文件系统中包含了一整套的文件和目录，且将一个分区和一个目录联系起来。这时要载入的一个分区将使它的存储空间在一个目录下获得。 示意图 硬盘说明 Linux 硬盘分IDE 硬盘和SCSI 硬盘，目前基本上是SCSI 硬盘 对于IDE 硬盘，驱动器标识符为“hdx”,其中“hd”表明分区所在设备的类型，这里是指IDE 硬盘了。“x”为盘号（a 为基本盘，b 为基本从属盘，c 为辅助主盘，d 为辅助从属盘）,“”代表分区，前四个分区用数字1 到4 表示，它们是主分区或扩展分区，从5 开始就是逻辑分区。例，hda3 表示为第一个IDE 硬盘上的第三个主分区或扩展分区,hdb2表示为第二个IDE 硬盘上的第二个主分区或扩展分区。 对于SCSI 硬盘则标识为“sdx~”，SCSI 硬盘是用“sd”来表示分区所在设备的类型的，其余则和IDE 硬盘的表示方法一样 查看所有设备挂载情况命令：lsblk 或者lsblk -f p59 增加磁盘应用实例挂载的经典案例说明：下面我们以增加一块硬盘为例来熟悉下磁盘的相关指令和深入理解磁盘分区、挂载、卸载的概念。 如何增加一块硬盘 虚拟机添加硬盘 分区 格式化 挂载 设置可以自动挂载 虚拟机增加硬盘步骤1在【虚拟机】菜单中，选择【设置】，然后设备列表里添加硬盘，然后一路【下一步】，中间只有选择磁盘大小的地方需要修改，至到完成。然后重启系统（才能识别）！ 虚拟机增加硬盘步骤2分区命令fdisk /dev/sdb 开始对/sdb 分区m 显示命令列表p 显示磁盘分区同fdisk –ln 新增分区d 删除分区w 写入并退出 说明： 开始分区后输入n，新增分区，然后选择p ，分区类型为主分区。两次回车默认剩余全部空间。最后输入w写入分区并退出，若不保存退出输入q。 虚拟机增加硬盘步骤3格式化磁盘分区命令:mkfs -t ext4 /dev/sdb1其中ext4 是分区类型 虚拟机增加硬盘步骤4挂载: 将一个分区与一个目录联系起来，mount 设备名称挂载目录例如： mount /dev/sdb1 /newdisk umount 设备名称或者挂载目录 例如： umount /dev/sdb1 或者umount /newdisk 老师注意: 用命令行挂载,重启后会失效 问题：1.能否在一个目录下挂载多个分区 不能，只能挂载一个 如果切换挂载 已经写入的文件位置仍然不变 虚拟机增加硬盘步骤5永久挂载: 通过修改/etc/fstab 实现挂载添加完成后执行mount –a 即刻生效 p60 磁盘情况查询查询系统整体磁盘使用情况基本语法df -h 应用实例查询系统整体磁盘使用情况 查询指定目录的磁盘占用情况基本语法du -h 查询指定目录的磁盘占用情况，默认为当前目录-s 指定目录占用大小汇总-h 带计量单位-a 含文件–max-depth=1 子目录深度-c 列出明细的同时，增加汇总值 应用实例查询/opt 目录的磁盘占用情况，深度为1 p61 磁盘情况-工作实用指令 统计/opt 文件夹下文件的个数ls -l /opt | grep “^-“ | wc -l 统计/opt 文件夹下目录的个数ls -l /opt | grep “^d” | wc -l 统计/opt 文件夹下文件的个数，包括子文件夹里的ls -lR /opt | grep “^-“ | wc -l 统计/opt 文件夹下目录的个数，包括子文件夹里的ls -lR /opt | grep “^d” | wc -l 以树状显示目录结构tree 目录， 注意，如果没有tree ,则使用yum install tree 安装 p62 磁盘挂载小结p63 NAT网络原理图 p64 网络配置指令查看网络IP 和网关ip自动分配与指定ip 查看网关 查看windows 环境的中VMnet8 网络配置(ipconfig 指令) 查看linux 的网络配置ifconfig ping 测试主机之间网络连通性基本语法ping 目的主机（功能描述：测试当前服务器是否可以连接目的主机） 应用实例测试当前服务器是否可以连接百度ping www.baidu.com p65 网络配置实例linux 网络环境配置第一种方法(自动获取)：说明：登陆后，通过界面的来设置自动获取ip，特点：linux 启动后会自动获取IP,缺点是每次自动获取的ip 地址可能不一样 第二种方法(指定ip)说明直接修改配置文件来指定IP,并可以连接到外网(程序员推荐) 编辑vi /etc/sysconfig/network-scripts/ifcfg-ens33要求：将ip 地址配置的静态的，比如: ip 地址为192.168.200.130 ifcfg-ens33 文件说明DEVICE=eth0 #接口名（设备,网卡） HWADDR=00:0C:2x:6x:0x:xx #MAC 地址 TYPE=Ethernet #网络类型（通常是Ethemet） UUID=926a57ba-92c6-4231-bacb-f27e5e6a9f44 #随机id #系统启动的时候网络接口是否有效（yes/no）ONBOOT=yes #IP 的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP 协议|DHCP 协议）BOOTPROTO=static#IP 地址IPADDR=192.168.200.130#网关GATEWAY=192.168.200.2#域名解析器DNS1=192.168.200.2重启网络服务或者重启系统生效service network restart 、reboot p66 主机名和hosts映射设置主机名 为了方便记忆，可以给linux 系统设置主机名, 也可以根据需要修改主机名 指令hostname ： 查看主机名 修改文件在/etc/hostname 指定 修改后，重启生效 设置hosts 映射思考：如何通过主机名能够找到(比如ping) 某个linux 系统? windows在C:\\Windows\\System32\\drivers\\etc\\hosts 文件指定即可 win10无法修改host文件参考 案例: 192.168.200.130 hspedu100 linux在/etc/hosts 文件指定 案例: 192.168.200.1 ThinkPad-PC 主机名解析过程分析(Hosts、DNS)Hosts 是什么一个文本文件，用来记录IP 和Hostname(主机名)的映射关系 DNSDNS，就是Domain Name System 的缩写，翻译过来就是域名系统是互联网上作为域名和IP 地址相互映射的一个分布式数据库 应用实例: 用户在浏览器输入了www.baidu.com 浏览器先检查浏览器缓存中有没有该域名解析IP 地址，有就先调用这个IP 完成解析；如果没有，就检查DNS 解析器缓存，如果有直接返回IP 完成解析。这两个缓存，可以理解为本地解析器缓存 一般来说，当电脑第一次成功访问某一网站后，在一定时间内，浏览器或操作系统会缓存他的IP 地址（DNS 解析记录）.如在cmd 窗口中输入ipconfig /displaydns //DNS 域名解析缓存ipconfig /flushdns //手动清理dns 缓存 如果本地解析器缓存没有找到对应映射，检查系统中hosts 文件中有没有配置对应的域名IP 映射，如果有，则完成解析并返回。 如果本地DNS 解析器缓存和hosts 文件中均没有找到对应的IP，则到域名服务DNS 进行解析域 示意图 p67 网络配置小结p68 进程基本介绍 在LINUX 中，每个执行的程序都称为一个进程。每一个进程都分配一个ID 号(pid,进程号)。=&gt;windows =&gt; linux 每个进程都可能以两种方式存在的。前台与后台，所谓前台进程就是用户目前的屏幕上可以进行操作的。后台进程则是实际在操作，但由于屏幕上无法看到的进程，通常使用后台方式执行。 一般系统的服务都是以后台进程的方式存在，而且都会常驻在系统中。直到关机才才结束。 示意图 p69 ps指令详解显示系统执行的进程基本介绍ps 命令是用来查看目前系统中，有哪些正在执行，以及它们执行的状况。可以不加任何参数. ps 详解 指令：ps –aux|grep xxx ，比如我看看有没有sshd 服务 指令说明 System V 展示风格 USER：用户名称 PID：进程号 %CPU：进程占用CPU 的百分比 %MEM：进程占用物理内存的百分比 VSZ：进程占用的虚拟内存大小（单位：KB） RSS：进程占用的物理内存大小（单位：KB） TT：终端名称,缩写. STAT：进程状态，其中S-睡眠，s-表示该进程是会话的先导进程，N-表示进程拥有比普通优先级更 低的优先级，R- 正在运行，D-短期等待，Z-僵死进程，T-被跟踪或者被停止等等 STARTED：进程的启动时间 TIME：CPU 时间，即进程使用CPU 的总时间 COMMAND：启动进程所用的命令和参数，如果过长会被截断显示 p70 父子进程应用实例要求：以全格式显示当前所有的进程，查看进程的父进程。查看sshd 的父进程信息ps -ef 是以全格式显示当前所有的进程-e 显示所有进程。-f 全格式ps -ef|grep sshd 是BSD 风格UID：用户IDPID：进程IDPPID：父进程IDC：CPU 用于计算执行优先级的因子。数值越大，表明进程是CPU 密集型运算，执行优先级会降低；数值越小，表明进程是I/O 密集型运算，执行优先级会提高STIME：进程启动的时间TTY：完整的终端名称TIME：CPU 时间CMD：启动进程所用的命令和参数 p71 终止进程kill 和killall介绍:若是某个进程执行一半需要停止时，或是已消了很大的系统资源时，此时可以考虑停止该进程。使用kill 命令来完成此项任务。 基本语法kill [选项] 进程号（功能描述：通过进程号杀死/终止进程）killall 进程名称（功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用） 常用选项-9 :表示强迫进程立即停止 最佳实践 案例1：踢掉某个非法登录用户kill 进程号, 比如kill 11421 案例2: 终止远程登录服务sshd, 在适当时候再次重启sshd 服务kill sshd 对应的进程号; /bin/systemctl start sshd.service 案例3: 终止多个gedit , 演示killall gedit 案例4：强制杀掉一个终端, 指令kill -9 bash 对应的进程号 p72 查看进程树14.4.1 基本语法pstree [选项] ,可以更加直观的来看进程信息 14.4.2 常用选项-p :显示进程的PID-u :显示进程的所属用户 14.4.3 应用实例： 案例1：请你树状的形式显示进程的pidpstree -p 案例2：请你树状的形式进程的用户pstree -u p73 服务管理14.5.1 介绍:服务(service) 本质就是进程，但是是运行在后台的，通常都会监听某个端口，等待其它程序的请求，比如(mysqld , sshd防火墙等)，因此我们又称为守护进程，是Linux 中非常重要的知识点。【原理图】 14.5.2 service 管理指令 service 服务名[start | stop | restart | reload | status] 在CentOS7.0 后很多服务不再使用 service ,而是 systemctl (后面专门讲) service 指令管理的服务在/etc/init.d 查看 service 管理指令案例请使用service 指令，查看，关闭，启动network [注意：在虚拟系统演示，因为网络连接会关闭]指令:service network statusservice network stopservice network start 14.5.4 查看服务名:方式1：使用setup -&gt; 系统服务就可以看到全部。setup 按tab会进入图形化界面的下面的菜单 ，利于退出 方式2: /etc/init.d 看到service 指令管理的服务ls -l /etc/init.d p74 服务管理（2）14.5.5 服务的运行级别(runlevel):Linux 系统有7 种运行级别(runlevel)：常用的是级别3 和5运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root 权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)，不支持网络运行级别3：完全的多用户状态(有NFS)，无界面，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11 控制台，登陆后进入图形GUI 模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动开机的流程说明： p75 服务管理（3）14.5.7 chkconfig 指令介绍通过chkconfig 命令可以给服务的各个运行级别设置自启动/关闭chkconfig 指令管理的服务在/etc/init.d 查看注意: Centos7.0 后，很多服务使用systemctl 管理(后面马上讲) chkconfig 基本语法 查看服务chkconfig –list [| grep xxx] chkconfig 服务名–list chkconfig –level 5 服务名on/off 案例演示: 对network 服务进行各种操作, 把network 在3 运行级别,关闭自启动chkconfig –level 3 network offchkconfig –level 3 network on 使用细节chkconfig 重新设置服务后自启动或关闭，需要重启机器reboot 生效. p76 服务管理（4）14.5.8 systemctl 管理指令基本语法： systemctl [start | stop | restart | status] 服务名systemctl 指令管理的服务在/usr/lib/systemd/system 查看 14.5.9 systemctl 设置服务的自启动状态systemctl list-unit-files [ | grep 服务名] (查看服务开机启动状态, grep 可以进行过滤)systemctl enable 服务名(设置服务开机启动)systemctl disable 服务名(关闭服务开机启动)systemctl is-enabled 服务名(查询某个服务是否是自启动的) 14.5.10 应用案例：查看当前防火墙的状况，关闭防火墙和重启防火墙。=&gt; firewalld.servicesystemctl status firewalld; systemctl stop firewalld; systemctl start firewalld 14.5.11 细节讨论：关闭或者启用防火墙后，立即生效。[telnet 测试某个端口即可]这种方式只是临时生效，当重启系统后，还是回归以前对服务的设置。如果希望设置某个服务自启动或关闭永久生效，要使用systemctl [enable|disable] 服务名. [演示] p77 服务管理（5）14.5.12 打开或者关闭指定端口在真正的生产环境，往往需要将防火墙打开，但问题来了，如果我们把防火墙打开，那么外部请求数据包就不能跟服务器监听端口通讯。这时，需要打开指定的端口。比如80、22、8080 等，这个又怎么做呢？老韩给给大家讲一讲。[示意图] 14.5.13 firewall 指令 打开端口: firewall-cmd –permanent –add-port=端口号/协议 关闭端口: firewall-cmd –permanent –remove-port=端口号/协议 重新载入,才能生效: firewall-cmd –reload 查询端口是否开放: firewall-cmd –query-port=端口/协议 14.5.14 应用案例： 启用防火墙， 测试111 端口是否能telnet , 不行 开放111 端口firewall-cmd –permanent –add-port=111/tcp ; 需要firewall-cmd –reload 再次关闭111 端口firewall-cmd –permanent –remove-port=111/tcp ; 需要firewall-cmd –reload p78 动态监控进程介绍：top 与ps 命令很相似。它们都用来显示正在执行的进程。Top 与ps 最大的不同之处，在于top 在执行一段时间可以更新正在运行的的进程。 14.6.2 基本语法top [选项] 14.6.3 选项说明： p79 交互操作说明 14.6.5 应用实例 案例1.监视特定用户, 比如我们监控tom 用户top：输入此命令，按回车键，查看执行的进程。u：然后输入“u”回车，再输入用户名，即可, 案例2：终止指定的进程, 比如我们要结束tom 登录top：输入此命令，按回车键，查看执行的进程。k：然后输入“k”回车，再输入要结束的进程ID 号 案例3:指定系统状态更新的时间(每隔10 秒自动更新), 默认是3 秒top -d 10 p80 监控网络状态14.7.1 查看系统网络情况netstat基本语法netstat [选项] 选项说明-an 按一定顺序排列输出-p 显示哪个进程在调用 应用案例请查看服务名为sshd 的服务的信息。netstat -anp | grep sshd 14.7.2 检测主机连接命令ping：是一种网络检测工具，它主要是用检测远程主机是否正常，或是两部主机间的网线或网卡故障。如: ping 对方ip 地址 p81 进程管理小结p82 rpm管理（1）15.1 rpm 包的管理15.1.1 介绍rpm 用于互联网下载包的打包及安装工具，它包含在某些Linux 分发版中。它生成具有.RPM 扩展名的文件。RPM是RedHat Package Manager（RedHat 软件包管理工具）的缩写，类似windows 的setup.exe，这一文件格式名称虽然打上了RedHat 的标志，但理念是通用的。Linux 的分发版本都有采用（suse,redhat, centos 等等），可以算是公认的行业标准了。 15.1.2 rpm 包的简单查询指令查询已安装的rpm 列表rpm –qa|grep xx举例： 看看当前系统，是否安装了firefox指令: rpm -qa | grep firefox 15.1.3 rpm 包名基本格式一个rpm 包名：firefox-60.2.2-1.el7.centos.x86_64名称:firefox版本号：60.2.2-1适用操作系统: el7.centos.x86_64表示centos7.x 的64 位系统如果是i686、i386 表示32 位系统，noarch 表示通用 15.1.4 rpm 包的其它查询指令：rpm -qa :查询所安装的所有rpm 软件包rpm -qa | morerpm -qa | grep X [rpm -qa | grep firefox ] rpm -q 软件包名:查询软件包是否安装案例：rpm -q firefoxrpm -qi 软件包名：查询软件包信息案例: rpm -qi firefoxrpm -ql 软件包名:查询软件包中的文件比如： rpm -ql firefoxrpm -qf 文件全路径名查询文件所属的软件包rpm -qf /etc/passwdrpm -qf /root/install.log p83 rpm的卸载15.1.5 卸载rpm 包：基本语法rpm -e RPM 包的名称//erase 应用案例删除firefox 软件包rpm -e firefox 细节讨论 如果其它软件包依赖于您要卸载的软件包，卸载时则会产生错误信息。如： $ rpm -e fooremoving these packages would break dependencies:foo is needed by bar-1.0-1 如果我们就是要删除foo 这个rpm 包，可以增加参数–nodeps ,就可以强制删除，但是一般不推荐这样做，因为依赖于该软件包的程序可能无法运行如：$ rpm -e –nodeps foo 15.1.6 安装rpm 包基本语法rpm -ivh RPM 包全路径名称 参数说明i=install 安装v=verbose 提示h=hash 进度条 应用实例演示卸载和安装firefox 浏览器rpm -e firefoxrpm -ivh firefox 😢 2021年2月20日10点59分 p84 yum15.2.1 介绍：Yum 是一个Shell 前端软件包管理器。基于RPM 包管理，能够从指定的服务器自动下载RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包。示意图 15.2.2 yum 的基本指令查询yum 服务器是否有需要安装的软件yum list|grep xx 软件列表 15.2.3 安装指定的yum 包yum install xxx 下载安装 15.2.4 yum 应用实例：案例：请使用yum 的方式来安装firefoxrpm -e firefoxyum list | grep firefoxyum install firefox p85 软件包管理小结p86 安装配置JDK1.816.1 概述如果需要在Linux 下进行JavaEE 的开发，我们需要安装如下软件 16.2 安装JDK16.2.1 安装步骤 mkdir /opt/jdk 通过xftp6 上传到/opt/jdk 下 cd /opt/jdk 解压tar -zxvf jdk-8u261-linux-x64.tar.gz mkdir /usr/local/java mv /opt/jdk/jdk1.8.0_261 /usr/local/java 配置环境变量的配置文件vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_261 export PATH=$JAVA_HOME/bin:$PATH source /etc/profile [让新的环境变量生效] 刷新系统环境变量 16.2.2 测试是否安装成功编写一个简单的Hello.java 输出”hello,world!” p87 tomcat 的安装16.3.1 步骤: 上传安装文件，并解压缩到/opt/tomcat 进入解压目录/bin , 启动tomcat ./startup.sh 开放端口8080 , 回顾firewall-cmd 16.3.2 测试是否安装成功：在windows、Linux 下访问http://linuxip:8080 p88 idea2020 的安装16.4.1 步骤 下载地址: https://www.jetbrains.com/idea/download/#section=windows 解压缩到/opt/idea 启动idea bin 目录下./idea.sh，配置jdk 编写Hello world 程序并测试成功！ p89 mysql5.7 的安装(!!) 新建文件夹/opt/mysql，并cd进去 运行wget http://dev.mysql.com/get/mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar，下载mysql安装包 PS：centos7.6自带的类mysql数据库是mariadb，会跟mysql冲突，要先删除。 运行tar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar 运行rpm -qa|grep mari，查询mariadb相关安装包 运行rpm -e –nodeps mariadb-libs，卸载 然后开始真正安装mysql，依次运行以下几条 rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm 运行systemctl start mysqld.service，启动mysql 然后开始设置root用户密码 Mysql自动给root用户设置随机密码，运行grep “password” /var/log/mysqld.log可看到当前密码 运行mysql -u root -p，用root用户登录，提示输入密码可用上述的，可以成功登陆进入mysql命令行 设置root密码，对于个人开发环境，如果要设比较简单的密码（生产环境服务器要设复杂密码），可以运行 set global validate_password_policy=0; 提示密码设置策略 （validate_password_policy默认值1，） set password for ‘root’@’localhost’ =password(‘xjxj1109’); 运行flush privileges;使密码设置生效 p90 小结p91 shell编程快速入门17.1 为什么要学习Shell 编程 Linux 运维工程师在进行服务器集群管理时，需要编写Shell 程序来进行服务器管理。 对于JavaEE 和Python 程序员来说，工作的需要，你的老大会要求你编写一些Shell 脚本进行程序或者是服务器的维护，比如编写一个定时备份数据库的脚本。 对于大数据程序员来说，需要编写Shell 程序来管理集群 17.2 Shell 是什么Shell 是一个命令行解释器，它为用户提供了一个向Linux 内核发送请求以便运行程序的界面系统级程序，用户可以用Shell 来启动、挂起、停止甚至是编写一些程序。看一个示意图 17.3 Shell 脚本的执行方式17.3.1 脚本格式要求 脚本以#!/bin/bash 开头 脚本需要有可执行权限 chmod u+x [file] 17.3.2 编写第一个Shell 脚本需求说明：创建一个Shell 脚本，输出hello world!vim hello.sh#!/bin/bashecho “hello,world~” 可以使用绝对也可以使用相对路径来执行这个脚本，前提是有可执行权限 ./hello.sh 相对路径 /root/shcode/hello.sh 绝对路径 17.3.3 脚本的常用执行方式方式1(输入脚本的绝对路径或相对路径)说明：首先要赋予helloworld.sh 脚本的+x 权限， 再执行脚本比如./hello.sh 或者使用绝对路径/root/shcode/hello.sh方式2(sh+脚本)说明：不用赋予脚本+x 权限，直接执行即可。比如sh hello.sh , 也可以使用绝对路径 p92 shell变量17.4 Shell 的变量17.4.1 Shell 变量介绍 Linux Shell 中的变量分为，系统变量和用户自定义变量。 系统变量：$HOME、$PWD、$SHELL、$USER 等等，比如： echo $HOME 等等.. 显示当前shell 中所有变量：set 17.4.2 shell 变量的定义基本语法 定义变量：变量名=值 中间不要空格 撤销变量：unset 变量 声明静态变量：readonly 变量，注意：不能unset快速入门 案例1：定义变量A 案例2：撤销变量A 案例3：声明静态的变量B=2，不能unset 12345678910111213141516171819202122#!/bin/bash#案例1：定义变量AA=100#输出变量需要加上$echo A=$Aecho \"A=$A\"#案例2：撤销变量Aunset Aecho \"A=$A\"#案例3：声明静态的变量B=2，不能unsetreadonly B=2echo \"B=$B\"#unset B#将指令返回的结果赋给变量:&lt;&lt;!C=`date`D=$(date)echo \"C=$C\"echo \"D=$D\"!#使用环境变量TOMCAT_HOMEecho \"tomcat_home=$TOMCAT_HOME\" 案例4：可把变量提升为全局环境变量，可供其他shell 程序使用[该案例后面讲] 17.4.3 shell 变量的定义定义变量的规则 变量名称可以由字母、数字和下划线组成，但是不能以数字开头。5A=200(×) 等号两侧不能有空格 变量名称一般习惯为大写， 这是一个规范，我们遵守即可将命令的返回值赋给变量 A=date反引号，运行里面的命令，并把结果返回给变量A A=$(date) 等价于反引号 p93 设置环境变量17.5 设置环境变量17.5.1 基本语法 export 变量名=变量值（功能描述：将shell 变量输出为环境变量/全局变量） source 配置文件（功能描述：让修改后的配置信息立即生效） echo $变量名（功能描述：查询环境变量的值） 示意 17.5.2 快速入门 在/etc/profile 文件中定义TOMCAT_HOME 环境变量 查看环境变量TOMCAT_HOME 的值 在另外一个shell 程序中使用TOMCAT_HOME注意：在输出TOMCAT_HOME 环境变量前，需要让其生效source /etc/profile shell 脚本的多行注释:&lt;&lt;! 内容! p94 位置参数变量17.6.1 介绍当我们执行一个shell 脚本时，如果希望获取到命令行的参数信息，就可以使用到位置参数变量比如： ./myshell.sh 100 200 , 这个就是一个执行shell 的命令行，可以在myshell 脚本中获取到参数信息 17.6.2 基本语法$n （功能描述：n 为数字，$0 代表命令本身，$1-$9 代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@（功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$#（功能描述：这个变量代表命令行中所有参数的个数） 17.6.3 位置参数变量案例：编写一个shell 脚本position.sh ， 在脚本中获取到命令行的各个参数信息。 p95 预定义变量17.7 预定义变量17.7.1 基本介绍就是shell 设计者事先已经定义好的变量，可以直接在shell 脚本中使用 17.7.2 基本语法 $$ （功能描述：当前进程的进程号（PID）） $! （功能描述：后台运行的最后一个进程的进程号（PID）） $？（功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。） 17.7.3 应用实例在一个shell 脚本中简单使用一下预定义变量preVar.sh#!/bin/bashecho “当前执行的进程id=$$”#以后台的方式运行一个脚本，并获取他的进程号/root/shcode/myshell.sh &amp;echo “最后一个后台方式运行的进程id=$!”echo “执行的结果是=$?” p96 运算符17.8.1 基本介绍学习如何在shell 中进行各种运算操作。 17.8.2 基本语法12341) “$((运算式))”或“$[运算式]”或者expr m + n //expression 表达式2) 注意expr 运算符间要有 空格 , 如果希望将expr 的结果赋给某个变量，使用``3) expr m - n4) expr \\*, /, % 乘，除，取余 17.8.3 应用实例oper.sh123456789101112131415161718案例1：计算（2+3）X4 的值案例2：请求出命令行的两个参数[整数]的和20 50#!/bin/bash#案例1：计算（2+3）X4 的值#使用第一种方式RES1=$(((2+3)*4))echo \"res1=$RES1\"#使用第二种方式, 推荐使用RES2=$[(2+3)*4]echo \"res2=$RES2\"#使用第三种方式exprTEMP=`expr 2 + 3`RES4=`expr $TEMP \\* 4`echo \"temp=$TEMP\"echo \"res4=$RES4\"#案例2：请求出命令行的两个参数[整数]的和20 50SUM=$[$1+$2]echo \"sum=$SUM\" p97 条件判断17.9.1 判断语句基本语法 😢[ condition ]（注意condition 前后要有空格）#非空返回true，可使用$?验证（0 为true，&gt;1 为false） 应用实例 🔕[ hspEdu ] 返回true[ ] 返回false[ condition ] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 判断语句 🖕常用判断条件 = 字符串比较 两个整数的比较 -lt 小于-le 小于等于little equal-eq 等于-gt 大于-ge 大于等于-ne 不等于 按照文件权限进行判断 -r 有读的权限-w 有写的权限-x 有执行的权限 按照文件类型进行判断 -f 文件存在并且是一个常规的文件-e 文件存在-d 文件存在并是一个目录 应用实例🚀案例1：”ok”是否等于”ok”判断语句：使用=案例2：23 是否大于等于22判断语句：使用-ge案例3：/root/shcode/aaa.txt 目录中的文件是否存在判断语句： 使用-f代码如下: p98 流程控制17.10.1 if 判断基本语法12345678910111213if [ 条件判断式]then代码fi或者, 多分支if [ 条件判断式]then代码elif [条件判断式]then代码fi 注意事项：[ 条件判断式]，中括号和条件判断式之间必须有空格 应用实例ifCase.sh案例：请编写一个shell 程序，如果输入的参数，大于等于60，则输出”及格了”，如果小于60,则输出”不及格” 😗😏 p99 流程控制（2）17.10.2 case 语句基本语法case $变量名in“值1”）如果变量的值等于值1，则执行程序1;;“值2”）如果变量的值等于值2，则执行程序2;;…省略其他分支… *）如果变量的值都不是以上的值，则执行此程序;;esac 应用实例 testCase.sh案例1 ：当命令行参数是1 时，输出”周一”, 是2 时，就输出”周二”， 其它情况输出”other” p100 for 循环基本语法1🤕for 变量in 值1 值2 值3…do程序/代码done 应用实例testFor1.sh🏭案例1 ：打印命令行输入的参数[这里可以看出$* 和$@ 的区别] 基本语法2🤒for (( 初始值;循环控制条件;变量变化))do程序/代码done 应用实例testFor2.sh👊案例1 ：从1 加到100 的值输出显示 p101 while循环基本语法1while [ 条件判断式]do程序/代码done注意：while 和[有空格，条件判断式和[也有空格 应用实例testWhile.sh案例1 ：从命令行输入一个数n，统计从1+..+ n 的值是多少？ 1234567891011#!/bin/bash#案例1 ：从命令行输入一个数n，统计从1+..+ n 的值是多少？SUM=0i=0while [ $i -le $1 ]doSUM=$[$SUM+$i]#i 自增i=$[$i+1]doneecho \"执行结果=$SUM\" p102 read 读取控制台输入17.11.1 基本语法read(选项)(参数)选项：-p：指定读取值时的提示符；-t：指定读取值时等待的时间（秒），如果没有在指定的时间内输入，就不再等待了。。参数变量：指定读取值的变量名 17.11.2 应用实例testRead.sh案例1：读取控制台输入一个NUM1 值案例2：读取控制台输入一个NUM2 值，在10 秒内输入。代码: 1234567#!/bin/bash#案例1：读取控制台输入一个NUM1 值read -p \"请输入一个数NUM1=\" NUM1echo \"你输入的NUM1=$NUM1\"#案例2：读取控制台输入一个NUM2 值，在10 秒内输入。read -t 10 -p \"请输入一个数NUM2=\" NUM2echo \"你输入的NUM2=$NUM2\" p103 函数17.12.1 函数介绍shell 编程和其它编程语言一样，有系统函数，也可以自定义函数。系统函数中，我们这里就介绍两个。 17.12.2 系统函数basename 基本语法功能：返回完整路径最后/ 的部分，常用于获取文件名basename [pathname] [suffix]basename [string] [suffix] （功能描述：basename 命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。选项：suffix 为后缀，如果suffix 被指定了，basename 会将pathname 或string 中的suffix 去掉。 应用实例案例1：请返回/home/aaa/test.txt 的”test.txt” 部分basename /home/aaa/test.txt dirname 基本语法功能：返回完整路径最后/ 的前面的部分，常用于返回路径部分dirname 文件绝对路径（功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 应用实例案例1：请返回/home/aaa/test.txt 的/home/aaadirname /home/aaa/test.txt p104 自定义函数基本语法[ function ] funname[()]{Action;[return int;]}调用直接写函数名：funname [值]应用实例案例1：计算输入两个参数的和(动态的获取)， getSum代码 123456789101112#!/bin/bash#案例1：计算输入两个参数的和(动态的获取)， getSum#定义函数getSumfunction getSum() {SUM=$[$n1+$n2]echo \"和是=$SUM\"}#输入两个值read -p \"请输入一个数n1=\" n1read -p \"请输入一个数n2=\" n2#调用自定义函数getSum $n1 $n2 p105 Shell 编程综合案例17.13.1 需求分析 每天凌晨2:30 备份数据库hspedu 到/data/backup/db 备份开始和备份结束能够给出相应的提示信息 备份后的文件要求以备份时间为文件名，并打包成.tar.gz 的形式，比如：2021-03-12_230201.tar.gz 在备份的同时，检查是否有10 天前备份的数据库文件，如果有就将其删除。 画一个思路分析图 17.13.2 代码/usr/sbin/mysql_db.backup.sh1234567891011121314151617181920212223242526#备份目录BACKUP=/data/backup/db#当前时间DATETIME=$(date +%Y-%m-%d_%H%M%S)echo $DATETIME#数据库的地址HOST=localhost#数据库用户名DB_USER=root#数据库密码DB_PW=hspedu100#备份的数据库名DATABASE=hspedu#创建备份目录, 如果不存在，就创建[ ! -d \"${BACKUP}/${DATETIME}\" ] &amp;&amp; mkdir -p \"${BACKUP}/${DATETIME}\"#备份数据库mysqldump -u${DB_USER} -p${DB_PW} --host=${HOST} -q -R --databases ${DATABASE} | gzip &gt;${BACKUP}/${DATETIME}/$DATETIME.sql.gz#将文件处理成tar.gzcd ${BACKUP}tar -zcvf $DATETIME.tar.gz ${DATETIME}#删除对应的备份目录rm -rf ${BACKUP}/${DATETIME}#删除10 天前的备份文件find ${BACKUP} -atime +10 -name \"*.tar.gz\" -exec rm -rf {} \\;echo \"备份数据库${DATABASE} 成功~\" p106 备份数据库p107 小结p108 Ubuntu安装p109 中文包p110 ubuntu的root18.4.1 介绍安装ubuntu 成功后，都是普通用户权限，并没有最高root 权限，如果需要使用root 权限的时候，通常都会在命令前面加上sudo 。有的时候感觉很麻烦。(演示)我们一般使用su 命令来直接切换到root 用户的，但是如果没有给root 设置初始密码，就会抛出su : Authenticationfailure 这样的问题。所以，我们只要给root 用户设置一个初始密码就好了。 18.4.2 给root 用户设置密码并使用 输入sudo passwd 命令，设定root 用户密码。 设定root 密码成功后，输入su 命令，并输入刚才设定的root 密码，就可以切换成root 了。提示符$代表一般用户，提示符#代表root 用户。 以后就可以使用root 用户了 输入exit 命令，退出root 并返回一般用户 p111 Ubuntu 下开发Python18.5.1 说明安装好Ubuntu 后，默认就已经安装好Python 的开发环境。 18.5.2 在Ubuntu 下开发一个Python 程序vi hello.py [编写hello.py]python3 hello.py [运行hello.py] p112 APT 软件管理和远程登录19.1 apt 介绍apt 是Advanced Packaging Tool 的简称，是一款安装包管理工具。在Ubuntu 下，我们可以使用apt 命令进行软件包的安装、删除、清理等，类似于Windows 中的软件管理工具。unbuntu 软件管理的原理示意图： 19.2 Ubuntu 软件操作的相关命令sudo apt-get update 更新源 sudo apt-get install package 安装包 sudo apt-get remove package 删除包 sudo apt-cache search package 搜索软件包 sudo apt-cache show package 获取包的相关信息，如说明、大小、版本等 sudo apt-get install package –reinstall 重新安装包 sudo apt-get -f install 修复安装 sudo apt-get remove package –purge 删除包，包括配置文件等 sudo apt-get build-dep package 安装相关的编译环境 sudo apt-get upgrade 更新已安装的包 sudo apt-get dist-upgrade 升级系统 sudo apt-cache depends package 了解使用该包依赖那些包 sudo apt-cache rdepends package 查看该包被哪些包依赖 sudo apt-get source package 更新Ubuntu 软件下载地址 19.3.1 原理介绍(画出示意图) 19.3.2 寻找国内镜像源https://mirrors.tuna.tsinghua.edu.cn/所谓的镜像源：可以理解为提供下载软件的地方，比如Android 手机上可以下载软件的安卓市场；iOS 手机上可以下载软件的AppStore 19.3.3 寻找国内镜像源19.3.4 备份Ubuntu 默认的源地址sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup 19.3.5 更新源服务器列表先清空sources.list 文件复制镜像网站的地址 复制镜像网站的地址， 拷贝到sources.list 文件 p113 更新源和实例19.3.6 更新源更新源地址：sudo apt-get update 19.4 Ubuntu 软件安装，卸载的最佳实践案例说明：使用apt 完成安装和卸载vim 软件，并查询vim 软件的信息：（因为使用了镜像网站， 速度很快）sudo apt-get remove vim //删除sudo apt-get install vim //安装sudo apt-cache show vim //获取软件信息 p114 ubuntu远程登录和集群19.5.1 ssh 介绍SSH 为Secure Shell 的缩写，由IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议。 SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录。几乎所有UNIX/LInux平台都可运行SSH。使用SSH 服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A 机器想被B 机器远程控制，那么，A 机器需要安装SSH 服务器，B 机器需要安装SSH 客户端。和CentOS 不一样，Ubuntu 默认没有安装SSHD 服务(使用netstat 指令查看: apt install net-tools)，因此，我们不能进行远程登录。 19.5.2 原理示意图 19.5.3 安装SSH 和启用sudo apt-get install openssh-server执行上面指令后，在当前这台Linux 上就安装了SSH 服务端和客户端。service sshd restart执行上面的指令，就启动了sshd 服务。会监听端口22 19.5.4 在Windows 使用XShell6/XFTP6 登录Ubuntu前面我们已经安装了XShell6，直接使用即可。注意：使用hspEdu 用户登录，需要的时候再su - 切换成root 用户 19.5.5 从一台linux 系统远程登陆另外一台linux 系统在创建服务器集群时，会使用到该技术 基本语法：ssh 用户名@IP例如：ssh hspedu@192.168.200.130使用ssh 访问，如访问出现错误。可查看是否有该文件～/.ssh/known_ssh 尝试删除该文件解决，一般不会有问题 登出登出命令：exit 或者logout p115 小结p116 CentOS8.1/8.2的使用安装Centos8.1/8.220.1.1 Centos 下载地址CentOS-8.1.1911-x86_64-dvd1.iso CentOS 8.1/8.2 DVD 版8G (未来的主流.)https://mirrors.aliyun.com/centos/8.1.1911/isos/x86_64/ p117 日志管理21.1 基本介绍 日志文件是重要的系统信息文件，其中记录了许多重要的系统事件，包括用户的登录信息、系统的启动信息、系统的安全信息、邮件相关信息、各种服务相关信息等。 日志对于安全来说也很重要，它记录了系统每天发生的各种事情，通过日志来检查错误发生的原因，或者受到攻击时攻击者留下的痕迹。 可以这样理解日志是用来记录重大事件的工具 21.2 系统常用的日志 /var/log/ 目录就是系统日志文件的保存位置，看张图 系统常用的日志 应用案例使用root 用户通过xshell6 登陆, 第一次使用错误的密码，第二次使用正确的密码登录成功看看在日志文件/var/log/secure 里有没有记录相关信息 p118 日志管理服务rsyslogd21.3 日志管理服务rsyslogdCentOS7.6 日志服务是rsyslogd ， CentOS6.x 日志服务是syslogd 。rsyslogd 功能更强大。rsyslogd 的使用、日志文件的格式，和syslogd 服务兼容的。原理示意图 p119 日志服务配置文件查询Linux 中的rsyslogd 服务是否启动ps aux | grep “rsyslog” | grep -v “grep” -v 表示反向匹配 表示查询不包含grep的服务 查询rsyslogd 服务的自启动状态systemctl list-unit-files | grep rsyslog 配置文件：/etc/rsyslog.conf编辑文件时的格式为： . 存放日志文件其中第一个代表日志类型，第二个代表日志级别 日志类型分为：auth ##pam 产生的日志 authpriv ##ssh、ftp 等登录信息的验证信息corn ##时间任务相关kern ##内核lpr ##打印mail ##邮件mark(syslog)-rsyslog##服务内部的信息，时间标识news ##新闻组user ##用户程序产生的相关信息uucp ##unix to nuix copy 主机之间相关的通信local 1-7 ##自定义的日志设备2) 日志级别分为：debug ##有调试信息的，日志通信最多info ##一般信息日志，最常用notice ##最具有重要性的普通条件的信息warning ##警告级别err ##错误级别，阻止某个功能或者模块不能正常工作的信息crit ##严重级别，阻止整个系统或者整个软件不能正常工作的信息alert ##需要立刻修改的信息emerg ##内核崩溃等重要信息none ##什么都不记录注意：从上到下，级别从低到高，记录信息越来越少 由日志服务rsyslogd 记录的日志文件，日志文件的格式包含以下4 列： 事件产生的时间 产生事件的服务器的主机名 产生事件的服务名或程序名 事件的具体信息 日志如何查看实例查看一下/var/log/secure 日志，这个日志中记录的是用户验证和授权方面的信息来分析如何查看 p120 自定义日志服务日志管理服务应用实例在/etc/rsyslog.conf 中添加一个日志文件/var/log/hsp.log,当有事件发送时(比如sshd 服务相关事件)，该文件会接收到信息并保存. 给小伙伴演示重启，登录的情况，看看是否有日志保存 p121 日志轮替21.4.1 基本介绍日志轮替就是把旧的日志文件移动并改名，同时建立新的空日志文件，当旧日志文件超出保存的范围之后，就会进行删除 21.4.2 日志轮替文件命名 centos7 使用logrotate 进行日志轮替管理，要想改变日志轮替文件名字，通过/etc/logrotate.conf 配置文件中“dateext”参数： 如果配置文件中有“dateext”参数，那么日志会用日期来作为日志文件的后缀，例如“secure-20201010”。这样日志文件名不会重叠，也就不需要日志文件的改名， 只需要指定保存日志个数，删除多余的日志文件即可。 如果配置文件中没有“dateext”参数，日志文件就需要进行改名了。当第一次进行日志轮替时，当前的“secure”日志会自动改名为“secure.1”，然后新建“secure”日志， 用来保存新的日志。当第二次进行日志轮替时，“secure.1”会自动改名为“secure.2”， 当前的“secure”日志会自动改名为“secure.1”，然后也会新建“secure”日志，用来保存新的日志，以此类推。 21.4.3 logrotate 配置文件/etc/logrotate.conf 为logrotate 的全局配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243# rotate log files weekly, 每周对日志文件进行一次轮替weekly# keep 4 weeks worth of backlogs, 共保存4 份日志文件，当建立新的日志文件时，旧的将会被删除rotate 4# create new (empty) log files after rotating old ones, 创建新的空的日志文件，在日志轮替后create# use date as a suffix of the rotated file, 使用日期作为日志轮替文件的后缀dateext# uncomment this if you want your log files compressed, 日志文件是否压缩。如果取消注释，则日志会在转储的同时进行压缩#compress#RPM packages drop log rotation information into this directoryinclude /etc/logrotate.d# 包含/etc/logrotate.d/ 目录中所有的子配置文件。也就是说会把这个目录中所有子配置文件读取进来，#下面是单独设置，优先级更高。# no packages own wtmp and btmp -- we'll rotate them here/var/log/wtmp {monthly # 每月对日志文件进行一次轮替create 0664 root utmp # 建立的新日志文件，权限是0664 ，所有者是root ，所属组是utmp 组minsize 1M # 日志文件最小轮替大小是1MB 。也就是日志一定要超过1MB 才会轮替，否则就算时间达到一个月，也不进行日志转储rotate 1 # 仅保留一个日志备份。也就是只有wtmp 和wtmp.1 日志保留而已}/var/log/btmp {missingok # 如果日志不存在，则忽略该日志的警告信息monthlycreate 0600 root utmprotate 1} p122 自定义日志轮替参数说明1234567891011121314151617参数参数说明daily 日志的轮替周期是每天weekly 日志的轮替周期是每周monthly 日志的轮替周期是每月rotate 数字保留的日志文件的个数。0 指没有备份compress 日志轮替时，旧的日志进行压缩create mode owner group 建立新日志，同时指定新日志的权限与所有者和所属组。mail address 当日志轮替时，输出内容通过邮件发送到指定的邮件地址。missingok 如果日志不存在，则忽略该日志的警告信息notifempty 如果日志为空文件，则不进行日志轮替minsize 大小日志轮替的最小值。也就是日志一定要达到这个最小值才会轮替，否则就算时间达到也不轮替size 大小日志只有大于指定大小才进行日志轮替，而不是按照时间轮替。dateext 使用日期作为日志轮替文件的后缀。sharedscripts 在此关键字之后的脚本只执行一次。prerotate/endscript 在日志轮替之前执行脚本命令。postrotate/endscript 在日志轮替之后执行脚本命令。 21.4.4 把自己的日志加入日志轮替 第一种方法是直接在/etc/logrotate.conf 配置文件中写入该日志的轮替策略 第二种方法是在/etc/logrotate.d/目录中新建立该日志的轮替文件，在该轮替文件中写入正确的轮替策略，因为该目录中的文件都会被“include”到主配置文件中，所以也可以把日志加入轮替。 推荐使用第二种方法，因为系统中需要轮替的日志非常多，如果全都直接写入/etc/logrotate.conf 配置文件，那么这个文件的可管理性就会非常差，不利于此文件的维护。 在/etc/logrotate.d/ 配置轮替文件一览 21.4.5 应用实例看一个案例, 在/etc/logrotate.conf 进行配置, 或者直接在/etc/logrotate.d/ 下创建文件hsplog 编写如下内容, 具体轮替的效果可以参考/var/log 下的boot.log 情况. p123 日志轮替机制原理21.5 日志轮替机制原理日志轮替之所以可以在指定的时间备份日志，是依赖系统定时任务。在/etc/cron.daily/目录，就会发现这个目录中是有logrotate 文件(可执行)，logrotate 通过这个文件依赖定时任务执行的。 p124 查看内存日志journalctl 可以查看内存日志, 这里我们看看常用的指令journalctl ##查看全部journalctl -n 3 ##查看最新3 条journalctl –since 19:00 –until 19:10:10 #查看起始时间到结束时间的日志可加日期journalctl -p err ##报错日志journalctl -o verbose ##日志详细内容journalctl _PID=1245 _COMM=sshd ##查看包含这些参数的日志（在详细日志查看）或者journalctl | grep sshd 注意: journalctl 查看的是内存日志, 重启清空演示案例:使用journalctl | grep sshd 来看看用户登录清空, 重启系统，再次查询，看看日志有什么变化没有 p125 小结p126 定制自己的linux系统22.1 基本介绍通过裁剪现有Linux 系统(CentOS7.6)，创建属于自己的min Linux 小系统，可以加深我们对linux 的理解。老韩利用centos7.6，搭建一个小小linux 系统, 很有趣。 22.2 基本原理启动流程介绍：制作Linux 小系统之前，再了解一下Linux 的启动流程：1、首先Linux 要通过自检，检查硬件设备有没有故障2、如果有多块启动盘的话，需要在BIOS 中选择启动磁盘3、启动MBR 中的bootloader 引导程序4、加载内核文件5、执行所有进程的父进程、老祖宗systemd6、欢迎界面在Linux 的启动流程中，加载内核文件时关键文件：1）kernel 文件: vmlinuz-3.10.0-957.el7.x86_642）initrd 文件: initramfs-3.10.0-957.el7.x86_64.img 22.3 制作min linux 思路分析 在现有的Linux 系统(centos7.6)上加一块硬盘/dev/sdb，在硬盘上分两个分区，一个是/boot，一个是/，并将其格式化。需要明确的是，现在加的这个硬盘在现有的Linux 系统中是/dev/sdb，但是，当我们把东西全部设置好时，要把这个硬盘拔除，放在新系统上，此时，就是/dev/sda 在/dev/sdb 硬盘上，将其打造成独立的Linux 系统，里面的所有文件是需要拷贝进去的 作为能独立运行的Linux 系统，内核是一定不能少，要把内核文件和initramfs 文件也一起拷到/dev/sdb 上 以上步骤完成，我们的自制Linux 就完成, 创建一个新的linux 虚拟机，将其硬盘指向我们创建的硬盘，启动即可 示意图 制作自己的min linux(基于CentOS7.6) 首先，我们在现有的linux添加一块大小为20G的硬盘 点击完成，就OK了， 可以使用 lsblk 查看，需要重启 添加完成后，点击确定，然后启动现有的linux(centos7.6)。 通过fdisk来给我们的/dev/sdb进行分区 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 1 [root@localhost ~]# fdisk /dev/sdb 2 Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel 3 Building a new DOS disklabel with disk identifier 0x4fde4cd0. 4 Changes will remain in memory only, until you decide to write them. 5 After that, of course, the previous content won't be recoverable. 6 7 8 Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite) 9 10 11 WARNING: DOS-compatible mode is deprecated. It's strongly recommended to12 switch off the mode (command 'c') and change display units to13 sectors (command 'u').14 15 16 Command (m for help): n17 Command action18 e extended19 p primary partition (1-4)20 p21 Partition number (1-4): 122 First cylinder (1-2610, default 1):23 Using default value 124 Last cylinder, +cylinders or +size{K,M,G} (1-2610, default 2610): +500M25 26 27 Command (m for help): n28 Command action29 e extended30 p primary partition (1-4)31 p32 Partition number (1-4): 233 First cylinder (15-2610, default 15):34 Using default value 1535 Last cylinder, +cylinders or +size{K,M,G} (15-2610, default 2610):36 Using default value 261037 #查看分区38 Command (m for help): p39 40 41 Disk /dev/sdb: 21.5 GB, 21474836480 bytes42 255 heads, 63 sectors/track, 2610 cylinders43 Units = cylinders of 16065 * 512 = 8225280 bytes44 Sector size (logical/physical): 512 bytes / 512 bytes45 I/O size (minimum/optimal): 512 bytes / 512 bytes46 Disk identifier: 0x4fde4cd047 48 49 Device Boot Start End Blocks Id System50 /dev/sdb1 1 14 112423+ 83 Linux51 /dev/sdb2 15 2610 20852370 83 Linux52 #保存并退出53 Command (m for help): w54 The partition table has been altered! 接下来，我们对/dev/sdb的分区进行格式化 12[root@localhost ~]# mkfs.ext4 /dev/sdb1[root@localhost ~]# mkfs.ext4 /dev/sdb2 创建目录，并挂载新的磁盘 123#mkdir -p /mnt/boot /mnt/sysroot #mount /dev/sdb1 /mnt/boot #mount /dev/sdb2 /mnt/sysroot/ 安装grub, 内核文件拷贝至目标磁盘 1234#grub2-install --root-directory=/mnt /dev/sdb#我们可以来看一下二进制确认我们是否安装成功#hexdump -C -n 512 /dev/sdb #cp -rf /boot/* /mnt/boot/ 修改 grub2/grub.cfg 文件, 标红的部分 是需要使用 指令来查看的 在grub.cfg文件中 , 红色部分用 上面 sdb1 的 UUID替换，蓝色部分用 sdb2的UUID来替换, 紫色部分是添加的，表示 selinux给关掉，同时设定一下init，告诉内核不要再去找这个程序了，不然开机的时候会出现错误的 123456789101112131415161718192021222324252627282930313233### BEGIN /etc/grub.d/10_linux ###menuentry 'CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-957.el7.x86_64-advanced-2eef594e-68fc-49a0-8b23-07cf87dda424' { load_video set gfxpayload=keep insmod gzio insmod part_msdos insmod ext2 set root='hd0,msdos1' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint-bios=hd0,msdos1 --hint-efi=hd0,msdos1 --hint-baremetal=ahci0,msdos1--hint='hd0,msdos1' 6ba72e9a-19ec-4552-ae54-e35e735142d4 else search --no-floppy --fs-uuid --set=root 6ba72e9a-19ec-4552-ae54-e35e735142d4 fi linux16 /vmlinuz-3.10.0-957.el7.x86_64 root=UUID=d2e0ce0f-e209-472a-a4f1-4085f777d9bb ro crashkernel=auto rhgb quiet LANG=zh_CN.UTF-8 selinux=0 init=/bin/bash initrd16 /initramfs-3.10.0-957.el7.x86_64.img}menuentry 'CentOS Linux (0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d-advanced-2eef594e-68fc-49a0-8b23-07cf87dda424' { load_video insmod gzio insmod part_msdos insmod ext2 set root='hd0,msdos1' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint-bios=hd0,msdos1 --hint-efi=hd0,msdos1 --hint-baremetal=ahci0,msdos1 --hint='hd0,msdos1' 6ba72e9a-19ec-4552-ae54-e35e735142d4 else search --no-floppy --fs-uuid --set=root 6ba72e9a-19ec-4552-ae54-e35e735142d4 fi linux16 /vmlinuz-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d root=UUID=d2e0ce0f-e209-472a-a4f1-4085f777d9bb ro crashkernel=auto rhgb quiet selinux=0 init=/bin/bash initrd16 /initramfs-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d.img}### END /etc/grub.d/10_linux ### 创建目标主机根文件系统 1#mkdir -pv /mnt/sysroot/{etc/rc.d,usr,var,proc,sys,dev,lib,lib64,bin,sbin,boot,srv,mnt,media,home,root} 拷贝需要的bash(也可以拷贝你需要的指令)和库文件给新的系统使用 12#cp /lib64/*.* /mnt/sysroot/lib64/ #cp /bin/bash /mnt/sysroot/bin/ 现在我们就可以创建一个新的虚拟机，然后将默认分配的硬盘 移除掉，指向我们刚刚创建的磁盘即可. 这时，很多指令都不能使用，比如 ls , reboot 等，可以将需要的指令拷贝到对应的目录即可 如果要拷贝指令，重新进入到原来的 linux****系统拷贝相应的指令即可，比较将 /bin/ls 拷贝到 /mnt/sysroot/bin 将/sbin/reboot 拷贝到 /mnt/sysroot/sbin 1234root@hspedu100 ~]# mount /dev/sdb2 /mnt/sysroot/[root@hspedu100 ~]# cp /bin/ls /mnt/sysroot/bin/[root@hspedu100 ~]# cp /bin/systemctl /mnt/sysroot/bin/[root@hspedu100 ~]# cp /sbin/reboot /mnt/sysroot/sbin/ 再重新启动新的min linux系统，就可以使用 ls , reboot 指令了 p127 定制自己的linux系统(2)p128 小结p129 Linux 内核源码介绍&amp;内核升级23.1 为什么要阅读linux 内核? 爱好，就是喜欢linux(黑客精神) 想深入理解linux 底层运行机制，对操作系统有深入理解 阅读Linux 内核，你会对整个计算机体系有一个更深刻的认识。作为开发者，不管你从事的是驱动开发，应用开发还是后台开发，你都需要了解操作系统内核的运行机制，这样才能写出更好的代码。 作为开发人员不应该只局限在自己的领域，你设计的模块看起来小，但是你不了解进程的调用机制，你不知道进程为什么会阻塞、就绪、执行几个状态。那么很难写出优质的代码。 找工作面试的需要😏 老韩忠告，作为有追求的程序员，还是应该深入的了解一个操作系统的底层机制,(比如linux/unix) 最好是源码级别的，这样你写多线程高并发程序，包括架构，优化，算法等，高度不一样的，当然老韩也不是要求小伙伴儿把一个非常庞大的Linux 内核每一行都读懂。我觉得。你至少能看几个核心的模块。 23.2 linux0.01 内核源码23.2.1 基本介绍Linux 的内核源代码可以从网上下载, 解压缩后文件一般也都位于linux 目录下。内核源代码有很多版本，可以从linux0.01 内核入手，总共的代码1w 行左右， 最新版本5.9.8 总共代码超过700w 行，非常庞大.内核地址：https://www.kernel.org/ 很多人害怕读Linux 内核，Linux 内核这样大而复杂的系统代码，阅读起来确实有很多困难，但是也不象想象的那么高不可攀。老韩建议可以从linux0.01 入手。 p130 linux0.01 内核源码目录&amp;阅读23.2.2 linux0.01 内核源码目录&amp;阅读老韩提示阅读内核源码技巧 linux0.01 的阅读需要懂c 语言 阅读源码前，应知道Linux 内核源码的整体分布情况。现代的操作系统一般由进程管理、内存管理、文件系统、驱动程序和网络等组成。Linux 内核源码的各个目录大致与此相对应. 在阅读方法或顺序上，有纵向与横向之分。所谓纵向就是顺着程序的执行顺序逐步进行；所谓横向，就是按模块进行。它们经常结合在一起进行。 对于Linux 启动的代码可顺着Linux 的启动顺序一步步来阅读；对于像内存管理部分，可以单独拿出来进行阅读分析。实际上这是一个反复的过程，不可能读一遍就理解linux 内核源码阅读&amp;目录介绍&amp;main.c 说明 p131 linux 内核最新版和内核升级23.3.1 内核地址：https://www.kernel.org/ 查看23.3.2 下载&amp;解压最新版wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.11.2.tar.gztar -zxvf linux-5.8.16.tar.gz 23.3.3 linux 内核升级应用实例将Centos 系统从7.6 内核升级到7.8 版本内核(兼容性问题) 23.3.4 具体步骤，看老师演示uname -a // 查看当前的内核版本yum info kernel -q //检测内核版本，显示可以升级的内核yum update kernel //升级内核yum list kernel -q //查看已经安装的内核 p132 linux 内核最新版和内核升级(2)p133 第24 章linux 系统-备份与恢复24.1 基本介绍实体机无法做快照，如果系统出现异常或者数据损坏，后果严重， 要重做系统，还会造成数据丢失。所以我们可以使用备份和恢复技术linux 的备份和恢复很简单， 有两种方式： 把需要的文件(或者分区)用TAR 打包就行，下次需要恢复的时候，再解压开覆盖即可 使用dump 和restore 命令 示意图 24.2 安装dump 和restore如果linux 上没有dump 和restore 指令，需要先按照yum -y install dumpyum -y install restore 24.3 使用dump 完成备份24.3.1 基本介绍dump 支持分卷和增量备份（所谓增量备份是指备份上次备份后修改/增加过的文件，也称差异备份）。 24.3.2 dump 语法说明dump [ -cu] [-123456789] [ -f &lt;备份后文件名&gt;] [-T &lt;日期&gt;] [ 目录或文件系统]dump []-wW-c ： 创建新的归档文件，并将由一个或多个文件参数所指定的内容写入归档文件的开头。-0123456789： 备份的层级。0 为最完整备份，会备份所有文件。若指定0 以上的层级，则备份至上一次备份以来修改或新增的文件, 到9 后，可以再次轮替.-f &lt;备份后文件名&gt;： 指定备份后文件名-j : 调用bzlib 库压缩备份文件，也就是将备份后的文件压缩成bz2 格式，让文件更小-T &lt;日期&gt;： 指定开始备份的时间与日期-u ： 备份完毕后，在/etc/dumpdares 中记录备份的文件系统，层级，日期与时间等。-t ： 指定文件名，若该文件已存在备份文件中，则列出名称-W ：显示需要备份的文件及其最后一次备份的层级，时间，日期。-w ：与-W 类似，但仅显示需要备份的文件。 24.3.3 dump 应用案例1将/boot 分区所有内容备份到/opt/boot.bak0.bz2 文件中，备份层级为“0”􀀃dump -0uj -f /opt/boot.bak0.bz2 /boot 24.3.4 dump 应用案例2在/boot 目录下增加新文件，备份层级为“1”(只备份上次使用层次“0”备份后发生过改变的数据), 注意比较看看这次生成的备份文件boot1.bak 有多大dump -1uj -f /opt/boot.bak1.bz2 /boot老韩提醒: 通过dump 命令在配合crontab 可以实现无人值守备份 只有分区支持增量备份 24.3.5 dump -W显示需要备份的文件及其最后一次备份的层级，时间，日期 24.3.6 查看备份时间文件cat /etc/dumpdates 24.3.7 dump 备份文件或者目录前面我们在备份分区时，是可以支持增量备份的，如果备份文件或者目录，不再支持增量备份, 即只能使用0 级别备份案例， 使用dump 备份/etc 整个目录dump -0j -f /opt/etc.bak.bz2 /etc/#下面这条语句会报错，提示DUMP: Only level 0 dumps are allowed on a subdirectorydump -1j -f /opt/etc.bak.bz2 /etc/ 24.3.8 老韩提醒如果是重要的备份文件， 比如数据区，建议将文件上传到其它服务器保存，不要将鸡蛋放在同一个篮子. p134 数据备份与恢复24.4 使用restore 完成恢复24.4.1 基本介绍restore 命令用来恢复已备份的文件，可以从dump 生成的备份文件中恢复原文件 24.4.2 restore 基本语法restore [模式选项] [选项] 说明下面四个模式， 不能混用，在一次命令中， 只能指定一种。-C ：使用对比模式，将备份的文件与已存在的文件相互对比。-i：使用交互模式，在进行还原操作时，restors 指令将依序询问用户-r：进行还原模式-t : 查看模式，看备份文件有哪些文件选项-f &lt;备份设备&gt;：从指定的文件中读取备份数据，进行还原操作 24.4.3 应用案例1restore 命令比较模式，比较备份文件和原文件的区别测试mv /boot/hello.java /boot/hello100.javarestore -C -f boot.bak1.bz2 //注意和最新的文件比较 mv /boot/hello100.java /boot/hello.javarestore -C -f boot.bak1.bz2 24.4.4 应用案例2restore 命令查看模式，看备份文件有哪些数据/文件 测试restore -t -f boot.bak0.bz2 24.4.5 应用案例3restore 命令还原模式, 注意细节： 如果你有增量备份，需要把增量备份文件也进行恢复， 有几个增量备份文件，就要恢复几个，按顺序来恢复即可。测试mkdir /opt/boottmpcd /opt/boottmprestore -r -f /opt/boot.bak0.bz2 //恢复到第1 次完全备份状态restore -r -f /opt/boot.bak1.bz2 //恢复到第2 次增量备份状态 24.4.6 应用案例4restore 命令恢复备份的文件，或者整个目录的文件基本语法： restore -r -f 备份好的文件测试[root@hspedu100 opt]# mkdir etctmp[root@hspedu100 opt]# cd etctmp/[root@hspedu100 etctmp]# restore -r -f /opt/etc.bak0.bz2 p135 数据备份与恢复（2）p136 数据备份与恢复小结p137 Linux 可视化管理-webmin 和bt 运维工具25.1 webmin25.1.1 基本介绍Webmin 是功能强大的基于Web 的Unix/linux 系统管理工具。管理员通过浏览器访问Webmin 的各种管理功能并完成相应的管理操作。除了各版本的linux 以外还可用于：AIX、HPUX、Solaris、Unixware、Irix 和FreeBSD 等系统 25.1.2 安装webmin&amp;配置 下载地址: http://download.webmin.com/download/yum/ , 用下载工具下载即可 也可以使用wget http://download.webmin.com/download/yum/webmin-1.700-1.noarch.rpm 安装： rpm -ivh webmin-1.700-1.noarch.rpm 重置密码/usr/libexec/webmin/changepass.pl /etc/webmin root testroot 是webmin 的用户名，不是OS 的, 这里就是把webmin 的root 用户密码改成了test 修改webmin 服务的端口号（默认是10000 出于安全目的）vim /etc/webmin/miniserv.conf # 修改端口 将port=10000 修改为其他端口号，如port=6666 重启webmin/etc/webmin/restart # 重启/etc/webmin/start # 启动/etc/webmin/stop # 停止 防火墙放开6666 端口firewall-cmd –zone=public –add-port=6666/tcp –permanent # 配置防火墙开放6666 端口firewall-cmd –reload # 更新防火墙配置firewall-cmd –zone=public –list-ports # 查看已经开放的端口号 在这个位置我出现了登录不上去的问题，搜了很多方案没有解决😢包括 1.杀进程换端口 2.reboot 3.在gnu上登录 登录webminhttp://ip:6666 可以访问了用root 账号和重置的新密码test p138 webmin演示25.1.3 简单使用演示比如修改语言设置，IP 访问控制，查看进程, 修改密码， 任务调度，mysql 等. p139 bt宝塔介绍和安装25.2 bt(宝塔)25.2.1 基本介绍bt 宝塔Linux 面板是提升运维效率的服务器管理软件，支持一键LAMP/LNMP/集群/监控/网站/FTP/数据库/JAVA 等多项服务器管理功能。 25.2.2 安装和使用 安装: yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh 安装成功后控制台会显示登录地址，账户密码，复制浏览器打开登录， myself command 外网面板地址: http://58.221.242.226:8888/d9b5227e内网面板地址: http://192.168.200.130:8888/d9b5227eusername: tz6prifgpassword: 2276bcb2 p140 介绍25.2.3 使用介绍， 比如可以登录终端, 配置，快捷安装运行环境和系统工具, 添加计划任务脚本http://192.168.200.130:8888/2e673418/ 25.2.4 如果bt 的用户名，密码忘记了，使用bt default 可以查看 p141 小结p142 Linux 面试题-(腾讯,百度,美团,滴滴)26.1 分析日志t.log(访问量)，将各个ip 地址截取，并统计出现次数,并按从大到小排序(腾讯)http://192.168.200.10/index1.htmlhttp://192.168.200.10/index2.htmlhttp://192.168.200.20/index1.htmlhttp://192.168.200.30/index1.htmlhttp://192.168.200.40/index1.htmlhttp://192.168.200.30/order.htmlhttp://192.168.200.10/order.html答案: cat t.txt | cut -d ‘/‘ -f 3 | sort | uniq -c | sort -nr 26.2 统计连接到服务器的各个ip 情况，并按连接数从大到小排序(腾讯)netstat -an | grep ESTABLISHED | awk -F “ “ ‘{print $5}’ | cut -d “:” -f 1 | sort | uniq -c| sort -nr 其他redis 相关指令 123456789101112131415161718# 重新加载系统服务systemctl daemon-reload# 开机启动system enable redis-server.service# 关闭redis-serversystem stop redis-server.service# 启动redis-serversystem start redis-server.service# 重新启动redis-serversystem restart redis-server.service# 查看redis-server运行状态system status redis-server.service","link":"/2021/04/09/linux%E9%9F%A9%E9%A1%BA%E5%B9%B32021/"},{"title":"ES","text":"前言 最近的elasticsearch笔记 ​ 2021-04-20 18:05:26 ES个人使用环境为 es7.12 1.ElasticSearch 核心概念介绍4.1 ElasticSearch 十大核心概念4.1.1 集群（Cluster）一个或者多个安装了 es 节点的服务器组织在一起，就是集群，这些节点共同持有数据，共同提供搜索服务。 一个集群有一个名字，这个名字是集群的唯一标识，该名字成为 cluster name，默认的集群名称是 elasticsearch，具有相同名称的节点才会组成一个集群。 可以在 config/elasticsearch.yml 文件中配置集群名称： 1cluster.name: javaboy-es 在集群中，节点的状态有三种：绿色、黄色、红色： 绿色：节点运行状态为健康状态。所有的主分片、副本分片都可以正常工作。 黄色：表示节点的运行状态为警告状态，所有的主分片目前都可以直接运行，但是至少有一个副本分片是不能正常工作的。 红色：表示集群无法正常工作。 4.1.2 节点（Node）集群中的一个服务器就是一个节点，节点中会存储数据，同时参与集群的索引以及搜索功能。一个节点想要加入一个集群，只需要配置一下集群名称即可。默认情况下，如果我们启动了多个节点，多个节点还能够互相发现彼此，那么它们会自动组成一个集群，这是 es 默认提供的，但是这种方式并不可靠，有可能会发生脑裂现象。所以在实际使用中，建议一定手动配置一下集群信息。 4.1.3 索引（Index）索引可以从两方面来理解： 名词 具有相似特征文档的集合。 动词 索引数据以及对数据进行索引操作。 4.1.4 类型（Type）类型是索引上的逻辑分类或者分区。在 es6 之前，一个索引中可以有多个类型，从 es7 开始，一个索引中，只能有一个类型。在 es6.x 中，依然保持了兼容，依然支持单 index 多个 type 结构，但是已经不建议这么使用。 4.1.5 文档（Document）一个可以被索引的数据单元。例如一个用户的文档、一个产品的文档等等。文档都是 JSON 格式的。 4.1.6 分片（Shards）索引都是存储在节点上的，但是受限于节点的空间大小以及数据处理能力，单个节点的处理效果可能不理想，此时我们可以对索引进行分片。当我们创建一个索引的时候，就需要指定分片的数量。每个分片本身也是一个功能完善并且独立的索引。 默认情况下，一个索引会自动创建 1 个分片，并且为每一个分片创建一个副本。 4.1.7 副本（Replicas）副本也就是备份，是对主分片的一个备份。 4.1.8 Settings集群中对索引的定义信息，例如索引的分片数、副本数等等。 4.1.9 MappingMapping 保存了定义索引字段的存储类型、分词方式、是否存储等信息。 4.1.10 Analyzer字段分词方式的定义。 4.2 ElasticSearch Vs 关系型数据库 关系型数据库 ElasticSearch 数据库 索引 表 类型 行 文档 列 字段 表结构 映射（Mapping） SQL DSL(Domain Specific Language) Select * from xxx GET http:// update xxx set xx=xxx PUT http:// Delete xxx DELETE http:// 索引 全文索引 2.ElasticSearch 分词器1.1 内置分词器ElasticSearch 核心功能就是数据检索，首先通过索引将文档写入 es。查询分析则主要分为两个步骤： 词条化：分词器将输入的文本转为一个一个的词条流。 过滤：比如停用词过滤器会从词条中去除不相干的词条（的，嗯，啊，呢）停用词；另外还有同义词过滤器、小写过滤器等。 ElasticSearch 中内置了多种分词器可以供使用。 内置分词器： 分词器 作用 Standard Analyzer 标准分词器，适用于英语等。 Simple Analyzer 简单分词器，基于非字母字符进行分词，单词会被转为小写字母。 Whitespace Analyzer 空格分词器。按照空格进行切分。 Stop Analyzer 类似于简单分词器，但是增加了停用词的功能。 Keyword Analyzer 关键词分词器，输入文本等于输出文本。 Pattern Analyzer 利用正则表达式对文本进行切分，支持停用词。 Language Analyzer 针对特定语言的分词器。 Fingerprint Analyzer 指纹分析仪分词器，通过创建标记进行重复检测。 1.2 中文分词器在 Es 中，使用较多的中文分词器是 elasticsearch-analysis-ik，这个是 es 的一个第三方插件，代码托管在 GitHub 上： https://github.com/medcl/elasticsearch-analysis-ik 1.2.1 安装两种使用方式： 第一种： 首先打开分词器官网：https://github.com/medcl/elasticsearch-analysis-ik。 在 https://github.com/medcl/elasticsearch-analysis-ik/releases 页面找到最新的正式版，下载下来。我们这里的下载链接是 https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip。 将下载文件解压。 在 es/plugins 目录下，新建 ik 目录，并将解压后的所有文件拷贝到 ik 目录下。 重启 es 服务。 第二种： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip 1.2.2 测试es 重启成功后，首先创建一个名为 test 的索引： 接下来，在该索引中进行分词测试： 1.2.3 自定义扩展词库1.2.3.1 本地自定义在 es/plugins/ik/config 目录下，新建 ext.dic 文件（文件名任意），在该文件中可以配置自定义的词库。 如果有多个词，换行写入新词即可。 然后在 es/plugins/ik/config/IKAnalyzer.cfg.xml 中配置扩展词典的位置： 1.2.3.2 远程词库也可以配置远程词库，远程词库支持热更新（不用重启 es 就可以生效）。 热更新只需要提供一个接口，接口返回扩展词即可。 具体使用方式如下，新建一个 Spring Boot 项目，引入 Web 依赖即可。然后在 resources/stastic 目录下新建 ext.dic 文件，写入扩展词： 接下来，在 es/plugins/ik/config/IKAnalyzer.cfg.xml 文件中配置远程扩展词接口： 配置完成后，重启 es ，即可生效。 热更新，主要是响应头的 Last-Modified 或者 ETag 字段发生变化，ik 就会自动重新加载远程扩展辞典。 3. ElasticSearch 索引管理启动一个 master 节点和两个 slave 节点进行测试。 2.1 新建索引2.1.1 通过 head 插件新建索引在 head 插件中，选择 索引选项卡，然后点击新建索引。新建索引时，需要填入索引名称、分片数以及副本数。 索引创建成功后，如下图： 0、1、2、3、4 分别表示索引的分片，粗框表示主分片，细框表示副本（点一下框，通过 primary 属性可以查看是主分片还是副本）。.kibana 索引只有一个分片和一个副本，所以只有 0。 2.1.2 通过请求创建可以通过 postman 发送请求，也可以通过 kibana 发送请求，由于 kibana 有提示，所以这里采用 kibana。 创建索引请求： 1PUT book 创建成功后，可以查看索引信息： 需要注意两点： 索引名称不能有大写字母 索引名是唯一的，不能重复，重复创建会出错 2.2 更新索引索引创建好之后，可以修改其属性。 例如修改索引的副本数： 1234PUT book/_settings{ \"number_of_replicas\": 2} 修改成功后，如下： 更新分片数也是一样。 2.3 修改索引的读写权限索引创建成功后，可以向索引中写入文档： 1234PUT book/_doc/1{ \"title\":\"三国演义\"} 写入成功后，可以在 head 插件中查看： 默认情况下，索引是具备读写权限的，当然这个读写权限可以关闭。 例如，关闭索引的写权限： 1234PUT book/_settings{ \"blocks.write\": true} 关闭之后，就无法添加文档了。关闭了写权限之后，如果想要再次打开，方式如下： 1234PUT book/_settings{ \"blocks.write\": false} 其他类似的权限有： blocks.write blocks.read blocks.read_only 2.4 查看索引head 插件查看方式如下： 请求查看方式如下： 1GET book/_settings 也可以同时查看多个索引信息： 1GET book,test/_settings 也可以查看所有索引信息： 1GET book,test/_settings 2.5 删除索引head 插件可以删除索引： 请求删除如下： 1DELETE test 删除一个不存在的索引会报错。 5.6 索引打开/关闭关闭索引： 1POST book/_close 打开索引： 1POST book/_open 当然，可以同时关闭/打开多个索引，多个索引用 , 隔开，或者直接使用 _all 代表所有索引。 2.7 复制索引索引复制，只会复制数据，不会复制索引配置。 12345POST _reindex{ \"source\": {\"index\":\"book\"}, \"dest\": {\"index\":\"book_new\"}} 复制的时候，可以添加查询条件。 2.8 索引别名可以为索引创建别名，如果这个别名是唯一的，该别名可以代替索引名称。 1234567891011POST /_aliases{ \"actions\": [ { \"add\": { \"index\": \"book\", \"alias\": \"book_alias\" } } ]} 添加结果如下： 将 add 改为 remove 就表示移除别名： 1234567891011POST /_aliases{ \"actions\": [ { \"remove\": { \"index\": \"book\", \"alias\": \"book_alias\" } } ]} 查看某一个索引的别名： 1GET /book/_alias 查看某一个别名对应的索引（book_alias 表示一个别名）： 1GET /book_alias/_alias 可以查看集群上所有可用别名： 1GET /_alias 6.ElasticSearch 文档基本操作6.1创建文档首先新建一个索引 blog 然后向索引添加一个文档 123456PUT blog/_doc/1{ \"title\":\"ElasticSearch 文档基本操作\", \"data\":\"2021-04-09\", \"content\":\"### 6.1创建文档首先新建一个索引 blog\"} 1 表示新建文档的id 添加成功后响应的json如下： 123456789101112131415{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1} _index 表示文档的索引。 _type 表示文档的类型。 _id 表示文档的id。 _version 表示文档的版本(更新文档，版本会自动+1，针对一个文档的更新)。 result 表示执行结果。 _shards 表示分片信息。 _seq_no _primary_term 这两个也是版本控制用的(针对当前index) 添加成功后可以添加查看的文档： 添加文档是也可以不指定id，此时系统会默认给出一个id，如果不指定id，则需要使用POST请求，而不能使用PUT请求 1234{ \"error\" : \"Incorrect HTTP method for uri [/blog/_doc?pretty=true] and method [PUT], allowed: [POST]\", \"status\" : 405} 123456789101112{\"_index\": \"blog\",\"_type\": \"_doc\",\"_id\": \"5zULtngB3KliN6uQB99S\",\"_version\": 1,\"_score\": 1,\"_source\": {\"title\": \"666\",\"data\": \"2021-04-09\",\"content\": \"### 6.1创建文档首先新建一个索引 blog\"}} 6.2获取文档Es 中提供了GET API来查看存储在es中的文档。使用方式如下 1GET blog/_doc/5zULtngB3KliN6uQB99S 上面的命令表示获取id为 5zULtngB3KliN6uQB99S 的文档。 如果获取不存在的文档，会返回如下信息 123456{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"found\" : false} 如果仅仅只是想探测某个文档是否存在，可以使用head请求： 不存在的响应 存在的响应 也可以批量获取文档 1234GET blog/_mget{ \"ids\":[\"1\",\"5zULtngB3KliN6uQB99S\"]} GET 请求携带了请求体？ 某些特定请求，例如JS的HTTP请求库是不允许存在GET请求有请求体的，实际上在RFC7231文档中，并没有规定GET请求的请求体改如何处理，这造成了一定程度的混乱，有的HTTP服务器支持GET请求携带请求体，有的HTTP服务器则不支持。虽然ES工程师倾向于使用GET做查询，但是为了保证兼容性，ES同时也支持使用POST，例如上面的的批量查询案例也可以使用POST请求。 6.3文档更新文档更新一次，version就会自增1。 可以直接更新整个文档 1234PUT blog/_doc/5zULtngB3KliN6uQB99S{ \"title\":\"666\"} 这总方式，更新的文档会覆盖掉原有的文档 只想更新文档字段，可以通过脚本来实现 12345678910POST blog/_update/1{ \"script\": { \"lang\":\"painless\", \"source\":\"ctx._source.title=params.title\", \"params\":{ \"title\":\"666666\" } }} 更新的请求格式：POST{index}/_update/{id} 在脚本中lang表示脚本语言,painless是es内置的一种脚本语言，source表示具体执行的脚本，ctx是一个上下文对象，通过ctx可以访问到_source、_title等。 也可以通过同样的方式向文档中添加字段 1234567POST blog/_update/1{ \"script\": { \"lang\": \"painless\", \"source\": \"ctx._source.tags=[\\\"java\\\",\\\"php\\\"]\" }} 成功后的文档如下 12345678910111213141516171819{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 3, \"_seq_no\" : 2, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"title\" : \"666666\", \"data\" : \"2021-04-09\", \"content\" : \"### 6.1创建文档首先新建一个索引 blog\", \"tags\" : [ \"java\", \"php\" ] }} 7.ElasticSearch 文档路由es 是一个分布式系统，当我们存储一个文档到 es 上之后，这个文档实际上是被存储到 master 节点中的某一个主分片上。 例如新建一个索引，该索引有两个分片，0个副本，如下： 接下来，向该索引中保存一个文档： 1234PUT blog/_doc/a{ \"title\":\"a\"} 文档保存成功后，可以查看该文档被保存到哪个分片中去了： 1GET _cat/shards/blog?v 查看结果如下： 123index shard prirep state docs store ip nodeblog 1 p STARTED 0 208b 127.0.0.1 masterblog 0 p STARTED 0 208b 127.0.0.1 slave02 从这个结果中，可以看出，文档被保存到分片 0 中。 那么 es 中到底是按照什么样的规则去分配分片的？ es 中的路由机制是通过哈希算法，将具有相同哈希值的文档放到一个主分片中，分片位置的计算方式如下： shard=hash(routing) % number_of_primary_shards routing 可以是一个任意字符串，es 默认是将文档的 id 作为 routing 值，通过哈希函数根据 routing 生成一个数字，然后将该数字和分片数取余，取余的结果就是分片的位置。 默认的这种路由模式，最大的优势在于负载均衡，这种方式可以保证数据平均分配在不同的分片上。但是他有一个很大的劣势，就是查询时候无法确定文档的位置，此时它会将请求广播到所有的分片上去执行。另一方面，使用默认的路由模式，后期修改分片数量不方便。 当然开发者也可以自定义 routing 的值，方式如下： 1234PUT blog/_doc/d?routing=javaboy{ \"title\":\"d\"} 如果文档在添加时指定了 routing，则查询、删除、更新时也需要指定 routing。 1GET blog/_doc/d?routing=javaboy 自定义 routing 有可能会导致负载不均衡，这个还是要结合实际情况选择。 典型场景： 对于用户数据，我们可以将 userid 作为 routing，这样就能保证同一个用户的数据保存在同一个分片中，检索时，同样使用 userid 作为 routing，这样就可以精准的从某一个分片中获取数据。 8.ElasticSearch 并发的处理方式：锁和版本控制当我们使用 es 的 API 去进行文档更新时，它首先读取原文档出来，然后对原文档进行更新，更新完成后再重新索引整个文档。不论你执行多少次更新，最终保存在 es 中的是最后一次更新的文档。但是如果有两个线程同时去更新，就有可能出问题。 要解决问题，就是锁。 8.1 锁悲观锁 很悲观，每一次去读取数据的时候，都认为别人可能会修改数据，所以屏蔽一切可能破坏数据完整性的操作。关系型数据库中，悲观锁使用较多，例如行锁、表锁等等。 乐观锁 很乐观，每次读取数据时，都认为别人不会修改数据，因此也不锁定数据，只有在提交数据时，才会检查数据完整性。这种方式可以省去锁的开销，进而提高吞吐量。 在 es 中，实际上使用的就是乐观锁。 8.2 版本控制es6.7之前 在 es6.7 之前，使用 version+version_type 来进行乐观并发控制。根据前面的介绍，文档每被修改一个，version 就会自增一次，es 通过 version 字段来确保所有的操作都有序进行。 version 分为内部版本控制和外部版本控制。 8.2.1 内部版本es 自己维护的就是内部版本，当创建一个文档时，es 会给文档的版本赋值为 1。 每当用户修改一次文档，版本号就回自增 1。 如果使用内部版本，es 要求 version 参数的值必须和 es 文档中 version 的值相当，才能操作成功。 8.2.2 外部版本也可以维护外部版本。 在添加文档时，就指定版本号： 1234PUT blog/_doc/1?version=200&amp;version_type=external{ \"title\":\"2222\"} 以后更新的时候，版本要大于已有的版本号。 version_type=external 或者 version_type=external_gt 表示以后更新的时候，版本要大于已有的版本号。 version_type=external_gte 表示以后更新的时候，版本要大于等于已有的版本号。 8.2.3 最新方案（Es6.7 之后）现在使用 if_seq_no 和 if_primary_term 两个参数来做并发控制。 seq_no 不属于某一个文档，它是属于整个索引的（version 则是属于某一个文档的，每个文档的 version 互不影响）。现在更新文档时，使用 seq_no 来做并发。由于 seq_no 是属于整个 index 的，所以任何文档的修改或者新增，seq_no 都会自增。 现在就可以通过 seq_no 和 primary_term 来做乐观并发控制。 1234PUT blog/_doc/2?if_seq_no=5&amp;if_primary_term=1{ \"title\":\"6666\"} es倒排索引倒排索引是 es 中非常重要的索引结构，是从文档词项到文档 ID 的一个映射过程。 8.1 “正排索引”我们在关系型数据库中见到的索引，就是“正排索引”。 关系型数据库中的索引如下，假设我有一个博客表： 我们可以针对这个表建立索引（正排索引）： 当我们通过 id 或者标题去搜索文章时，就可以快速搜到。 但是如果我们按照文章内容的关键字去搜索，就只能去内容中做字符匹配了。为了提高查询效率，就要考虑使用倒排索引。 8.2 倒排索引倒排索引就是以内容的关键字建立索引，通过索引找到文档 id，再进而找到整个文档。 一般来说，倒排索引分为两个部分： 单词词典（记录所有的文档词项，以及词项到倒排列表的关联关系） 倒排列表（记录单词与对应的关系，由一系列倒排索引项组成，倒排索引项指：文档 id、词频（TF）（词项在文档中出现的次数，评分时使用）、位置（Position，词项在文档中分词的位置）、偏移（记录词项开始和结束的位置）） 当我们去索引一个文档时，就回建立倒排索引，搜索时，直接根据倒排索引搜索。 9.ElasticSearch 动态映射与静态映射映射就是 Mapping，它用来定义一个文档以及文档所包含的字段该如何被存储和索引。所以，它其实有点类似于关系型数据库中表的定义。 9.1 映射分类动态映射 顾名思义，就是自动创建出来的映射。es 根据存入的文档，自动分析出来文档中字段的类型以及存储方式，这种就是动态映射。 举一个简单例子，新建一个索引，然后查看索引信息： 在创建好的索引信息中，可以看到，mappings 为空，这个 mappings 中保存的就是映射信息。 现在我们向索引中添加一个文档，如下： 12345PUT blog/_doc/1{ \"title\":\"1111\", \"date\":\"2020-11-11\"} 文档添加成功后，就会自动生成 Mappings： 可以看到，date 字段的类型为 date，title 的类型有两个，text 和 keyword。 默认情况下，文档中如果新增了字段，mappings 中也会自动新增进来。 有的时候，如果希望新增字段时，能够抛出异常来提醒开发者，这个可以通过 mappings 中 dynamic 属性来配置。 dynamic 属性有三种取值： true，默认即此。自动添加新字段。 false，忽略新字段。 strict，严格模式，发现新字段会抛出异常。 具体配置方式如下，创建索引时指定 mappings（这其实就是静态映射）： 1234567891011121314PUT blog{ \"mappings\": { \"dynamic\":\"strict\", \"properties\": { \"title\":{ \"type\": \"text\" }, \"age\":{ \"type\":\"long\" } } }} 然后向 blog 中索引中添加数据： 123456PUT blog/_doc/2{ \"title\":\"1111\", \"date\":\"2020-11-11\", \"age\":99} 在添加的文档中，多出了一个 date 字段，而该字段没有预定义，所以这个添加操作就回报错： 12345678910111213{ \"error\" : { \"root_cause\" : [ { \"type\" : \"strict_dynamic_mapping_exception\", \"reason\" : \"mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed\" } ], \"type\" : \"strict_dynamic_mapping_exception\", \"reason\" : \"mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed\" }, \"status\" : 400} 动态映射还有一个日期检测的问题。 例如新建一个索引，然后添加一个含有日期的文档，如下： 1234PUT blog/_doc/1{ \"remark\":\"2020-11-11\"} 添加成功后，remark 字段会被推断是一个日期类型。 此时，remark 字段就无法存储其他类型了。 1234PUT blog/_doc/1{ \"remark\":\"javaboy\"} 此时报错如下： 123456789101112131415161718192021{ \"error\" : { \"root_cause\" : [ { \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'\" } ], \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'\", \"caused_by\" : { \"type\" : \"illegal_argument_exception\", \"reason\" : \"failed to parse date field [javaboy] with format [strict_date_optional_time||epoch_millis]\", \"caused_by\" : { \"type\" : \"date_time_parse_exception\", \"reason\" : \"Failed to parse with all enclosed parsers\" } } }, \"status\" : 400} 要解决这个问题，可以使用静态映射，即在索引定义时，将 remark 指定为 text 类型。也可以关闭日期检测。 123456PUT blog{ \"mappings\": { \"date_detection\": false }} 此时日期类型就回当成文本来处理。 静态映射 略。 9.2 类型推断es 中动态映射类型推断方式如下： 10.ElasticSearch 四种字段类型详解10.1 核心类型10.1.1 字符串类型 string：这是一个已经过期的字符串类型。在 es5 之前，用这个来描述字符串，现在的话，它已经被 text 和 keyword 替代了。 text：如果一个字段是要被全文检索的，比如说博客内容、新闻内容、产品描述，那么可以使用 text。用了 text 之后，字段内容会被分析，在生成倒排索引之前，字符串会被分词器分成一个个词项。text 类型的字段不用于排序，很少用于聚合。这种字符串也被称为 analyzed 字段。 keyword：这种类型适用于结构化的字段，例如标签、email 地址、手机号码等等，这种类型的字段可以用作过滤、排序、聚合等。这种字符串也称之为 not-analyzed 字段。 10.1.2 数字类型 在满足需求的情况下，优先使用范围小的字段。字段长度越短，索引和搜索的效率越高。 浮点数，优先考虑使用 scaled_float。通过缩放因子（底层将一个浮点数变为整数和一个缩放倍数，用来节省空间）将浮点数缩放，更好的空间利用 在使用scaled_float时，需要制定缩放因子scaled_float scaled_float 举例： 1234567891011121314PUT product{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\" }, \"price\":{ \"type\": \"scaled_float\", \"scaling_factor\": 100 } } }} 10.1.3 日期类型由于 JSON 中没有日期类型，所以 es 中的日期类型形式就比较多样： 2020-11-11 或者 2020-11-11 11:11:11 一个从 1970.1.1 零点到现在的一个秒数或者毫秒数。 es 内部将时间转为 UTC，然后将时间按照 millseconds-since-the-epoch 的长整型来存储。 自定义日期类型： 12345678910PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" } } }} 这个能够解析出来的时间格式比较多。 123456789101112131415PUT product/_doc/1{ \"date\":\"2020-11-11\"}PUT product/_doc/2{ \"date\":\"2020-11-11T11:11:11Z\"}PUT product/_doc/3{ \"date\":\"1604672099958\"} 上面三个文档中的日期都可以被解析，内部存储的是毫秒计时的长整型数。 10.1.4 布尔类型（boolean）JSON 中的 “true”、“false”、true、false 都可以。 10.1.5 二进制类型（binary）二进制接受的是 base64 编码的字符串，默认不存储，也不可搜索。 10.1.6 范围类型 integer_range float_range long_range double_range date_range ip_range 定义的时候，指定范围类型即可： 12345678910111213PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" }, \"price\":{ \"type\":\"float_range\" } } }} 插入文档的时候，需要指定范围的界限： 12345678910111213PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" }, \"price\":{ \"type\":\"float_range\" } } }} 指定范围的时，可以使用 gt、gte、lt、lte。 10.2 复合类型10.2.1 数组类型es 中没有专门的数组类型。默认情况下，任何字段都可以有一个或者多个值。需要注意的是，数组中的元素必须是同一种类型。 添加数组是，数组中的第一个元素决定了整个数组的类型。 10.2.2 对象类型（object）由于 JSON 本身具有层级关系，所以文档包含内部对象。内部对象中，还可以再包含内部对象。 1234567PUT product/_doc/2{ \"date\":\"2020-11-11T11:11:11Z\", \"ext_info\":{ \"address\":\"China\" }} 10.2.3 嵌套类型（nested）nested 是 object 中的一个特例。 如果使用 object 类型，假如有如下一个文档： 123456789101112{ \"user\":[ { \"first\":\"Zhang\", \"last\":\"san\" }, { \"first\":\"Li\", \"last\":\"si\" } ]} 由于 Lucene 没有内部对象的概念，所以 es 会将对象层次扁平化，将一个对象转为字段名和值构成的简单列表。即上面的文档，最终存储形式如下： 1234{\"user.first\":[\"Zhang\",\"Li\"],\"user.last\":[\"san\",\"si\"]} 扁平化之后，用户名之间的关系没了。这样会导致如果搜索 Zhang si 这个人，会搜索到。 此时可以 nested 类型来解决问题，nested 对象类型可以保持数组中每个对象的独立性。nested 类型将数组中的每一饿对象作为独立隐藏文档来索引，这样每一个嵌套对象都可以独立被索引。 {{“user.first”:”Zhang”,“user.last”:”san”},{“user.first”:”Li”,“user.last”:”si”}} 优点 文档存储在一起，读取性能高。 缺点 更新父或者子文档时需要更新更个文档。 10.3 地理类型使用场景： 查找某一个范围内的地理位置 通过地理位置或者相对中心点的距离来聚合文档 把距离整个到文档的评分中 通过距离对文档进行排序 10.3.1 geo_pointgeo_point 就是一个坐标点，定义方式如下： 12345678910PUT people{ \"mappings\": { \"properties\": { \"location\":{ \"type\": \"geo_point\" } } }} 创建时指定字段类型，存储的时候，有四种方式： 1234567891011121314151617181920212223//objectPUT people/_doc/1{ \"location\":{ \"lat\": 34.27, \"lon\": 108.94 }}//字符串PUT people/_doc/2{ \"location\":\"34.27,108.94\"}//经纬度的hash值PUT people/_doc/3{ \"location\":\"uzbrgzfxuzup\"}//数组 经度在前，纬度在后PUT people/_doc/4{ \"location\":[108.94,34.27]} 注意，使用数组描述，先经度后纬度。 地址位置转 geo_hash：http://www.csxgame.top/#/ 10.3.2 geo_shape 指定 geo_shape 类型： 12345678910PUT people{ \"mappings\": { \"properties\": { \"location\":{ \"type\": \"geo_shape\" } } }} 添加文档时需要指定具体的类型： 1234567PUT people/_doc/1{ \"location\":{ \"type\":\"point\", \"coordinates\": [108.94,34.27] }} 如果是 linestring，如下： 1234567PUT people/_doc/2{ \"location\":{ \"type\":\"linestring\", \"coordinates\": [[108.94,34.27],[100,33]] }} 10.4 特殊类型10.4.1 IP存储 IP 地址，类型是 ip： 12345678910PUT blog{ \"mappings\": { \"properties\": { \"address\":{ \"type\": \"ip\" } } }} 添加文档： 1234PUT blog/_doc/1{ \"address\":\"192.168.91.1\"} 搜索文档： 12345678GET blog/_search{ \"query\": { \"term\": { \"address\": \"192.168.0.0/16\" } }} 10.4.2 token_count用于统计字符串分词后的词项个数。 12345678910111213141516PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"fields\": { \"length\":{ \"type\":\"token_count\", \"analyzer\":\"standard\" } } } } }} 相当于新增了 title.length 字段用来统计分词后词项的个数。 添加文档： 1234PUT blog/_doc/1{ \"title\":\"zhang san\"} 可以通过 token_count 去查询： 12345678GET blog/_search{ \"query\": { \"term\": { \"title.length\": 2 } }} 11.ElasticSearch 23 种映射参数详解11.1 analyzer定义文本字段的分词器。默认对索引和查询都是有效的。 假设不用分词器，我们先来看一下索引的结果，创建一个索引并添加一个文档： 123456PUT blogPUT blog/_doc/1{ \"title\":\"定义文本字段的分词器。默认对索引和查询都是有效的。\"} 查看词条向量（term vectors） 1234GET blog/_termvectors/1{ \"fields\": [\"title\"]} 查看结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"took\" : 0, \"term_vectors\" : { \"title\" : { \"field_statistics\" : { \"sum_doc_freq\" : 22, \"doc_count\" : 1, \"sum_ttf\" : 23 }, \"terms\" : { \"义\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 1, \"start_offset\" : 1, \"end_offset\" : 2 } ] }, \"分\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 7, \"start_offset\" : 7, \"end_offset\" : 8 } ] }, \"和\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 15, \"start_offset\" : 16, \"end_offset\" : 17 } ] }, \"器\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 9, \"start_offset\" : 9, \"end_offset\" : 10 } ] }, \"字\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 4, \"start_offset\" : 4, \"end_offset\" : 5 } ] }, \"定\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 0, \"start_offset\" : 0, \"end_offset\" : 1 } ] }, \"对\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 12, \"start_offset\" : 13, \"end_offset\" : 14 } ] }, \"引\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 14, \"start_offset\" : 15, \"end_offset\" : 16 } ] }, \"效\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 21, \"start_offset\" : 22, \"end_offset\" : 23 } ] }, \"文\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 2, \"start_offset\" : 2, \"end_offset\" : 3 } ] }, \"是\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 19, \"start_offset\" : 20, \"end_offset\" : 21 } ] }, \"有\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 20, \"start_offset\" : 21, \"end_offset\" : 22 } ] }, \"本\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 3, \"start_offset\" : 3, \"end_offset\" : 4 } ] }, \"查\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 16, \"start_offset\" : 17, \"end_offset\" : 18 } ] }, \"段\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 5, \"start_offset\" : 5, \"end_offset\" : 6 } ] }, \"的\" : { \"term_freq\" : 2, \"tokens\" : [ { \"position\" : 6, \"start_offset\" : 6, \"end_offset\" : 7 }, { \"position\" : 22, \"start_offset\" : 23, \"end_offset\" : 24 } ] }, \"索\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 13, \"start_offset\" : 14, \"end_offset\" : 15 } ] }, \"认\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 11, \"start_offset\" : 12, \"end_offset\" : 13 } ] }, \"词\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 8, \"start_offset\" : 8, \"end_offset\" : 9 } ] }, \"询\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 17, \"start_offset\" : 18, \"end_offset\" : 19 } ] }, \"都\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 18, \"start_offset\" : 19, \"end_offset\" : 20 } ] }, \"默\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 10, \"start_offset\" : 11, \"end_offset\" : 12 } ] } } } }} 可以看到，默认情况下，中文就是一个字一个字的分，这种分词方式没有任何意义。如果这样分词，查询就只能按照一个字一个字来查，像下面这样： 12345678GET blog/_search{ \"query\": { \"term\": { \"title\": \"定\" } }} 无意义！！！ 所以，我们要根据实际情况，配置合适的分词器。 给字段设定分词器： 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\":\"text\", \"analyzer\": \"ik_smart\" } } }} 存储文档： 1234PUT blog/_doc/1{ \"title\":\"定义文本字段的分词器。默认对索引和查询都是有效的。\"} 查看词条向量： 1234GET blog/_termvectors/1{ \"fields\": [\"title\"]} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"took\" : 1, \"term_vectors\" : { \"title\" : { \"field_statistics\" : { \"sum_doc_freq\" : 12, \"doc_count\" : 1, \"sum_ttf\" : 13 }, \"terms\" : { \"分词器\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 4, \"start_offset\" : 7, \"end_offset\" : 10 } ] }, \"和\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 8, \"start_offset\" : 16, \"end_offset\" : 17 } ] }, \"字段\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 2, \"start_offset\" : 4, \"end_offset\" : 6 } ] }, \"定义\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 0, \"start_offset\" : 0, \"end_offset\" : 2 } ] }, \"对\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 6, \"start_offset\" : 13, \"end_offset\" : 14 } ] }, \"文本\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 1, \"start_offset\" : 2, \"end_offset\" : 4 } ] }, \"有效\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 11, \"start_offset\" : 21, \"end_offset\" : 23 } ] }, \"查询\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 9, \"start_offset\" : 17, \"end_offset\" : 19 } ] }, \"的\" : { \"term_freq\" : 2, \"tokens\" : [ { \"position\" : 3, \"start_offset\" : 6, \"end_offset\" : 7 }, { \"position\" : 12, \"start_offset\" : 23, \"end_offset\" : 24 } ] }, \"索引\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 7, \"start_offset\" : 14, \"end_offset\" : 16 } ] }, \"都是\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 10, \"start_offset\" : 19, \"end_offset\" : 21 } ] }, \"默认\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 5, \"start_offset\" : 11, \"end_offset\" : 13 } ] } } } }} 然后就可以通过词去搜索了： 12345678GET blog/_search{ \"query\": { \"term\": { \"title\": \"索引\" } }} 11.2 search_analyzer查询时候的分词器。默认情况下，如果没有配置 search_analyzer，则查询时，首先查看有没有 search_analyzer，有的话，就用 search_analyzer 来进行分词，如果没有，则看有没有 analyzer，如果有，则用 analyzer 来进行分词，否则使用 es 默认的分词器。 11.3 normalizernormalizer 参数用于解析前（索引或者查询）的标准化配置。 比如，在 es 中，对于一些我们不想切分的字符串，我们通常会将其设置为 keyword，搜索时候也是使用整个词进行搜索。如果在索引前没有做好数据清洗，导致大小写不一致，例如 javaboy 和 JAVABOY，此时，我们就可以使用 normalizer 在索引之前以及查询之前进行文档的标准化。 先来一个反例，创建一个名为 blog 的索引，设置 author 字段类型为 keyword： 12345678910PUT blog{ \"mappings\": { \"properties\": { \"author\":{ \"type\": \"keyword\" } } }} 添加两个文档： 123456789PUT blog/_doc/1{ \"author\":\"javaboy\"}PUT blog/_doc/2{ \"author\":\"JAVABOY\"} 然后进行搜索： 12345678GET blog/_search{ \"query\": { \"term\": { \"author\": \"JAVABOY\" } }} 大写关键字可以搜到大写的文档，小写关键字可以搜到小写的文档。 如果使用了 normalizer，可以在索引和查询时，分别对文档进行预处理。 normalizer 定义方式如下： 123456789101112131415161718192021PUT blog{ \"settings\": { \"analysis\": { \"normalizer\":{ \"my_normalizer\":{ \"type\":\"custom\", \"filter\":[\"lowercase\"] } } } }, \"mappings\": { \"properties\": { \"author\":{ \"type\": \"keyword\", \"normalizer\":\"my_normalizer\" } } }} 在 settings 中定义 normalizer，然后在 mappings 中引用。 测试方式和前面一致。此时查询的时候，大写关键字也可以查询到小写文档，因为无论是索引还是查询，都会将大写转为小写。 11.4 boostboost 参数可以设置字段的权重。 默认权重为1 boost 有两种使用思路，一种就是在定义 mappings 的时候使用，在指定字段类型时使用；另一种就是在查询时使用。 实际开发中建议使用后者，前者有问题：如果不重新索引文档，权重无法修改。 只支持 term 查询 (不支持prefix，range，fuzzy查询) mapping 中使用 boost（不推荐）：7.12出现提示将在8.0移除 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"content\":{ \"type\": \"text\", \"boost\": 2 } } }} 另一种方式就是在查询的时候，指定 boost 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"content\":{ \"type\": \"text\" , \"analyzer\": \"ik_smart\" } } }} 1234POST blog/_doc{ \"content\":\"你好测试的人,你好测试的人,你好测试的人\"} 1234567891011GET blog/_search{ \"query\": { \"match\": { \"content\": { \"query\": \"你好\", \"boost\": 2 } } }} 为何在建索引时加权重是一个不好的行为？ 1.如果不重新索引文档，权重无法修改。 2.在查询时可以调整权重而不需要重新索引 3.在索引添加权重会占用一个字节的空间，This reduces the resolution of the field length normalization factor which can lead to lower quality relevance calculations.(这一段没看懂) 11.5 coercecoerce 用来清除脏数据，默认为 true。 比如一个数字 5 ，一般是integer，但是可能是String “5”，也可能是是浮点数 float 5.0, 甚至 “5.0”String 123456789101112131415161718192021222324PUT blog{ \"mappings\": { \"properties\": { \"number_one\":{ \"type\": \"integer\" }, \"number_two\":{ \"type\": \"integer\", \"coerce\": false } } }}PUT blog/_doc/1{ \"number_one\":\"10\"}PUT blog/_doc/2{ \"number_two\":\"10\"} 可以通过update mapping api 让coerce 更新已有的字段 同样可以在建索引时全局禁用coercion并启用部分 123456789101112131415161718192021222324252627PUT my-index-000001{ \"settings\": { \"index.mapping.coerce\": false }, \"mappings\": { \"properties\": { \"number_one\": { \"type\": \"integer\", \"coerce\": true }, \"number_two\": { \"type\": \"integer\" } } }}PUT my-index-000001/_doc/1{ \"number_one\": \"10\" } PUT my-index-000001/_doc/2{ \"number_two\": \"10\" } 11.6 copy_to将多个字段的值复制到一个字段中 123456789101112131415161718192021222324252627282930313233343536PUT my-index-000001{ \"mappings\": { \"properties\": { \"first_name\": { \"type\": \"text\", \"copy_to\": \"full_name\" }, \"last_name\": { \"type\": \"text\", \"copy_to\": \"full_name\" }, \"full_name\": { \"type\": \"text\" } } }}PUT my-index-000001/_doc/1{ \"first_name\": \"John\", \"last_name\": \"Smith\"}GET my-index-000001/_search{ \"query\": { \"match\": { \"full_name\": { \"query\": \"John Smith\", \"operator\": \"and\" } } }} 11.7 doc_values 和 fielddataes 中的搜索主要是用到倒排索引，doc_values 参数是为了加快排序、聚合操作而生的。当建立倒排索引的时候，会额外增加列式存储映射。 doc_values 默认是开启的，如果确定某个字段不需要排序或者不需要聚合，那么可以关闭 doc_values。 大部分的字段在索引时都会生成 doc_values，除了 text。text 字段在查询时会生成一个 fielddata 的数据结构，fieldata 在字段首次被聚合、排序的时候生成。 doc_value fielddata 索引时创建 使用时动态创建 磁盘 内存 不占用内存 不占用磁盘 索引速度稍微低一点 文档很多时，动态创建慢，占内存 doc_values 默认开启，fielddata 默认关闭。 doc_values 演示： 1234567891011121314151617181920212223242526272829303132333435PUT usersPUT users/_doc/1{ \"age\":100}PUT users/_doc/2{ \"age\":99}PUT users/_doc/3{ \"age\":98}PUT users/_doc/4{ \"age\":101}GET users/_search{ \"query\": { \"match_all\": {} }, \"sort\":[ { \"age\":{ \"order\": \"desc\" } } ]} 由于 doc_values 默认时开启的，所以可以直接使用该字段排序，如果想关闭 doc_values ，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT users{ \"mappings\": { \"properties\": { \"age\":{ \"type\": \"integer\", \"doc_values\": false } } }}PUT users/_doc/1{ \"age\":100}PUT users/_doc/2{ \"age\":99}PUT users/_doc/3{ \"age\":98}PUT users/_doc/4{ \"age\":101}GET users/_search{ \"query\": { \"match_all\": {} }, \"sort\":[ { \"age\":{ \"order\": \"desc\" } } ]} 11.8 dynamic当对包含新字段的文档建立索引时，ElasticSearch会将字段动态添加到文档或文档中的内部对象。 内部对象从其父对象或映射类型继承动态设置。在类型级别禁用了动态映射，因此不会动态的添加新的顶级字段。 dynamic 可选参数 param description true 默认 runtime 新字段作为运行时字段添加到映射中。未建立索引，而是在查询时从_source加载 false 新字段会被忽略，不能被索引和搜索，但仍会出现在返回的匹配的_source字段中，不会添加到映射中，必须显式添加新字段 strict 如果检测到新字段，会引发异常并拒绝，必须显式添加新字段 11.9 enabledes 默认会索引所有的字段，但是有的字段可能只需要存储，不需要索引。此时可以通过 enabled 字段来控制： 只能应用于顶级映射定义和对象字段。仍然可以从_source字段中检索JSON，但无法搜索或以其他任何方式存储； 已经存在的字段和顶级映射无法更新enabled 可以将非对象字段添加到禁用字段 12345678910111213141516PUT my-index-000001{ \"mappings\": { \"properties\": { \"session_data\": { \"type\": \"object\", \"enabled\": false } } }}PUT my-index-000001/_doc/session_1{ \"session_data\": \"foo bar\" } 返回的结果为 1234567891011121314{ \"_index\" : \"my-index-000001\", \"_type\" : \"_doc\", \"_id\" : \"session_1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1} 11.10 format日期格式。format 可以规范日期格式，而且一次可以定义多个 format。 123456789101112131415161718192021PUT users{ \"mappings\": { \"properties\": { \"birthday\":{ \"type\": \"date\", \"format\": \"yyyy-MM-dd||yyyy-MM-dd HH:mm:ss\" } } }}PUT users/_doc/1{ \"birthday\":\"2020-11-11\"}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11\"} 多个日期格式之间，使用 || 符号连接，注意没有空格。 如果用户没有指定日期的 format，默认的日期格式是 strict_date_optional_time||epoch_mills 另外，所有的日期格式，可以在 https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 网址查看。 11.11 ignore_aboveigbore_above 用于指定分词和索引的字符串最大长度，超过最大长度的话，该字段将不会被索引，这个字段只适用于 keyword 类型。 123456789101112131415161718192021222324252627282930PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"keyword\", \"ignore_above\": 10 } } }}PUT blog/_doc/1{ \"title\":\"javaboy\"}PUT blog/_doc/2{ \"title\":\"javaboyjavaboyjavaboy\"}GET blog/_search{ \"query\": { \"term\": { \"title\": \"javaboyjavaboyjavaboy\" } }} 10.12 ignore_malformedignore_malformed 可以忽略不规则的数据，该参数默认为 false。 12345678910111213141516171819202122232425262728293031323334PUT users{ \"mappings\": { \"properties\": { \"birthday\":{ \"type\": \"date\", \"format\": \"yyyy-MM-dd||yyyy-MM-dd HH:mm:ss\" }, \"age\":{ \"type\": \"integer\", \"ignore_malformed\": true } } }}PUT users/_doc/1{ \"birthday\":\"2020-11-11\", \"age\":99}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11\", \"age\":\"abc\"}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11aaa\", \"age\":\"abc\"} 10.13 include_in_all这个是针对 _all 字段的，但是在 es7 中，该字段已经被废弃了。 10.14 indexindex 属性指定一个字段是否被索引，该属性为 true 表示字段被索引，false 表示字段不被索引。 12345678910111213141516171819202122232425PUT users{ \"mappings\": { \"properties\": { \"age\":{ \"type\": \"integer\", \"index\": false } } }}PUT users/_doc/1{ \"age\":99}GET users/_search{ \"query\": { \"term\": { \"age\": 99 } }} 如果 index 为 false，则不能通过对应的字段搜索。 10.15 index_optionsindex_options 控制索引时哪些信息被存储到倒排索引中（用在 text 字段中），有四种取值： index_options 备注 docs 只存储文档编号，默认 freqs 在doc基础上，存储词项频率 positions 在freqs基础上，存储词项偏移位置 offsets 在positions基础上，存储词项开始和结束的字符位置 123456789101112131415161718192021222324252627282930313233//这段没怎么懂PUT my-index-000001{ \"mappings\": { \"properties\": { \"text\": { \"type\": \"text\", \"index_options\": \"offsets\" } } }}PUT my-index-000001/_doc/1{ \"text\": \"Quick brown fox\"}GET my-index-000001/_search{ \"query\": { \"match\": { \"text\": \"brown fox\" } }, \"highlight\": { \"fields\": { \"text\": {} } }} 10.16 normsnorms 对字段评分有用，text 默认开启 norms，如果不是特别需要，不要开启 norms。 10.17 null_value在 es 中，值为 null 的字段不索引也不可以被搜索，null_value 可以让值为 null 的字段显式的可索引、可搜索： 1234567891011121314151617181920212223242526PUT users{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\", \"null_value\": \"javaboy_null\" /*这个里面的value有什么意义呢*/ } } }}PUT users/_doc/1{ \"name\":null, \"age\":99}GET users/_search{ \"query\": { \"term\": { \"name\": \"javaboy_null\" } }} 10.18 position_increment_gap被解析的 text 字段会将 term 的位置考虑进去，目的是为了支持近似查询和短语查询，当我们去索引一个含有多个值的 text 字段时，会在各个值之间添加一个假想的空间，将值隔开，这样就可以有效避免一些无意义的短语匹配，间隙大小通过 position_increment_gap 来控制，默认是 100。 123456789101112131415161718192021222324252627PUT my-index-000001/_doc/1{ \"names\": [ \"John Abraham\", \"Lincoln Smith\"]}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": { \"query\": \"Abraham Lincoln\" } } }}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": { \"query\": \"Abraham Lincoln\", \"slop\": 101 } } }} This phrase query matches our document, even though Abraham and Lincoln are in separate strings, because slop &gt; position_increment_gap. 两个get请求两种返回 123456789101112131415161718{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }} 12345678910111213141516171819202122232425262728293031{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.010358453, \"hits\" : [ { \"_index\" : \"my-index-000001\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.010358453, \"_source\" : { \"names\" : [ \"John Abraham\", \"Lincoln Smith\" ] } } ] }} 可以在映射中指定position_increment_gap 12345678910111213141516171819202122232425PUT my-index-000001{ \"mappings\": { \"properties\": { \"names\": { \"type\": \"text\", \"position_increment_gap\": 0 } } }}PUT my-index-000001/_doc/1{ \"names\": [ \"John Abraham\", \"Lincoln Smith\"]}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": \"Abraham Lincoln\" } }} 10.19 properties可以在查询，聚合等中使用点表示法来引用内部字段： 1234567891011121314151617181920212223GET my-index-000001/_search{ \"query\": { \"match\": { \"manager.name\": \"Alice White\" } }, \"aggs\": { \"Employees\": { \"nested\": { \"path\": \"employees\" }, \"aggs\": { \"Employee Ages\": { \"histogram\": { \"field\": \"employees.age\", \"interval\": 5 } } } } }} 10.20 similaritysimilarity 指定文档的评分模型，默认有三种： similarity 备注 BM25 es和lucene默认的评分模型 classic TF/IDF评分 boolean boolean模型评分 10.21 store默认情况下，字段会被索引，也可以搜索，但是不会存储，虽然不会被存储的，但是 _source 中有一个字段的备份。如果想将字段存储下来，可以通过配置 store 来实现。 123456789101112131415161718192021222324252627282930PUT my-index-000001{ \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"store\": true }, \"date\": { \"type\": \"date\", \"store\": true }, \"content\": { \"type\": \"text\" } } }}PUT my-index-000001/_doc/1{ \"title\": \"Some short title\", \"date\": \"2015-01-01\", \"content\": \"A very long content field...\"}GET my-index-000001/_search{ \"stored_fields\": [ \"title\", \"date\" ] } 10.22 term_vectorsterm_vectors 是通过分词器产生的信息，包括： 一组 terms 每个 term 的位置 term 的首字符/尾字符与原始字符串原点的偏移量 term_vectors 取值： 取值 备注 no 不存储信息，默认 yes term被存储 with_positions 在yes的基础上增加位置信息 with_offset 在yes的基础上增加偏移信息 with_positions_offsets term，positons，offsets都存储 with_positions_payloads with_positions_offsets_payloads 设置 with_postions_offsets将使字段索引大小翻倍 11.23 fieldsfields 参数可以让同一字段有多种不同的索引方式。例如： 1234567891011121314151617181920212223242526272829PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"fields\": { \"raw\":{ \"type\":\"keyword\" } } } } }}PUT blog/_doc/1{ \"title\":\"javaboy\"}GET blog/_search{ \"query\": { \"term\": { \"title.raw\": \"javaboy\" } }} 12.映射模板13.ElasticSearch 搜索数据导入 下载脚本bookdata.json。链接: https://pan.baidu.com/s/12Gj5aovYKI5g2X8pPJZtLw 提取码: a5h4 创建索引： 1234567891011121314151617181920212223242526272829PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 执行如下脚本导入命令： 1curl -XPOST \"http://localhost:9200/books/_bulk?pretty\" -H \"content-type:application/json\" --data-binary @bookdata.json 14.ElasticSearch 搜索入门搜索分为两个过程： 当向索引中保存文档时，默认情况下，es 会保存两份内容，一份是 _source 中的数据，另一份则是通过分词、排序等一系列过程生成的倒排索引文件，倒排索引中保存了词项和文档之间的对应关系。 搜索时，当 es 接收到用户的搜索请求之后，就会去倒排索引中查询，通过的倒排索引中维护的倒排记录表找到关键词对应的文档集合，然后对文档进行评分、排序、高亮等处理，处理完成后返回文档。 14.1 简单搜索查询文档： 123456GET books/_search{ \"query\": { \"match_all\": {} }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159{ \"took\" : 19, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 38, \"relation\" : \"eq\" }, \"max_score\" : 4.217799, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"279\", \"_score\" : 4.217799, \"_source\" : { \"name\" : \"计算机应用基础\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机理论、基础知识\", \"author\" : \"张宇，赖麟\", \"info\" : \"《计算机应用基础》是国家精品课程“计算机文件基础”的配套教材。全书按照工学结合人才培养模式的要求，以培养能力为目标，基于工作过程组织课程；以典型的工作任务为载体，采用任务驱动的方式来构造知识和技能平台，强调理论和训练一体化，做到“教、学、做”相结合，让学生对知识有整体认识，即按照“先行后知、先学后教”的思想编写。全书内容包括：计算机基础知识、WnwsXP操作系统、McsfOffc2003办公自动化软件、计算机网络基础。《计算机应用基础》的显著特点是以学生为主体，通过实际工作过程中的典型工作任务来训练学生，培养学生解决和处理实际问题的能力，将被动学习转变为主动学习，突出学生能力的培养，更加符合职业技术教育的特点和规律。《计算机应用基础》适合作为普通高等院校和高职高专院校“计算机应用基础”课程的教材，也可作为计算机初学者的入门参考书。\", \"price\" : 29 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"298\", \"_score\" : 3.8557193, \"_source\" : { \"name\" : \"高等学校大学计算机基础课程系列教材：大学计算机基础教程\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"\", \"info\" : \"本书作为高等学校非计算机专业计算机基础课程群（1+X）第一门课程的主教材，主要介绍计算机基础知识和应用技能。本书共包含7章：计算机基础知识、操作系统用户界面及使用、办公自动化软件及使用、多媒体技术基础及应用、网络技术基础及应用、网页设计与制作、数据库技术基础及应用。每章都结合通用的软件版本进行讲解，同时为了帮助学生加深对所学知识的理解，还配备了大量习题。作为国家精品课程主讲教材，本书配有丰富的教学资源，包括多媒体教学课件、课程实验系统、上机练习和考试评价系统、教学素材等计算机辅助教学软件，还有功能完善的教学专用网站。本书可作为高等学校学生学习第一门计算机课程的教材，也可作为计算机爱好者的自学读本。\", \"price\" : 22 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"789\", \"_score\" : 3.58548, \"_source\" : { \"name\" : \"高等学校文科计算机课程系列教材：计算机平面设计技术（CorelDRAW与Photoshop）（附光盘）\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"\", \"info\" : \"《计算机平面设计技术――CDRAW与Ps》是根据教育部高等学校文科类专业计算机课程教学基本要求（美术类）而编写的。《计算机平面设计技术――CDRAW与Ps》系统全面地介绍了平面设计艺术专业最常用、最基本、最普及、最成熟的两个应用软件――CDRAW与Ps，结合实际的设计案例，讲授平面图形的造型方法与技巧、图像的处理方法与技巧、跨程序编辑的方法和意义等知识与技术。教材浓缩了其重要的工具和功能的使用方法，可作为平面设计专业的计算机应用基础课程的教材，也可作为学习计算机平面设计技术的参考。\", \"price\" : 28 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"282\", \"_score\" : 3.4969957, \"_source\" : { \"name\" : \"计算机文化基础（Windows XP＋Office2003）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机理论、基础知识\", \"author\" : \"赵秀英\", \"info\" : \"《计算机文化基础(WnwsXP+Offc2003\", \"price\" : 0 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"783\", \"_score\" : 3.3814218, \"_source\" : { \"name\" : \"计算机科学与技术丛书：现代语音编码技术\", \"publish\" : \"科学出版社\", \"type\" : \"电子与通信\", \"author\" : \"\", \"info\" : \"本书全面、系统地阐述了现代语音编码的原理、技术及应用。 本书分为12章，第一章介绍现代语音编码的基础知识和基本概念，第二章阐述标量量化和矢量量化原理，这两章是语音编码的入门知识；第三章至第七章讨论广泛应用的几种重要的现代语音编码技术原理、系统组成、实用技术及编码标准，是本书的重点；第八章至第十章介绍目前语音编码的最新的一些研究课题及其进展；第十一章、第十二章讨论语音编码的今后发展趋势和方向。主要内容有：语音编码导论、量化原理、时域波形编码技术、频域波形编码技术、变换域波形编码技术、参数编码技术、混合编码技术、极低速率语音编码、宽频带高音质声频编码、第三代移动通信的语音编码、信源-信道联合编码、软件无线电技术在语音编码中的应用。 本书内容丰富、取材新颖、阐述清晰、结构合理、实用性强，包含有最近二十几年来现代语音编码技术的许多新成果和新进展，是一本很好的关于语音编码原理、技术及应用的教科书和参考书。\", \"price\" : 31 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"465\", \"_score\" : 3.3506408, \"_source\" : { \"name\" : \"国家精品课程主讲教材・高等学校大学计算机基础课程系列教材：大学计算机基础实践教程\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"詹国华\", \"info\" : \"《大学计算机基础实践教程》作为高等学校非计算机专业计算机基础课程群（1+X）第一门课程主教材《大学计算机基础教程》的配套实验教材，精心设计了一组综合性、应用性实验，注重实践技能的培养和理论知识的渗透。《大学计算机基础实践教程》共分7章：硬件连接和汉字输入实验、Wnws操作实验、办公软件操作实验、多媒体基础实验、因特网操作实验、网页设计与制作实验、Accss数据库操作实验。作为国家精品课程主讲教材的配套用书，《大学计算机基础实践教程》提供了丰富的教学资源，包括多媒体教学课件、课程实验系统、上机练习和考试评价系统、教学素材等计算机辅助教学软件，还有功能完善的课程教学网站。《大学计算机基础实践教程》可作为高等学校学生学习第一门计算机基础课程的配套教材，也可作为计算机爱好者的自学读本。\", \"price\" : 17 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"429\", \"_score\" : 3.1717708, \"_source\" : { \"name\" : \"微型计算机原理与接口技术（第2版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机组织与体系结构\", \"author\" : \"尹建华\", \"info\" : \"《微型计算机原理与接口技术》以In系列微处理器为背景，以16位微处理器8086为核心，追踪In主流系列高性能微型计算机萼术的发展方向，全面讲述微型计算机系统的基本组成、工作原理、硬件接口技术和典型应用，在此基础上介绍80386，80486和Pn等高档微处理器的发展和特点。使学生系统掌握汇编语言程序设计的基本方法和硬件接口技术，建立微型计算机系统的整体概念，并且使之具有微型计算机软件及硬件初步开发、设计的能力。为使于教师授课和学生学习，《微型计算机原理与接口技术》配备了多媒体CAI课件。全书共11章。主要内容包括：微型计算机基础知识、80x86CPU、微型计算机指令系统、汇编语言程序设计、存储器及其与CPU的接口、输入／输出接口及中断技术、总线和总线标准、常用可编程并行数字接口芯片及其应用、串行通信接口及总线标准、模拟接口技术、常用外设和人机交互接口。《微型计算机原理与接口技术》可作为高等学校工科非计算机专业微型计算机原理及应用课程的教材，也可供从事微型计算机硬件和软件设计的工程技术人员参考。\", \"price\" : 54 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"70\", \"_score\" : 2.9018917, \"_source\" : { \"name\" : \"计算机基础课程系列教材：数据库技术及应用 SQL Server\", \"publish\" : \"高等教育出版社\", \"type\" : \"数据库\", \"author\" : \"李雁翎\", \"info\" : \"SQLSv是一种典型的数据库管理系统，是目前深受广大用户欢迎的数据库应用开发平台。它适应网络技术环境，支持客户／服务器模式，能够满足创建各种类型数据库的需求，因此是目前高等学校讲授大型数据库管理系统的首选软件平台。《数据库技术及应用――SQLSv》以培养学生利用数据库技术对数据和信息进行管理、加工和利用的意识与能力为目标，以数据库原理和技术为知识讲授核心，建构教材的体例。《数据库技术及应用――SQLSv》分为上、下两篇。上篇为基础篇，主要介绍与数据库相关的基本概念，数据库设计方法，SQLSv-体系结构，数据库对象管理，T―SQL语言及应用，存储过程和触发器的使用技术。下篇为应用篇，主要介绍安全管理技术，数据备份、恢复及转换技术，ADO数据对象，VB／SQLSv的应用程序开发方法及实例。《数据库技术及应用――SQLSv》体系完整，结构清晰，实例丰富，图文并茂，精编精讲，易读易懂。全书体例创新，由一组系统化的、围绕一个数据库应用系统的相关例子贯穿始终，特色鲜明，具有普遍适用性。《数据库技术及应用――SQLSv》可作为高等学校本、专科学生的教科书，也可作为学习数据库应用技术读者的自学用书。为了方便教师教学和学生自主学习，《数据库技术及应用――SQLSv》配有《数据库技术及应用――习题与实验指导（SQLSv）》和电子教案、例题、实验软件的电子文档以及相关的教学网站，网址为：／／c.cncs.c／c／nx。\", \"price\" : 22 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"116\", \"_score\" : 2.9018917, \"_source\" : { \"name\" : \"全国计算机等级考试一级B教程（2010年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机考试\", \"author\" : \"\", \"info\" : \"《全国计算机等级考试一级B教程（2010年版）》由教育部考试中心组织，在全国计算机等级考试委员会指导下由有关专家按照《全国计算机等级考试一级B考试大纲（2007年版）》的要求而编写，内容包括计算机基础知识、WnwsXP操作系统、W2003的使用、Exc2003的使用、因特网的基础知识和简单应用等。由教育部考试中心组织和实施的计算机等级考试，是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国范围的等级考试。它面向社会，服务于社会。《全国计算机等级考试一级B教程（2010年版）》除了可以作为计算机等级考试的教材外，还可作为学习计算机基础知识的参考书。\", \"price\" : 33 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"138\", \"_score\" : 2.821856, \"_source\" : { \"name\" : \"全国计算机等级考试三级教程：PC技术（2010年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"考试认证\", \"author\" : \"\", \"info\" : \"《全国计算机等级考试三级教程：PC技术(2010年版\", \"price\" : 0 } } ] }} 123456GET books/_search{ \"query\": { \"match_all\": {} }} hits 中就是查询结果，total 是符合查询条件的文档数。 简单搜索可以简写为： 1GET books/_search 简单搜索默认查询 10 条记录。 14.2 词项查询即 term 查询，就是根据词去查询，查询指定字段中包含给定单词的文档，term 查询不被解析，只有搜索的词和文档中的词精确匹配，才会返回文档。应用场景如：人名、地名等等。 查询 name 字段中包含 十一五 的文档。 12345678GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }} 14.3 分页默认返回前 10 条数据，es 中也可以像关系型数据库一样，给一个分页参数： 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"size\": 10, \"from\": 10} 14.4 过滤返回字段如果返回的字段比较多，又不需要这么多字段，此时可以指定返回的字段： 1234567891011GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"size\": 10, \"from\": 10, \"_source\": [\"name\",\"author\"]} 此时，返回的字段就只有 name 和 author 了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119{ \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 142, \"relation\" : \"eq\" }, \"max_score\" : 1.7939949, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"323\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"徐春祥\", \"name\" : \"普通高等教育十一五国家级规划教材・医学化学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"476\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"陈后金，胡健，薛健\", \"name\" : \"普通高等教育“十一五”国家级规划教材：信号与系统\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"480\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"余家荣\", \"name\" : \"普通高等教育“十一五”国家级规划教材：复变函数\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"494\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"徐美银\", \"name\" : \"全国高职高专教育“十一五”规划教材：经济学原理\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"498\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：组合学讲义\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"502\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"卞毓宁\", \"name\" : \"全国高职高专教育十一五规划教材：统计学概论\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"614\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"陈洪亮，张峰，田社平\", \"name\" : \"普通高等教育“十一五”国家级规划教材：电路基础\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"13\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"胡立勇，丁艳锋\", \"name\" : \"普通高等教育“十一五”国家级规划教材：作物栽培学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"65\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：有机化学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"87\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：美术设计基础\" } } ] }} 14.5 最小评分有的文档得分特别低，说明这个文档和我们查询的关键字相关度很低。我们可以设置一个最低分，只有得分超过最低分的文档才会被返回。 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"min_score\":1.75, \"_source\": [\"name\",\"author\"]} 得分低于 1.75 的文档将直接被舍弃。 14.6 高亮查询关键字高亮： 123456789101112131415GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"min_score\":1.75, \"_source\": [\"name\",\"author\"], \"highlight\": { \"fields\": { \"name\": {} } }} 15.ElasticSearch 全文查询15.1 match querymatch query 会对查询语句进行分词，分词后，如果查询语句中的任何一个词项被匹配，则文档就会被索引到。 12345678GET books/_search{ \"query\": { \"match\": { \"name\": \"美术计算机\" } }} 这个查询首先会对 美术计算机 进行分词，分词之后，再去查询，只要文档中包含一个分词结果，就回返回文档。换句话说，默认词项之间是 OR 的关系，如果想要修改，也可以改为 AND。 1234567891011GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"美术计算机\", \"operator\": \"and\" } } }} 此时就会要求文档中必须同时包含 美术 和 计算机 两个词。 15.2 match_phrase querymatch_phrase query 也会对查询的关键字进行分词，但是它分词后有两个特点： 分词后的词项顺序必须和文档中词项的顺序一致 所有的词都必须出现在文档中 示例如下： 1234567891011GET books/_search{ \"query\": { \"match_phrase\": { \"name\": { \"query\": \"十一五计算机\", \"slop\": 7 } } }} query 是查询的关键字，会被分词器进行分解，分解之后去倒排索引中进行匹配。 slop 是指关键字之间的最小距离，但是注意不是关键字之间间隔的字数。文档中的字段被分词器解析之后，解析出来的词项都包含一个 position 字段表示词项的位置，查询短语分词之后 的 position 之间的间隔要满足 slop 的要求。 15.3 match_phrase_prefix query这个类似于 match_phrase query，只不过这里多了一个通配符，match_phrase_prefix 支持最后一个词项的前缀匹配，但是由于这种匹配方式效率较低，因此大家作为了解即可。 12345678GET books/_search{ \"query\": { \"match_phrase_prefix\": { \"name\": \"计\" } }} 这个查询过程，会自动进行单词匹配，会自动查找以计开始的单词，默认是 50 个，可以自己控制： 1234567891011GET books/_search{ \"query\": { \"match_phrase_prefix\": { \"name\": { \"query\": \"计\", \"max_expansions\": 3 } } }} match_phrase_prefix 是针对分片级别的查询，假设 max_expansions 为 1，可能返回多个文档，但是只有一个词，这是我们预期的结果。有的时候实际返回结果和我们预期结果并不一致，原因在于这个查询是分片级别的，不同的分片确实只返回了一个词，但是结果可能来自不同的分片，所以最终会看到多个词。 15.4 multi_match querymatch 查询的升级版，可以指定多个查询域： 123456789GET books/_search{ \"query\": { \"multi_match\": { \"query\": \"java\", \"fields\": [\"name\",\"info\"] } }} 这种查询方式还可以指定字段的权重： 123456789GET books/_search{ \"query\": { \"multi_match\": { \"query\": \"阳光\", \"fields\": [\"name^4\",\"info\"] } }} 这个表示关键字出现在 name 中的权重是出现在 info 中权重的 4 倍。 15.5 query_string queryquery_string 是一种紧密结合 Lucene 的查询方式，在一个查询语句中可以用到 Lucene 的一些查询语法： 123456789GET books/_search{ \"query\": { \"query_string\": { \"default_field\": \"name\", \"query\": \"(十一五) AND (计算机)\" } }} 15.6 simple_query_string这个是 query_string 的升级，可以直接使用 +、|、- 代替 AND、OR、NOT 等。 123456789GET books/_search{ \"query\": { \"simple_query_string\": { \"fields\": [\"name\"], \"query\": \"(十一五) + (计算机)\" } }} 查询结果和 query_string。 16.ElasticSearch词项查询16.1 term query词项查询。词项查询不会分析查询字符，直接拿查询字符去倒排索引中比对。 12345678GET books/_search{ \"query\": { \"term\": { \"name\": \"程序设计\" } }} 16.2 terms query词项查询，但是可以给多个关键词。 12345678GET books/_search{ \"query\": { \"terms\": { \"name\": [\"程序\",\"设计\",\"java\"] } }} 16.3 range query范围查询，可以按照日期范围、数字范围等查询。 range query 中的参数主要有四个： gt lt gte lte 案例： 123456789101112131415161718GET books/_search{ \"query\": { \"range\": { \"price\": { \"gte\": 10, \"lt\": 20 } } }, \"sort\": [ { \"price\": { \"order\": \"desc\" } } ]} 16.4 exists queryexists query 会返回指定字段中至少有一个非空值的文档： 12345678GET books/_search{ \"query\": { \"exists\": { \"field\": \"javaboy\" } }} 注意，空字符串也是有值。null 是空值。 16.5 prefix query前缀查询，效率略低，除非必要，一般不太建议使用。 给定关键词的前缀去查询： 12345678910GET books/_search{ \"query\": { \"prefix\": { \"name\": { \"value\": \"大学\" } } }} 16.6 wildcard querywildcard query 即通配符查询。支持单字符和多字符通配符： ？表示一个任意字符。 * 表示零个或者多个字符。 查询所有姓张的作者的书： 12345678910GET books/_search{ \"query\": { \"wildcard\": { \"author\": { \"value\": \"张*\" } } }} 查询所有姓张并且名字只有两个字的作者的书： 12345678910GET books/_search{ \"query\": { \"wildcard\": { \"author\": { \"value\": \"张?\" } } }} 16.7 regexp query支持正则表达式查询。 查询所有姓张并且名字只有两个字的作者的书： 12345678GET books/_search{ \"query\": { \"regexp\": { \"author\": \"张.\" } }} 16.8 fuzzy query在实际搜索中，有时我们可能会打错字，从而导致搜索不到，在 match query 中，可以通过 fuzziness 属性实现模糊查询。 fuzzy query 返回与搜索关键字相似的文档。怎么样就算相似？以LevenShtein 编辑距离为准。编辑距离是指将一个字符变为另一个字符所需要更改字符的次数，更改主要包括四种： 更改字符（javb–〉java） 删除字符（javva–〉java） 插入字符（jaa–〉java） 转置字符（ajva–〉java） 为了找到相似的词，模糊查询会在指定的编辑距离中创建搜索关键词的所有可能变化或者扩展的集合，然后进行搜索匹配。 12345678GET books/_search{ \"query\": { \"fuzzy\": { \"name\": \"javba\" } }} 16.9 ids query根据指定的 id 查询。 12345678GET books/_search{ \"query\": { \"ids\":{ \"values\": [1,2,3] } }} 17.ElasticSearch 复合查询17.1 constant_score query当我们不关心检索词项的频率（TF）对搜索结果排序的影响时，可以使用 constant_score 将查询语句或者过滤语句包裹起来。一般来说词项出现次数多会靠前？ 12345678910111213GET books/_search{ \"query\": { \"constant_score\": { \"filter\": { \"term\": { \"name\": \"java\" } }, \"boost\": 1.5 } }} 17.2 bool querybool query 可以将任意多个简单查询组装在一起，有四个关键字可供选择，四个关键字所描述的条件可以有一个或者多个。 must：文档必须匹配 must 选项下的查询条件。 should：文档可以匹配 should 下的查询条件，也可以不匹配。 must_not：文档必须不满足 must_not 选项下的查询条件。 filter：类似于 must，但是 filter 不评分，只是过滤数据。 例如查询 name 属性中必须包含 java，同时书价不在 [0,35] 区间内，info 属性可以包含 程序设计 也可以不包含程序 设计： 123456789101112131415161718192021222324252627282930313233GET books/_search{ \"query\": { \"bool\": { \"must\": [ { \"term\": { \"name\": { \"value\": \"java\" } } } ], \"must_not\": [ { \"range\": { \"price\": { \"gte\": 0, \"lte\": 35 } } } ], \"should\": [ { \"match\": { \"info\": \"程序设计\" } } ] } }} 这里还涉及到一个关键字，minmum_should_match 参数。 minmum_should_match 参数在 es 官网上称作最小匹配度。在之前学习的 multi_match 或者这里的 should 查询中，都可以设置 minmum_should_match 参数。 假设我们要做一次查询，查询 name 中包含 语言程序设计 关键字的文档： 12345678GET books/_search{ \"query\": { \"match\": { \"name\": \"语言程序设计\" } }} 在这个查询过程中，首先会进行分词，分词方式如下： 12345GET books/_analyze{ \"text\": [\"语言程序设计\"], \"analyzer\": \"ik_max_word\"} 分词后的 term 会构造成一个 should 的 bool query，每一个 term 都会变成一个 term query 的子句。换句话说，上面的查询和下面的查询等价： 12345678910111213141516171819202122232425262728293031323334353637GET books/_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"name\": { \"value\": \"语言\" } } }, { \"term\": { \"name\": { \"value\": \"程序设计\" } } }, { \"term\": { \"name\": { \"value\": \"程序\" } } }, { \"term\": { \"name\": { \"value\": \"设计\" } } } ] } }} 在这两个查询语句中，都是文档只需要包含词项中的任意一项即可，文档就回被返回，在 match 查询中，可以通过 operator 参数设置文档必须匹配所有词项。 如果想匹配一部分词项，就涉及到一个参数，就是 minmum_should_match，即最小匹配度。即至少匹配多少个词。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"operator\": \"and\" } } }}GET books/_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"name\": { \"value\": \"语言\" } } }, { \"term\": { \"name\": { \"value\": \"程序设计\" } } }, { \"term\": { \"name\": { \"value\": \"程序\" } } }, { \"term\": { \"name\": { \"value\": \"设计\" } } } ], \"minimum_should_match\": \"50%\" } }, \"from\": 0, \"size\": 70} 50% 表示词项个数的 50%。 如下两个查询等价（参数 4 是因为查询关键字分词后有 4 项）： 12345678910111213141516171819202122GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"minimum_should_match\": 4 } } }}GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"operator\": \"and\" } } }} 17.3 dis_max query假设现在有两本书： 123456789101112131415161718192021222324252627PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"content\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } }}POST blog/_doc{ \"title\":\"如何通过Java代码调用ElasticSearch\", \"content\":\"松哥力荐，这是一篇很好的解决方案\"}POST blog/_doc{ \"title\":\"初识 MongoDB\", \"content\":\"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\"} 现在假设搜索 Java解决方案 关键字，但是不确定关键字是在 title 还是在 content，所以两者都搜索： 12345678910111213141516171819GET blog/_search{ \"query\": { \"bool\": { \"should\": [ { \"match\": { \"title\": \"java解决方案\" } }, { \"match\": { \"content\": \"java解决方案\" } } ] } }} 搜索结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 882, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.1972204, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HhK653gBeADYd85qxnL3\", \"_score\" : 1.1972204, \"_source\" : { \"title\" : \"如何通过Java代码调用ElasticSearch\", \"content\" : \"松哥力荐，这是一篇很好的解决方案\" } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HxK753gBeADYd85qCnLy\", \"_score\" : 1.1069256, \"_source\" : { \"title\" : \"初识 MongoDB\", \"content\" : \"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\" } } ] }} 要理解这个原因，我们需要来看下 should query 中的评分策略： 首先会执行 should 中的两个查询 对两个查询结果的评分求和 对求和结果乘以匹配语句总数 在对第三步的结果除以所有语句总数 反映到具体的查询中： 前者 title 中 包含 java，假设评分是 1.1 content 中包含解决方案，假设评分是 1.2 有得分的 query 数量，这里是 2 总的 query 数量也是 2 最终结果：（1.1+1.2）*2/2=2.3 后者 title 中 不包含查询关键字，没有得分 content 中包含解决方案和 java，假设评分是 2 有得分的 query 数量，这里是 1 总的 query 数量也是 2 最终结果：2*1/2=1 在这种查询中，title 和 content 相当于是相互竞争的关系，所以我们需要找到一个最佳匹配字段。 为了解决这一问题，就需要用到 dis_max query（disjunction max query，分离最大化查询）：匹配的文档依然返回，但是只将最佳匹配的评分作为查询的评分。 12345678910111213141516171819GET blog/_search{ \"query\": { \"dis_max\": { \"queries\": [ { \"match\": { \"title\": \"java解决方案\" } }, { \"match\": { \"content\": \"java解决方案\" } } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 14, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.1069256, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HxK753gBeADYd85qCnLy\", \"_score\" : 1.1069256, \"_source\" : { \"title\" : \"初识 MongoDB\", \"content\" : \"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\" } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HhK653gBeADYd85qxnL3\", \"_score\" : 0.62177753, \"_source\" : { \"title\" : \"如何通过Java代码调用ElasticSearch\", \"content\" : \"松哥力荐，这是一篇很好的解决方案\" } } ] }} 在 dis_max query 中，还有一个参数 tie_breaker（取值在0～1），在 dis_max query 中，是完全不考虑其他 query 的分数，只是将最佳匹配的字段的评分返回。但是，有的时候，我们又不得不考虑一下其他 query 的分数，此时，可以通过 tie_breaker 来优化 dis_max query。tie_breaker 会将其他 query 的分数，乘以 tie_breaker，然后和分数最高的 query 进行一个综合计算。 17.4 function_score query场景：例如想要搜索附近的肯德基，搜索的关键字是肯德基，但是我希望能够将评分较高的肯德基优先展示出来。但是默认的评分策略是没有办法考虑到餐厅评分的，他只是考虑相关性，这个时候可以通过 function_score query 来实现。 准备两条测试数据： 1234567891011121314151617181920212223242526PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"votes\":{ \"type\": \"integer\" } } }}PUT blog/_doc/1{ \"title\":\"Java集合详解\", \"votes\":100}PUT blog/_doc/2{ \"title\":\"Java多线程详解，Java锁详解\", \"votes\":10} 现在搜索标题中包含 java 关键字的文档： 12345678GET blog/_search{ \"query\": { \"match\": { \"title\": \"java\" } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.22534126, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 0.22534126, \"_source\" : { \"title\" : \"Java多线程详解，Java锁详解\", \"votes\" : 10 } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.21799318, \"_source\" : { \"title\" : \"Java集合详解\", \"votes\" : 100 } } ] }} 默认情况下，id 为 2 的记录得分较高，因为他的 title 中包含两个 java。 如果我们在查询中，希望能够充分考虑 votes 字段，将 votes 较高的文档优先展示，就可以通过 function_score 来实现。 具体的思路，就是在旧的得分基础上，根据 votes 的数值进行综合运算，重新得出一个新的评分。 具体有几种不同的计算方式： weight random_score script_score field_value_factor weight weight 可以对评分设置权重，就是在旧的评分基础上乘以 weight，他其实无法解决我们上面所说的问题。具体用法如下： 1234567891011121314151617GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"weight\": 10 } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 11, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 2.2534127, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 2.2534127, \"_source\" : { \"title\" : \"Java多线程详解，Java锁详解\", \"votes\" : 10 } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 2.1799319, \"_source\" : { \"title\" : \"Java集合详解\", \"votes\" : 100 } } ] }} 可以看到，此时的评分，在之前的评分基础上*10 random_score random_score 会根据 uid 字段进行 hash 运算，生成分数，使用 random_score 时可以配置一个种子，如果不配置，默认使用当前时间。 1234567891011121314151617GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"random_score\": {} } ] } }} script_score 自定义评分脚本。假设每个文档的最终得分是旧的分数加上votes。查询方式如下： 12345678910111213141516171819202122GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"script_score\": { \"script\": { \"lang\": \"painless\", \"source\": \"_score + doc['votes'].value\" } } } ] } }} 现在，最终得分是 (oldScore+votes)*oldScore。 多了， 都会提示错误 如果不想乘以 oldScore，查询方式如下： 1234567891011121314151617181920212223GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"script_score\": { \"script\": { \"lang\": \"painless\", \"source\": \"_score + doc['votes'].value\" } } } ], \"boost_mode\": \"replace\" } }} 通过 boost_mode 参数，可以设置最终的计算方式。该参数还有其他取值： multiply：分数相乘 sum：分数相加 avg：求平均数 max：最大分 min：最小分 replace：不进行二次计算 field_value_factor 这个的功能类似于 script_score，但是不用自己写脚本。 假设每个文档的最终得分是旧的分数乘以votes。查询方式如下： 12345678910111213141516171819GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\" } } ] } }} 默认的得分就是oldScore*votes。 还可以利用 es 内置的函数进行一些更复杂的运算： 123456789101112131415161718192021GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\", \"modifier\": \"sqrt\" } } ], \"boost_mode\": \"replace\" } }} 此时，最终的得分是（sqrt(votes)）。 modifier 中可以设置内置函数，其他的内置函数还有： 参数名 含义 none 默认的，不进行任何计算 log 对字段值取对数 log1p 字段值+1 然后取对数 log2p 字段值+2 然后取对数 ln 取字段值的自然对数 ln1p 字段值+1 然后取自然对数 ln2p 字段值+2 然后取自然对数 sqrt 字段值求平方根 square 字段值的平方 reciprocal 倒数 另外还有个参数 factor ，影响因子。字段值先乘以影响因子，然后再进行计算。以 sqrt 为例，计算方式为 sqrt(factor*votes)： 12345678910111213141516171819202122GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\", \"modifier\": \"sqrt\", \"factor\": 10 } } ], \"boost_mode\": \"replace\" } }} 还有一个参数 max_boost，控制计算结果的范围： 123456789101112131415161718192021GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\" } } ], \"boost_mode\": \"sum\", \"max_boost\": 100 } }} max_boost 参数表示 functions 模块中，最终的计算结果上限。如果超过上限，就按照上线计算。 17.5 boosting queryboosting query 中包含三部分： positive：得分不变 negative：降低得分 negative_boost：降低的权重 123456789101112131415161718GET books/_search{ \"query\": { \"boosting\": { \"positive\": { \"match\": { \"name\": \"java\" } }, \"negative\": { \"match\": { \"name\": \"2008\" } }, \"negative_boost\": 0.5 } }} 查询结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647{ \"took\" : 15, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 4.5299835, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"549\", \"_score\" : 4.5299835, \"_source\" : { \"name\" : \"全国计算机等级考试笔试＋上机全真模拟：二级Java语言程序设计（最新版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"考试认证\", \"author\" : \"\", \"info\" : \"为了更好地服务于考生，引导考生尽快掌握考试大纲中要求的知识点和技能，顺利通过计算机等级考试，根据最新的考试大纲，高等教育出版社组织长期从事计算机等级考试命题研究和培训工作的专家编写了这套“笔试+上机考试全真模拟”，全面模拟考试真题，让考生在做题的同时全面巩固复习考点，提前熟悉考试环境，在短时间内冲刺过关。本书内容包括20套笔试模拟题和20套上机模拟题，还给出了参考答案和解析，尤其适合参加计算机等级考试的考生考前实战演练。\", \"price\" : 30 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"86\", \"_score\" : 2.5018067, \"_source\" : { \"name\" : \"全国计算机等级考试2级教程：Java语言程序设计（2008年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机考试\", \"author\" : \"\", \"info\" : \"由国家教育部考试中心推出的计算机等级考试是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国性考试，它面向社会，服务于社会。本书在教育部考试中心组织下、在全国计算机等级考试委员会指导下，由有关专家执笔编写而成。本书按照《全国计算机等级考试二级Java语言程序设计考试大纲（2007年版）》的要求编写，内容包括：Java体系结构、基本数据类型、流程控制语句、类、数组和字符串操作、输入输出及文件操作、图形用户界面编写、线程和串行化技术、A程序设计以及应用开发工具和安装使用等。本书是参加全国计算机等级考试二级Java语言程序设计的考生的良师益友，是教育部考试中心指定教材，也可作为欲学习Java编程的读者的参考书。\", \"price\" : 37 } } ] }} 可以看到，id 为 86 的文档满足条件，因此它的最终得分在旧的分数上*0.5。 18.ElasticSearch 嵌套查询关系型数据库中有表的关联关系，在 es 中，我们也有类似的需求，例如订单表和商品表，在 es 中，这样的一对多一般来说有两种方式： 嵌套文档（nested） 父子文档 18.1 嵌套文档假设：有一个电影文档，每个电影都有演员信息： 12345678910111213141516171819202122232425PUT movies{ \"mappings\": { \"properties\": { \"actors\":{ \"type\": \"nested\" } } }}PUT movies/_doc/1{ \"name\":\"霸王别姬\", \"actors\":[ { \"name\":\"张国荣\", \"gender\":\"男\" }, { \"name\":\"巩俐\", \"gender\":\"女\" } ]} 注意 actors 类型要是 nested，具体原因参考 10.2.3 小节。 缺点 查看文档数量： 1GET _cat/indices?v 查看结果如下： 添加了1个文档，显示存了3个 这是因为 nested 文档在 es 内部其实也是独立的 lucene 文档，只是在我们查询的时候，es 内部帮我们做了 join 处理，所以最终看起来就像一个独立文档一样。因此这种方案性能并不是特别好。 18.2 嵌套查询这个用来查询嵌套文档： 123456789101112131415161718192021222324GET movies/_search{ \"query\": { \"nested\": { \"path\": \"actors\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"actors.name\": \"张国荣\" } }, { \"match\": { \"actors.gender\": \"男\" } } ] } } } }} 18.3 父子文档相比于嵌套文档，父子文档主要有如下优势： 更新父文档时，不会重新索引子文档 创建、修改或者删除子文档时，不会影响父文档或者其他的子文档。 子文档可以作为搜索结果独立返回。 例如学生和班级的关系： 12345678910111213141516PUT stu_class{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"s_c\":{ \"type\": \"join\", \"relations\":{ \"class\":\"student\" } } } }} s_c 表示父子文档关系的名字，可以自定义。join 表示这是一个父子文档。relations 里边，class 这个位置是 parent，student 这个位置是 child。 接下来，插入两个父文档： 1234567891011121314PUT stu_class/_doc/1{ \"name\":\"一班\", \"s_c\":{ \"name\":\"class\" }}PUT stu_class/_doc/2{ \"name\":\"二班\", \"s_c\":{ \"name\":\"class\" }} 再来添加三个子文档： 123456789101112131415161718192021222324PUT stu_class/_doc/3?routing=1{ \"name\":\"zhangsan\", \"s_c\":{ \"name\":\"student\", \"parent\":1 }}PUT stu_class/_doc/4?routing=1{ \"name\":\"lisi\", \"s_c\":{ \"name\":\"student\", \"parent\":1 }}PUT stu_class/_doc/5?routing=2{ \"name\":\"wangwu\", \"s_c\":{ \"name\":\"student\", \"parent\":2 }} 首先大家可以看到，子文档都是独立的文档。特别需要注意的地方是，子文档需要和父文档在同一个分片上，所以 routing 关键字的值为父文档的 id。另外，name 属性表明这是一个子文档。 父子文档需要注意的地方： 每个索引只能定义一个 join filed 父子文档需要在同一个分片上（查询，修改需要routing） 可以向一个已经存在的 join filed 上新增关系 18.4 has_child query通过子文档查询父文档使用 has_child query。 12345678910111213GET stu_class/_search{ \"query\": { \"has_child\": { \"type\": \"student\", \"query\": { \"match\": { \"name\": \"wangwu\" } } } }} 查询 wangwu 所属的班级。 18.5 has_parent query通过父文档查询子文档： 12345678910111213GET stu_class/_search{ \"query\": { \"has_parent\": { \"parent_type\": \"class\", \"query\": { \"match\": { \"name\": \"二班\" } } } }} 查询二班的学生。但是大家注意，这种查询没有评分。 可以使用 parent id 查询子文档： 123456789GET stu_class/_search{ \"query\": { \"parent_id\":{ \"type\":\"student\", \"id\":1 } }} 通过 parent id 查询，默认情况下使用相关性计算分数。 18.6 小结整体上来说： 普通子对象实现一对多，会损失子文档的边界，子对象之间的属性关系丢失。 nested 可以解决第 1 点的问题，但是 nested 有两个缺点：更新主文档的时候要全部更新，不支持子文档属于多个主文档。 父子文档解决 1、2 点的问题，但是它主要适用于写多读少的场景。 19.ElasticSearch 地理位置查询19.1 数据准备创建一个索引： 12345678910111213PUT geo{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"location\":{ \"type\": \"geo_point\" } } }} 准备一个 geo.json 文件： 12345678910111213141516{\"index\":{\"_index\":\"geo\",\"_id\":1}}{\"name\":\"西安\",\"location\":\"34.288991865037524,108.9404296875\"}{\"index\":{\"_index\":\"geo\",\"_id\":2}}{\"name\":\"北京\",\"location\":\"39.926588421909436,116.43310546875\"}{\"index\":{\"_index\":\"geo\",\"_id\":3}}{\"name\":\"上海\",\"location\":\"31.240985378021307,121.53076171875\"}{\"index\":{\"_index\":\"geo\",\"_id\":4}}{\"name\":\"天津\",\"location\":\"39.13006024213511,117.20214843749999\"}{\"index\":{\"_index\":\"geo\",\"_id\":5}}{\"name\":\"杭州\",\"location\":\"30.259067203213018,120.21240234375001\"}{\"index\":{\"_index\":\"geo\",\"_id\":6}}{\"name\":\"武汉\",\"location\":\"30.581179257386985,114.3017578125\"}{\"index\":{\"_index\":\"geo\",\"_id\":7}}{\"name\":\"合肥\",\"location\":\"31.840232667909365,117.20214843749999\"}{\"index\":{\"_index\":\"geo\",\"_id\":8}}{\"name\":\"重庆\",\"location\":\"29.592565403314087,106.5673828125\"} 最后，执行如下命令，批量导入 geo.json 数据： 1curl -XPOST \"http://localhost:9200/geo/_bulk?pretty\" -H \"content-type:application/json\" --data-binary @geo.json 可能用到的工具网站： http://geojson.io/#map=6/32.741/116.521 19.2 geo_distance query给出一个中心点，查询距离该中心点指定范围内的文档： 1234567891011121314151617181920212223GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_distance\": { \"distance\": \"600km\", \"location\": { \"lat\": 34.288991865037524, \"lon\": 108.9404296875 } } } ] } }} 以(34.288991865037524,108.9404296875) 为圆心，以 600KM 为半径，这个范围内的数据。 19.3 geo_bounding_box query在某一个矩形内的点，通过两个点锁定一个矩形： 12345678910111213141516171819202122232425262728GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_bounding_box\": { \"location\": { \"top_left\": { \"lat\": 32.0639555946604, \"lon\": 118.78967285156249 }, \"bottom_right\": { \"lat\": 29.98824461550903, \"lon\": 122.20642089843749 } } } } ] } }} 以南京经纬度作为矩形的左上角，以舟山经纬度作为矩形的右下角，构造出来的矩形中，包含上海和杭州两个城市。 19.4 geo_polygon query在某一个多边形范围内的查询。 12345678910111213141516171819202122232425262728293031323334GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_polygon\": { \"location\": { \"points\": [ { \"lat\": 31.793755581217674, \"lon\": 113.8238525390625 }, { \"lat\": 30.007273923504556, \"lon\":114.224853515625 }, { \"lat\": 30.007273923504556, \"lon\":114.8345947265625 } ] } } } ] } }} 给定多个点，由多个点组成的多边形中的数据。 es7.12出现提示 #! Deprecated field [geo_polygon] used, replaced by [[geo_shape] query where polygons are defined in geojson or wkt] 19.5 geo_shape querygeo_shape 用来查询图形，针对 geo_shape，两个图形之间的关系有：相交、包含、不相交。 新建索引： 12345678910111213PUT geo_shape{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"location\":{ \"type\": \"geo_shape\" } } }} 然后添加一条线： 1234567891011PUT geo_shape/_doc/1{ \"name\":\"西安-郑州\", \"location\":{ \"type\":\"linestring\", \"coordinates\":[ [108.9404296875,34.279914398549934], [113.66455078125,34.768691457552706] ] }} 接下来查询某一个图形中是否包含该线： 12345678910111213141516171819202122232425262728293031323334GET geo_shape/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_shape\": { \"location\": { \"shape\": { \"type\": \"envelope\", \"coordinates\": [ [ 106.5234375, 36.80928470205937 ], [ 115.33447265625, 32.24997445586331 ] ] }, \"relation\": \"within\" } } } ] } }} relation 属性表示两个图形的关系： within 包含 intersects 相交 disjoint 不相交 20.ElasticSearch 特殊查询20.1 more_like_this querymore_like_this query 可以实现基于内容的推荐，给定一篇文章，可以查询出和该文章相似的内容。 12345678910111213GET books/_search{ \"query\": { \"more_like_this\": { \"fields\": [ \"info\" ], \"like\": \"大学战略\", \"min_term_freq\": 1, \"max_query_terms\": 12 } }} fields：要匹配的字段，可以有多个 like：要匹配的文本 min_term_freq：词项的最低频率，默认是 2。特别注意，这个是指词项在要匹配的文本中的频率，而不是 es 文档中的频率 max_query_terms：query 中包含的最大词项数目 min_doc_freq：最小的文档频率，搜索的词，至少在多少个文档中出现，少于指定数目，该词会被忽略 max_doc_freq：最大文档频率 analyzer：分词器，默认使用字段的分词器 stop_words：停用词列表 minmum_should_match 20.2 script query脚本查询，例如查询所有价格大于 200 的图书： 1234567891011121314151617GET books/_search{ \"query\": { \"bool\": { \"filter\": [ { \"script\": { \"script\": { \"lang\": \"painless\", \"source\": \"if(doc['price'].size()!=0){doc['price'].value &gt; 200}\" } } } ] } }} 20.3 percolate querypercolate query 译作渗透查询或者反向查询。 正常操作：根据查询语句找到对应的文档 query-&gt;document percolate query：根据文档，返回与之匹配的查询语句，document-&gt;query 应用场景： 价格监控 库存报警 股票警告 … 例如阈值告警，假设指定字段值大于阈值，报警提示。 percolate mapping 定义： 12345678910111213141516PUT log{ \"mappings\": { \"properties\": { \"threshold\":{ \"type\": \"long\" }, \"count\":{ \"type\": \"long\" }, \"query\":{ \"type\":\"percolator\" } } }} percolator 类型相当于 keyword、long 以及 integer 等。 插入文档： 123456789101112131415PUT log/_doc/1{ \"threshold\":10, \"query\":{ \"bool\":{ \"must\":{ \"range\":{ \"count\":{ \"gt\":10 } } } } }} 最后查询： 12345678910111213141516171819202122232425GET log/_search{ \"query\": { \"percolate\": { \"field\": \"query\", \"documents\": [ { \"count\":3 }, { \"count\":6 }, { \"count\":90 }, { \"count\":12 }, { \"count\":15 } ] } }} 查询结果中会列出不满足条件的文档。 查询结果中的 _percolator_document_slot 字段表示文档的 position，从 0 开始计。 21.ElasticSearch 搜索高亮与排序21.1 搜索高亮普通高亮，默认会自动添加 em 标签： 12345678910111213GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"fields\": { \"name\": {} } }} 正常来说，我们见到的高亮可能是红色、黄色之类的。 可以自定义高亮标签： 12345678910111213141516GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"fields\": { \"name\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] } } }} 有的时候，虽然我们是在 name 字段中搜索的，但是我们希望 info 字段中，相关的关键字也能高亮： 123456789101112131415161718192021GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"require_field_match\": \"false\", \"fields\": { \"name\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] }, \"info\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] } } }} require_field_match By default, only fields that contains a query match are highlighted. Set require_field_match to false to highlight all fields. Defaults to true. 21.2 排序排序很简单，默认是按照查询文档的相关度来排序的，即（_score 字段）： 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": { \"value\": \"java\" } } }} 等价于： 1234567891011121314151617GET books/_search{ \"query\": { \"term\": { \"name\": { \"value\": \"java\" } } }, \"sort\": [ { \"_score\": { \"order\": \"desc\" } } ]} match_all 查询只是返回所有文档，不评分，默认按照添加顺序返回，可以通过 _doc 字段对其进行排序： 1234567891011121314GET books/_search{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"_doc\": { \"order\": \"desc\" } } ], \"size\": 20} es 同时也支持多字段排序。 12345678910111213141516171819GET books/_search{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": { \"order\": \"asc\" } }, { \"_doc\": { \"order\": \"desc\" } } ], \"size\": 20} 22.ElasticSearch 指标聚合22.1 Max Aggregation统计最大值。例如查询价格最高的书： 12345678910GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"field\": \"price\" } } }} 1234567891011GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"field\": \"price\", \"missing\": 1000 } } }} 如果某个文档中缺少 price 字段，则设置该字段的值为 1000。 也可以通过脚本来查询最大值： 123456789101112GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 使用脚本时，可以先通过 doc['price'].size()!=0 去判断文档是否有对应的属性。 22.2 Min Aggregation统计最小值，用法和 Max Aggregation 基本一致： 1234567891011GET books/_search{ \"aggs\": { \"min_price\": { \"min\": { \"field\": \"price\", \"missing\": 1000 } } }} 脚本： 123456789101112GET books/_search{ \"aggs\": { \"min_price\": { \"min\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.3 Avg Aggregation统计平均值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } }}GET books/_search{ \"aggs\": { \"avg_price\": { \"avg\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.4 Sum Aggregation求和： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"sum_price\": { \"sum\": { \"field\": \"price\" } } }}GET books/_search{ \"aggs\": { \"sum_price\": { \"sum\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.5 Cardinality Aggregationcardinality aggregation 用于基数统计。类似于 SQL 中的 distinct count(0)： text 类型是分析型类型，默认是不允许进行聚合操作的，如果相对 text 类型进行聚合操作，需要设置其 fielddata 属性为 true，这种方式虽然可以使 text 类型进行聚合操作，但是无法满足精准聚合，如果需要精准聚合，可以设置字段的子域为 keyword。 方式一： 重新定义 books 索引： 123456789101112131415161718192021222324252627282930PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"fielddata\": true }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 定义完成后，重新插入数据（参考之前的视频）。 接下来就可以查询出版社的总数量： 12345678910GET books/_search{ \"aggs\": { \"publish_count\": { \"cardinality\": { \"field\": \"publish\" } } }} 这种聚合方式可能会不准确。可以将 publish 设置为 keyword 类型或者设置子域为 keyword。 12345678910111213141516171819202122232425262728PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"keyword\" }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 对比查询结果可知，使用 fileddata 的方式，查询结果不准确。 22.6 Stats Aggregation基本统计，一次性返回 count、max、min、avg、sum： 12345678910GET books/_search{ \"aggs\": { \"stats_query\": { \"stats\": { \"field\": \"price\" } } }} 22.7 Extends Stats Aggregation高级统计，比 stats 多出来：平方和、方差、标准差、平均值加减两个标准差的区间： 12345678910GET books/_search{ \"aggs\": { \"es\": { \"extended_stats\": { \"field\": \"price\" } } }} 22.8 Percentiles Aggregation百分位统计。 123456789101112131415161718192021GET books/_search{ \"aggs\": { \"p\": { \"percentiles\": { \"field\": \"price\", \"percents\": [ 1, 5, 10, 15, 25, 50, 75, 95, 99 ] } } }} 22.9 Value Count Aggregation可以按照字段统计文档数量（包含指定字段的文档数量）： 12345678910GET books/_search{ \"aggs\": { \"count\": { \"value_count\": { \"field\": \"price\" } } }} 23.ElasticSearch 桶聚合（bucket）23.1 Terms AggregationTerms Aggregation 用于分组聚合，例如，统计各个出版社出版的图书总数量: 1234567891011GET books/_search{ \"aggs\": { \"NAME\": { \"terms\": { \"field\": \"publish\", \"size\": 20 } } }} 在 terms 分桶的基础上，还可以对每个桶进行指标聚合。 统计不同出版社所出版的图书的平均价格： 123456789101112131415161718GET books/_search{ \"aggs\": { \"NAME\": { \"terms\": { \"field\": \"publish\", \"size\": 20 }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.2 Filter Aggregation过滤器聚合。可以将符合过滤器中条件的文档分到一个桶中，然后可以求其平均值。 例如查询书名中包含 java 的图书的平均价格： 12345678910111213141516171819GET books/_search{ \"aggs\": { \"NAME\": { \"filter\": { \"term\": { \"name\": \"java\" } }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.3 Filters Aggregation多过滤器聚合。过滤条件可以有多个。 例如查询书名中包含 java 或者 office 的图书的平均价格： 123456789101112131415161718192021222324252627GET books/_search{ \"aggs\": { \"NAME\": { \"filters\": { \"filters\": [ { \"term\":{ \"name\":\"java\" } },{ \"term\":{ \"name\":\"office\" } } ] }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.4 Range Aggregation按照范围聚合，在某一个范围内的文档数统计。 例如统计图书价格在 0-50、50-100、100-150、150以上的图书数量： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"NAME\": { \"range\": { \"field\": \"price\", \"ranges\": [ { \"to\": 50 },{ \"from\": 50, \"to\": 100 },{ \"from\": 100, \"to\": 150 },{ \"from\": 150 } ] } } }} 23.5 Date Range AggregationRange Aggregation 也可以用来统计日期，但是也可以使用 Date Range Aggregation，后者的优势在于可以使用日期表达式。 造数据： 123456789101112131415PUT blog/_doc/1{ \"title\":\"java\", \"date\":\"2018-12-30\"}PUT blog/_doc/2{ \"title\":\"java\", \"date\":\"2020-12-30\"}PUT blog/_doc/3{ \"title\":\"java\", \"date\":\"2022-10-30\"} 统计一年前到一年后的博客数量： 12345678910111213141516GET blog/_search{ \"aggs\": { \"NAME\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-12M/M\", \"to\": \"now+1y/y\" } ] } } }} 12M/M 表示 12 个月。 1y/y 表示 1年。 d 表示天 23.6 Date Histogram Aggregation时间直方图聚合。 例如统计各个月份的博客数量 1234567891011GET blog/_search{ \"aggs\": { \"NAME\": { \"date_histogram\": { \"field\": \"date\", \"calendar_interval\": \"month\" } } }} 23.7 Missing Aggregation空值聚合。 统计所有没有 price 字段的文档： 12345678910GET books/_search{ \"aggs\": { \"NAME\": { \"missing\": { \"field\": \"price\" } } }} 23.8 Children Aggregation可以根据父子文档关系进行分桶。 查询子类型为 student 的文档数量： 12345678910GET stu_class/_search{ \"aggs\": { \"NAME\": { \"children\": { \"type\": \"student\" } } }} 23.9 Geo Distance Aggregation对地理位置数据做统计。 例如查询(34.288991865037524,108.9404296875)坐标方圆 600KM 和 超过 600KM 的城市数量。 12345678910111213141516171819GET geo/_search{ \"aggs\": { \"NAME\": { \"geo_distance\": { \"field\": \"location\", \"origin\": \"34.288991865037524,108.9404296875\", \"unit\": \"km\", \"ranges\": [ { \"to\": 600 },{ \"from\": 600 } ] } } }} 23.10 IP Range AggregationIP 地址范围查询。 12345678910111213141516GET blog/_search{ \"aggs\": { \"NAME\": { \"ip_range\": { \"field\": \"ip\", \"ranges\": [ { \"from\": \"127.0.0.5\", \"to\": \"127.0.0.11\" } ] } } }} 24.ElasticSearch 管道聚合 类似于linux的 | 管道符管道聚合相当于在之前聚合的基础上，再次聚合。 24.1 Avg Bucket Aggregation计算聚合平均值。例如，统计每个出版社所出版图书的平均值，然后再统计所有出版社的平均值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"avg_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.2 Max Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最大值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"max_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.3 Min Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最小值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"min_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.4 Sum Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值之和： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"sum_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.5 Stats Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值的各种数据： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"stats_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.6 Extended Stats Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"extended_stats_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.7 Percentiles Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"percentiles_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 25.ElasticSearch Java Api 概览Java操作Es的方案: 1.直接使用Http请求以 HttpUrlConnection 为例，请求方式如下： 12345678910111213public class HttpRequestTest { public static void main(String[] args) throws IOException { URL url = new URL(\"http://localhost:9200/books/_search?pretty=true\"); HttpURLConnection con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } } }} 弊端：需要自己组装请求参数，自己去解析响应的json 2.Java Low Level REST Client从字面上来理解，这个叫做低级客户端。 它允许通过 Http 与一个 Elasticsearch 集群通信。将请求的 JSON 参数拼接和响应的 JSON 字符串解析留给用户自己处理。低级客户端最大的优势在于兼容所有的 ElasticSearch 的版本（因为它的 API 并没有封装 JSON 操作，所有的 JSON 操作还是由开发者自己完成），同时低级客户端要求 JDK 为 1.7 及以上。 低级客户端主要包括如下一些功能： 最小的依赖 跨所有可用节点的负载均衡 节点故障和特定响应代码时的故障转移 连接失败重试（是否重试失败的节点取决于它失败的连续次数；失败次数越多，客户端在再次尝试同一节点之前等待的时间越长） 持久连接 跟踪请求和响应的日志记录 可选自动发现集群节点 Java Low Level REST Client 的操作其实比较简单，松哥后面会录制一个视频和大家分享相关操作。 3.Java High Level REST Client从字面上来理解，这个叫做高级客户端，也是目前使用最多的一种客户端。它其实有点像之前的 TransportClient。 这个所谓的高级客户端它的内部其实还是基于低级客户端，只不过针对 ElasticSearch 它提供了更多的 API，将请求参数和响应参数都封装成了相应的 API，开发者只需要调用相关的方法就可以拼接参数或者解析响应结果。 Java High Level REST Client 中的每个 API 都可以同步或异步调用，同步方法返回一个响应对象，而异步方法的名称则以 Async 为后缀结尾，异步请求一般需要一个监听器参数，用来处理响应结果。 相对于低级客户端，高级客户端的兼容性就要差很多（因为 JSON 的拼接和解析它已经帮我们做好了）。高级客户端需要 JDK1.8 及以上版本并且依赖版本需要与 ElasticSearch 版本相同（主版本号需要一致，次版本号不必相同）。 举个简单例子： 7.0 客户端能够与任何 7.x ElasticSearch 节点进行通信，而 7.1 客户端肯定能够与 7.1，7.2 和任何后来的 7.x 版本进行通信，但与旧版本的 ElasticSearch 节点通信时可能会存在不兼容的问题。 4.其他ElasticSearch 的 Java 客户端 TransportClient Jest Spring Data Elasticsearch Java Low Level REST Client Java High Level REST Client TransportClient 官方已经不再推荐使用 TransportClient，并且表示会在 ElasticSearch8.0 中完全移除相关支持。 Jest Jest 提供了更流畅的 API 和更容易使用的接口，并且它的版本是遵循 ElasticSearch 的主版本号的，这样可以确保客户端和服务端之间的兼容性。 早期的 ElasticSearch 官方客户端对 RESTful 支持不够完美， Jest 在一定程度上弥补了官方客户端的不足，但是随着近两年官方客户端对 RESTful 功能的增强，Jest 早已成了明日黄花，最近的一次更新也停留在 2018 年 4 月，所以 Jest 小伙伴们也不必花时间去学了，知道曾经有过这么一个东西就行了。 Spring Data Elasticsearch Spring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点： Spring Data 项目支持 NoSQL 存储： MongoDB （文档数据库） Neo4j（图形数据库） Redis（键/值存储） Hbase（列族数据库） ElasticSearch Spring Data 项目所支持的关系数据存储技术： JDBC JPA 26.ElasticSearch普通HTTP请求以 HttpUrlConnection 为例，请求方式如下： 12345678910111213public class HttpRequestTest { public static void main(String[] args) throws IOException { URL url = new URL(\"http://localhost:9200/books/_search?pretty=true\"); HttpURLConnection con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } } }} 弊端：需要自己组装请求参数，自己去解析响应的json 27.ElasticSearch Java Low Level REST Client首先创建一个普通的 Maven 工程，添加如下依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.10.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 然后添加如下代码，发起一个简单的查询请求： 1234567891011121314151617181920212223242526272829public class LowLevelTest { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //这种请求发起方式，会阻塞后面的代码 Response response = restClient.performRequest(request); //5.解析 response，获取响应结果 BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); }} 这个查询请求，是一个同步请求，在请求的过程中，后面的代码会被阻塞，如果不希望后面的代码被阻塞，可以使用异步请求。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LowLevelTest2 { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //异步请求 restClient.performRequestAsync(request, new ResponseListener() { //请求成功的回调 @Override public void onSuccess(Response response) { //5.解析 response，获取响应结果 try { BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败的回调 @Override public void onFailure(Exception e) { } }); }} 开发者在请求时，也可以携带 JSON 参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class LowLevelTest3 { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //添加请求体 request.setEntity(new NStringEntity(\"{\\\"query\\\": {\\\"term\\\": {\\\"name\\\": {\\\"value\\\": \\\"java\\\"}}}}\", ContentType.APPLICATION_JSON)); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //异步请求 restClient.performRequestAsync(request, new ResponseListener() { //请求成功的回调 @Override public void onSuccess(Response response) { //5.解析 response，获取响应结果 try { BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败的回调 @Override public void onFailure(Exception e) { } }); }} 28.Java High Level REST Client28.1 索引管理28.1.1 创建索引首先创建一个普通的 Maven 项目，然后引入 high level rest client 依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.10.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意，依赖的版本和 Es 的版本要对应。 创建一个索引： 1234567891011121314151617181920212223public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式 blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} mapping 的配置，还有另外两种方式： 第一种，通过 map 构建 mapping： 12345678910111213141516171819202122232425262728293031public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式 Map&lt;String, String&gt; title = new HashMap&lt;&gt;(); title.put(\"type\", \"text\"); Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;(); properties.put(\"title\", title); Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;(); mappings.put(\"properties\", properties); blog1.mapping(mappings); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 第二种，通过 XContentBuilder 构建 mapping： 1234567891011121314151617181920212223242526272829303132333435363738394041public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式// Map&lt;String, String&gt; title = new HashMap&lt;&gt;();// title.put(\"type\", \"text\");// Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;();// properties.put(\"title\", title);// Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;();// mappings.put(\"properties\", properties);// blog1.mapping(mappings); //XContentBuilder 方式 XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); builder.startObject(\"properties\"); builder.startObject(\"title\"); builder.field(\"type\", \"text\"); builder.endObject(); builder.endObject(); builder.endObject(); blog1.mapping(builder); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 还可以给索引配置别名： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式// Map&lt;String, String&gt; title = new HashMap&lt;&gt;();// title.put(\"type\", \"text\");// Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;();// properties.put(\"title\", title);// Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;();// mappings.put(\"properties\", properties);// blog1.mapping(mappings); //XContentBuilder 方式 XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); builder.startObject(\"properties\"); builder.startObject(\"title\"); builder.field(\"type\", \"text\"); builder.endObject(); builder.endObject(); builder.endObject(); blog1.mapping(builder); //配置别名 blog1.alias(new Alias(\"blog_alias\")); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 如果觉得调 API 太麻烦，也可以直接上 JSON： 1234567891011121314151617181920public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 另外还有一些其他的可选配置： 123456789101112131415161718192021222324public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //请求超时时间，连接所有节点的超时时间 blog1.setTimeout(TimeValue.timeValueMinutes(2)); //连接 master 节点的超时时间 blog1.setMasterTimeout(TimeValue.timeValueMinutes(1)); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 前面所有的请求都是同步的，会阻塞的，也可以异步： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //请求超时时间，连接所有节点的超时时间 blog1.setTimeout(TimeValue.timeValueMinutes(2)); //连接 master 节点的超时时间 blog1.setMasterTimeout(TimeValue.timeValueMinutes(1)); //执行请求，创建索引// client.indices().create(blog1, RequestOptions.DEFAULT); //异步创建索引 client.indices().createAsync(blog1, RequestOptions.DEFAULT, new ActionListener&lt;CreateIndexResponse&gt;() { //请求成功 @Override public void onResponse(CreateIndexResponse createIndexResponse) { //关闭 client try { client.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败 @Override public void onFailure(Exception e) { } }); //关闭 client// client.close(); }} 28.1.2 查询索引是否存在1234567891011121314public class HighLevelTest3 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetIndexRequest blog = new GetIndexRequest(\"blog2\"); boolean exists = client.indices().exists(blog, RequestOptions.DEFAULT); System.out.println(\"exists = \" + exists); //关闭 client client.close(); }} 28.1.3 关闭/打开索引关闭： 1234567891011121314151617public class HighLevelTest4 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); CloseIndexRequest blog = new CloseIndexRequest(\"blog\"); CloseIndexResponse close = client.indices().close(blog, RequestOptions.DEFAULT); List&lt;CloseIndexResponse.IndexResult&gt; indices = close.getIndices(); for (CloseIndexResponse.IndexResult index : indices) { System.out.println(\"index.getIndex() = \" + index.getIndex()); } //关闭 client client.close(); }} 触发warning[“the default value for the ?wait_for_active_shards parameter will change from ‘0’ to ‘index-setting’ in version 8; specify ‘?wait_for_active_shards=index-setting’ to adopt the future default behaviour, or ‘?wait_for_active_shards=0’ to preserve today’s behaviour”] 打开： 12345678910111213public class HighLevelTest4 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); OpenIndexRequest blog = new OpenIndexRequest(\"blog\"); client.indices().open(blog, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.4 索引修改1234567891011121314public class HighLevelTest5 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateSettingsRequest request = new UpdateSettingsRequest(\"blog\"); request.settings(Settings.builder().put(\"index.blocks.write\", true).build()); client.indices().putSettings(request, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.5 克隆索引被克隆的索引需要是只读索引，可以通过 28.1.4 小节中的方式设置索引为只读。 12345678910111213public class HighLevelTest6 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); ResizeRequest request = new ResizeRequest(\"blog2\", \"blog\"); client.indices().clone(request, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.6 查看索引12345678910111213141516171819public class HighLevelTest7 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetSettingsRequest request = new GetSettingsRequest().indices(\"blog\"); //设置需要互殴去的具体的参数，不设置则返回所有参数 request.names(\"index.blocks.write\"); GetSettingsResponse response = client.indices().getSettings(request, RequestOptions.DEFAULT); ImmutableOpenMap&lt;String, Settings&gt; indexToSettings = response.getIndexToSettings(); System.out.println(indexToSettings); String s = response.getSetting(\"blog\", \"index.number_of_replicas\"); System.out.println(s); //关闭 client client.close(); }} 28.1.7 Refresh &amp; FlushEs 底层依赖 Lucene，而 Lucene 中有 reopen 和 commit 两种操作，还有一个特殊的概念叫做 segment。 Es 中，基本的存储单元是 shard，对应到 Lucene 上，就是一个索引，Lucene 中的索引由 segment 组成，每个 segment 相当于 es 中的倒排索引。每个 es 文档创建时，都会写入到一个新的 segment 中，删除文档时，只是从属于它的 segment 处标记为删除，并没有从磁盘中删除。 Lucene 中： reopen 可以让数据搜索到，但是不保证数据被持久化到磁盘中。 commit 可以让数据持久化。 Es 中： 默认是每秒 refresh 一次（Es 中文档被索引之后，首先添加到内存缓冲区，refresh 操作将内存缓冲区中的数据拷贝到新创建的 segment 中，这里是在内存中操作的）。 flush 将内存中的数据持久化到磁盘中。一般来说，flush 的时间间隔比较久，默认 30 分钟。 123456789101112131415public class HighLevelTest8 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); RefreshRequest request = new RefreshRequest(\"blog\"); client.indices().refresh(request, RequestOptions.DEFAULT); FlushRequest flushRequest = new FlushRequest(\"blog\"); client.indices().flush(flushRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.9 索引别名索引的别名类似于 MySQL 中的视图。 28.1.9.1 添加别名添加一个普通的别名： 12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.ADD); aliasAction.index(\"books\").alias(\"books_alias\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 添加一个带 filter 的别名： 12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.ADD); aliasAction.index(\"books\").alias(\"books_alias2\").filter(\"{\\\"term\\\": {\\\"name\\\": \\\"java\\\"}}\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 现在，books 索引将存在两个别名，其中，books_alias2 自动过滤 name 中含有 java 的文档。 28.1.9.2 删除别名12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.REMOVE); aliasAction.index(\"books\").alias(\"books_alias\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 第二种移除方式： 12345678910111213public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); DeleteAliasRequest deleteAliasRequest = new DeleteAliasRequest(\"books\", \"books_alias2\"); client.indices().deleteAlias(deleteAliasRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.9.3 判断别名是否存在12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetAliasesRequest books_alias = new GetAliasesRequest(\"books_alias\"); //指定查看某一个索引的别名，不指定，则会搜索所有的别名 books_alias.indices(\"books\"); boolean b = client.indices().existsAlias(books_alias, RequestOptions.DEFAULT); System.out.println(b); //关闭 client client.close(); }} 28.1.9.4 获取别名1234567891011121314151617public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetAliasesRequest books_alias = new GetAliasesRequest(\"books_alias\"); //指定查看某一个索引的别名，不指定，则会搜索所有的别名 books_alias.indices(\"books\"); GetAliasesResponse response = client.indices().getAlias(books_alias, RequestOptions.DEFAULT); Map&lt;String, Set&lt;AliasMetadata&gt;&gt; aliases = response.getAliases(); System.out.println(\"aliases = \" + aliases); //关闭 client client.close(); }} 29.1 添加文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class DocTest01 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //构建一个 IndexRequest 请求，参数就是索引名称 IndexRequest request = new IndexRequest(\"book\"); //给请求配置一个 id，这个就是文档 id。如果指定了 id，相当于 put book/_doc/id ，也可以不指定 id，相当于 post book/_doc// request.id(\"1\"); //构建索引文本，有三种方式：JSON 字符串、Map 对象、XContentBuilder request.source(\"{\\\"name\\\": \\\"三国演义\\\",\\\"author\\\": \\\"罗贯中\\\"}\", XContentType.JSON); //执行请求，有同步和异步两种方式 //同步 IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); //获取文档id String id = indexResponse.getId(); System.out.println(\"id = \" + id); //获取索引名称 String index = indexResponse.getIndex(); System.out.println(\"index = \" + index); //判断文档是否添加成功 if (indexResponse.getResult() == DocWriteResponse.Result.CREATED) { System.out.println(\"文档添加成功\"); } //判断文档是否更新成功（如果 id 已经存在） if (indexResponse.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"文档更新成功\"); } ReplicationResponse.ShardInfo shardInfo = indexResponse.getShardInfo(); //判断分片操作是否都成功 if (shardInfo.getTotal() != shardInfo.getSuccessful()) { System.out.println(\"有存在问题的分片\"); } //有存在失败的分片 if (shardInfo.getFailed() &gt; 0) { //打印错误信息 for (ReplicationResponse.ShardInfo.Failure failure : shardInfo.getFailures()) { System.out.println(\"failure.reason() = \" + failure.reason()); } } //异步// client.indexAsync(request, RequestOptions.DEFAULT, new ActionListener&lt;IndexResponse&gt;() {// @Override// public void onResponse(IndexResponse indexResponse) {//// }//// @Override// public void onFailure(Exception e) {//// }// }); client.close(); }} 演示分片存在问题的情况。由于我只有三个节点，但是在创建索引时，设置需要三个副本，此时的节点就不够用： 1234567PUT book{ \"settings\": { \"number_of_replicas\": 3, \"number_of_shards\": 3 }} 创建完成后，再次执行上面的添加代码，此时就会打印出 有存在问题的分片。 构建索引信息，有三种方式： 123456789101112//构建索引文本，有三种方式：JSON 字符串、Map 对象、XContentBuilder//request.source(\"{\\\"name\\\": \\\"三国演义\\\",\\\"author\\\": \\\"罗贯中\\\"}\", XContentType.JSON);//Map&lt;String, String&gt; map = new HashMap&lt;&gt;();//map.put(\"name\", \"水浒传\");//map.put(\"author\", \"施耐庵\");//request.source(map).id(\"99\");XContentBuilder jsonBuilder = XContentFactory.jsonBuilder();jsonBuilder.startObject();jsonBuilder.field(\"name\", \"西游记\");jsonBuilder.field(\"author\", \"吴承恩\");jsonBuilder.endObject();request.source(jsonBuilder); 默认情况下，如果 request 中包含有 id 属性，则相当于 PUT book/_doc/1 这样的请求，如果 request 中不包含 id 属性，则相当于 POST book/_doc，此时 id 会自动生成。对于前者，如果 id 已经存在，则会执行一个更新操作。也就是 es 的具体操作，会自动调整。 当然，也可以直接指定操作。例如，指定为添加文档的操作： 12345678910111213//构建一个 IndexRequest 请求，参数就是索引名称IndexRequest request = new IndexRequest(\"book\");XContentBuilder jsonBuilder = XContentFactory.jsonBuilder();jsonBuilder.startObject();jsonBuilder.field(\"name\", \"西游记\");jsonBuilder.field(\"author\", \"吴承恩\");jsonBuilder.endObject();request.source(jsonBuilder).id(\"99\");//这是一个添加操作，不要自动调整为更新操作request.opType(DocWriteRequest.OpType.CREATE);//执行请求，有同步和异步两种方式//同步IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); 29.2 获取文档根据 id 获取文档： 1234567891011121314151617181920212223public class GetDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetRequest request = new GetRequest(\"book\", \"98\"); GetResponse response = client.get(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); if (response.isExists()) { //如果文档存在 long version = response.getVersion(); System.out.println(\"version = \" + version); String sourceAsString = response.getSourceAsString(); System.out.println(\"sourceAsString = \" + sourceAsString); }else{ System.out.println(\"文档不存在\"); } client.close(); }} 29.3 判断文档是否存在判断文档是否存在和获取文档的 API 是一致的。只不过在判断文档是否存在时，不需要获取 source。 1234567891011121314public class ExistsDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetRequest request = new GetRequest(\"book\", \"99\"); request.fetchSourceContext(new FetchSourceContext(false)); boolean exists = client.exists(request, RequestOptions.DEFAULT); System.out.println(\"exists = \" + exists); client.close(); }} 29.4 删除文档删除 id 为 99 的文档： 123456789101112131415161718192021222324public class DeleteDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); DeleteRequest request = new DeleteRequest(\"book\", \"99\"); DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); ReplicationResponse.ShardInfo shardInfo = response.getShardInfo(); if (shardInfo.getTotal() != shardInfo.getSuccessful()) { System.out.println(\"有分片存在问题\"); } if (shardInfo.getFailed() &gt; 0) { for (ReplicationResponse.ShardInfo.Failure failure : shardInfo.getFailures()) { System.out.println(\"failure.reason() = \" + failure.reason()); } } client.close(); }} 删除文档的响应和添加文档成功的响应类似，可以对照着理解。 29.4 更新文档通过脚本更新： 12345678910111213141516171819202122public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); //通过脚本更新 Map&lt;String, Object&gt; params = Collections.singletonMap(\"name\", \"三国演义666\"); Script inline = new Script(ScriptType.INLINE, \"painless\", \"ctx._source.name=params.name\", params); request.script(inline); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 通过 JSON 更新： 12345678910111213141516171819public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); request.doc(\"{\\\"name\\\": \\\"三国演义\\\"}\", XContentType.JSON); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 当然，这个 JSON 字符串也可以通过 Map 或者 XContentBuilder 来构建： Map: 123456789101112131415161718192021public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); Map&lt;String, Object&gt; docMap = new HashMap&lt;&gt;(); docMap.put(\"name\", \"三国演义888\"); request.doc(docMap); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} XContentBuilder: 1234567891011121314151617181920212223public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); jsonBuilder.startObject(); jsonBuilder.field(\"name\", \"三国演义666\"); jsonBuilder.endObject(); request.doc(jsonBuilder); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 也可以通过 upsert 方法实现文档不存在时就添加文档： 1234567891011121314151617181920212223242526public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"99\"); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); jsonBuilder.startObject(); jsonBuilder.field(\"name\", \"三国演义666\"); jsonBuilder.endObject(); request.doc(jsonBuilder); request.upsert(\"{\\\"name\\\": \\\"红楼梦\\\",\\\"author\\\": \\\"曹雪芹\\\"}\", XContentType.JSON); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } else if (response.getResult() == DocWriteResponse.Result.CREATED) { System.out.println(\"文档添加成功\"); } client.close(); }}","link":"/2021/04/13/ES%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"vue","slug":"vue","link":"/tags/vue/"},{"name":"eladmin","slug":"eladmin","link":"/tags/eladmin/"},{"name":"githubpage","slug":"githubpage","link":"/tags/githubpage/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"es","slug":"es","link":"/tags/es/"}],"categories":[{"name":"eladmin","slug":"eladmin","link":"/categories/eladmin/"},{"name":"githubpage","slug":"githubpage","link":"/categories/githubpage/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"es","slug":"es","link":"/categories/es/"}]}