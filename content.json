{"pages":[{"title":"contact","text":"","link":"/contact/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"404","text":"","link":"/404/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"friends","text":"","link":"/friends/index.html"}],"posts":[{"title":"DB2 windows下9.5安装教程","text":"前言 DB2 windows下9.5安装教程 DB2 windows下9.5安装教程1.下载安装包，解压。打开应用程序2.如图，再次选择好解压路径，点击Unzip 3.进入解压好的文件夹，点击setup.exe 4.进入安装页面，选择安装产品，然后根据你的需要选择版本，这里我选择的是企业版 5.等待检测，选择下一步 6.接收许可，下一步 7.根据需求选择安装类型，这里我选择默认的典型安装。如果不熟悉DB2的功能组件和功能使用，不建议选择定制安装，一般选择典型安装即可。 8.选择响应文件安装目录，下一步。默认C盘,可以选择其他磁盘安装。关于响应文件，查了下资料，保存：在执行手动安装时，您可以让安装程序生成响应文件，其中记录您在安装过程中选择的选项，然后根据需要做出某些较小的修改。如果您要在大量的计算机上安装完全相同的组件，这是相当有用的。编辑：在公共模板基础上修改响应文件。模板响应文件显示您可以指定的所有可能选项，但是这样更加困难，因为您必须是响应文件模板方面的专家。在自动安装脚本中，已经提供了几个响应文件。您可以直接使用它们，做出某些修改，或者用户可以覆盖响应文件。清单 1 显示了 DB2 响应文件的示例。简而言之，是用于批量安装DB2数据库，保存手动安装时的选项，也可以进行编辑，因为他实际上也是文本。 9.选择数据库安装文件夹，默认C盘，建议安装在非系统盘。 10.设置用户信息。这里说明一下用户，DB2数据库的用户使用的是系统本地的用户。建议摄入你当前使用的本地用户和密码，如果输入一个新的而系统不存在的用户，DB2将为你的系统新建一个用户。 11.配置实例，点击配置，可以查看当前实例的配置，一般不需要更改。默认下一步。 12. 勾选“准备DB2工具目录”，点击“下一步” 13.设置通知，不需要设置通知，取消设置，下一步。 14.启用操作系统安全性，默认，下一步。如果取消启用，使用数据库将可能报错。 15.点击完成，等待完成安装。 16.安装完成后，重启系统，才可以正常使用DB2数据库。17.重启后，搜索框输入“第一步”，点击打开。这是一个简单的DB2数据库使用帮助。你可以根据上面的教程创建SAMPLE 基本数据库 或者选择创建您自己的数据库。如果想了解更多DB2的使用，可以仔细阅读上面的文档。 18.安装可视化工具DbVisualizer 9.0.7 19.一路下一步+同意协议+调整安装位置可连接默认数据库 TOOLSDB 20.会出现许可过期 这时需要找一个永久的license添加到db2即可db2licm -l命令可以查看到db2的license信息。 1db2licm -l 如果license过期： 可以找一个永久的license添加到db2即可 把db2ese_c.lic放到目录下，一般都是：\\db2\\license下然后cd到该目录下，执行： 1db2licm -a db2ese_c.lic 重启数据库连接工具，发现已经ok了 参考:https://blog.csdn.net/weixin_43835492/article/details/115858923","link":"/2021/05/11/DB2%20windows%E4%B8%8B9.5%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"},{"title":"DB2v9.7Linux安装","text":"前言 DB2v9.7Linux安装 123下载地址：ftp://public.dhe.ibm.com/software/hk/cobra/db2exc_nlpack_970_LNX_x86.tar.gz建议迅雷下载安装环境：centos7 DB2v9.7Express-C安装1.解压 1tar -zxvf db2exc_970_LNX_x86_64.tar.gz 2.检查安装条件 cd进入解压的后的文件中 123456./db2prereqcheckWARNING: The 32 bit library file libstdc++.so.6 is not found on the system. 32-bit applcations may be affected.#出现warning 按照提示安装yum install -y libstdc++.so.6 3.安装 1./db2_install 4.创建DB2运行所需要的用户组和用户 123456groupadd -g 901 db2grpgroupadd -g 902 db2fgrpgroupadd -g 903 db2agrpuseradd -g db2grp -u 801 -d /home/db2inst1 -m -s /bin/sh db2inst1useradd -g db2fgrp -u 802 -d /home/db2fenc -m -s /bin/sh db2fencuseradd -g db2agrp -u 803 -d /home/db2das -m -s /bin/sh db2das 5.添加密码 123passwd db2inst1 我设置的分别为zyksdb21 zyksdb22 zyksdb22passwd db2fencpasswd db2das 6.进入/opt/ibm/db2/V9.7/instance目录 12345cd /opt/ibm/db2/V9.7/instance执行以下命令./dascrt -u db2das./db2icrt -u db2inst1 db2inst1这里dascrt创建的是DB2 adminstration server，每台服务器只有一个这种server，为进行DB2管理（比如运行控制中心）所必须，同时指定其管理用户是db2das。db2icrt创建的是实例，其名字一般和管理用户名一样，这里均为db2inst1 6.db2set -g DB2SYSTEM=localhost.localdomain db2 terminate 1234566.vi /home/db2inst1/.bash_profile插入下面这句话：# The following three lines have been added by IBM DB2 instance utilities.if [ -f /home/db2inst1/sqllib/db2profile ]; then . /home/db2inst1/sqllib/db2profilefi db2 不起作用 如果配置了 还是不起作用那么就只有每次切换db2inst1用户后手动执行source /home/db2inst1/sqllib/db2profile 进行环境变量初始化 123457.vi /home/db2inst1/.profileexport PATH=$PATH:/home/db2inst1/sqllib/adm:/home/db2inst1/bin8.vi /home/db2das/.profileexport PATH=$PATH:/home/db2das/sqllib/adm:/home/db2das/bin 9.切换到db2das用户，执行db2admin启动DB2管理服务器。 1234cd /opt/ibm/db2/V9.7/bindb2admin start切换到db2inst1用户，执行db2start启动数据库实例。db2start 10. 配置DB2 12345678910111213141516171 设置DB2自启动。 使用root用户执行以下命令： cd /opt/ibm/db2/V9.7/instance ./db2iauto -on db2inst1 设置对db2inst1在LINUX启动时自动启动。2 配置网络 切换到db2inst1用户。 su - db2inst1 修改DB2的服务端口为50000，这里默认端口就是50000。 db2 update dbm cfg using SVCENAME 50000 修改DB2连接方式为TCPIP，然后可通过JDBC、ODBC等访问本DB2服务器上的数据库，安装了DB2客户端的其它机器也可访问数据库。 db2set DB2COMM=TCPIP3 禁用防火墙 vi /etc/selinux/config,修改为： SELINUX=disabled4 服务禁用防火墙 显示状态： firewall-cmd –state关闭：systemctl stop firewalld开机禁用 ： systemctl disable firewalld 11.Centos7提示” xxx 不在 sudoers 文件中。此事将被报告。” 12345sucd /etcchmod 740 sudoersvim sudoersfendo ALL=(root) ALL, !/usr/bin/passwd [A-Za-z]*, !/usr/bin/passwd root 五．批注 备注： 创建和访问数据库，安装验证1 启动与关闭数据库实例su - db2ins1db2startdb2 force applications alldb2stop（在调用toad连接创建数据库时，先自己在命令行下随意创建一个表，用来初始化这个用户的schema）db2 create database test1 2 DB2 for linux卸载由于某种原因，要卸载DB2再重新安装，一定要完全卸载DB2，否则不能重新安装或安装后的DB2不可用。因为卸载步骤比较复杂，我建议在虚拟机上安装该软件的朋友，应该先做一个快照，然后方可进行，出错就恢复快照，重新来过。在主机上卸载则最好先做一个备份，以防万一。为了操作方便，可以同时打开几个Shell，分别属于不同用户，配合完成下面的操作。 12345678910111213141516171819202122232425262728293031323334353637383940411、在linux上卸载DB2的一般过程：a.删除所有数据库。可以使用“控制中心”或drop database命令删除数据库。笔者卸载而未删除数据库，结果是重新安装后无法建立同名数据库。b.停止DB2管理服务器。c.停止DB2实例。d.除去DB2管理服务器。e.除去DB2实例。f.除去DB2产品。2、停止DB2管理服务器：必须要停止DB2管理服务器才能在linux上卸载DB2。a.作为DB2管理服务器所有者登陆。b.用db2admin stop命令停止DB2管理服务器。3、停止DB2实例：必须要停止DB2实例才能在linux上卸载DB2。a.作为具有root用户权限的用户登陆。b.输入/opt/ibm/db2/V9.7/bin/db2ilist命令，获取系统上的所有DB2实例的名称。c.注销。d.作为想要停止的实例的所有者登陆。e.进入该用户的主目录下，运行脚本：. sqllib/db2profiled.输入db2 force application all命令来停止所有数据库应用程序。e.输入db2stop命令来停止DB2数据库管理器。f.输入db2 terminate来确认DB2数据库管理器已停止。g.对每一个要删除的实例重复以上步骤。4、删除DB2管理服务器：必须删除DB2管理服务器才能卸载DB2。a.作为DB2管理服务器所有者登陆。b.进入该用户的主目录下，运行脚本：. das/dasprofile.c.注销。d.作为root登陆，通过输入命令/opt/ibm/db2/V9.7/instance/dasdrop除去DB2管理服务器。5、删除DB2实例：一旦删除系统上的实例，该实例下的所有DB2数据库都将不可用。a.通过输入/opt/ibm/db2/V9.7/instance/db2idrop db2instname删除实例。6、卸载DB2产品以root身份登陆，到DB2版本产品CD-ROM上的根目录或DB2安装文件（通常就是tar解包文件）下找到db2_deinstall命令，运行db2_deinstall －a命令可以删除所有DB2产品。可能需要输入DB2安装路径，这里是/opt/ibm/db2/V9.7然后也可以在LINUX中删除DB2用户，这并非必须，重新安装仍可使用它们。 运行时报错-sh-3.1$ ./db2startSQL10007N Message “-1390” could not be retrieved. Reason code: “3”.sudo usermod -s /bin/bash db2inst1 sudo gedit /home/db2inst1/.profile 添加以下内容 export PATH=$PATH:/home/db2inst1/sqllib/adm:/home/db2inst1/bin 重新尝试以db2inst1用户登录，发现shell已经变了。try!:./unload to extract and ./db2gen.sh","link":"/2021/05/12/DB2v9.7Express-CLinux%E4%B8%8B%E5%AE%89%E8%A3%85/"},{"title":"Java线程池学习总结","text":"前言 Java线程池学习总结 ​ 2021-08-13 08:28:27 参考链接https://snailclimb.gitee.io/javaguide/#/./docs/java/multi-thread/java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93 https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html https://www.cnblogs.com/thisiswhy/p/12690630.html https://blog.csdn.net/woshiluoye9/article/details/60883461 守护线程： Daemon Threadjava中的线程有两类，守护线程和用户线程守护线程指在程序运行在后台的一种特殊线程，为其他的线程服务，比如垃圾回收线程就是一个很称职的守护线程。它并不属于程序中不可或缺的部分，但是确实是很有用的， 如果用户线程已经全部退出运行了，只剩下守护线程存在了，虚拟机也就退出了，但是用户进程只要还存在一个，虚拟机就不会退出。守护线程有几个特点：1、 thread.setDaemon(true)必须在thread.start()之前设置，你不能把一个正在运行的线程转化为守护线程2、 在守护线程内的产生的线程也是守护线程3、 永远不要试图利用守护线程去访问文件、数据库等固有资源，因为守护线程可以被任何用户线程所中断，线程的优先级最低。 还没写完、、、","link":"/2021/08/13/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"},{"title":"DB2v9.7Express-CLinux下安装","text":"前言 DB2v9.7Express-CLinux下安装 1.解压db2安装包1tar -zxvf v9.7_linuxx64_server.tar.gz 2.进入server目录下，执行安装检查12cd server./db2prereqcheck 3.运行安装程序123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@server]./db2_install要选择另一个目录用于安装吗？[yes/no]--输入no默认安装opt下，选择yes自己输入安装目录指定下列其中一个关键字以安装 DB2 产品--输入ESEESE正在初始化 DB2 安装。要执行的任务总数为：47要执行的所有任务的总估计时间为：2070任务 #1 启动描述：正在检查许可协议的接受情况估计时间 1 秒任务 #1 结束…任务 #47 启动描述：正在注册 DB2 更新服务估计时间 30 秒任务 #47 结束任务 #48 启动描述：正在更新全局概要文件注册表估计时间 3 秒任务 #48 结束已成功完成执行。 4.安装license(这一步可以在创建好用户后在db2inst1用户下进行)db2licm -l命令可以查看到db2的license信息。可以找一个永久的license添加到db2数据库即可,把db2ese_c.lic放到一目录下： /opt/ibm/db2/V9.7/license/db2ese_c.lic,在/opt/ibm/db2/V9.7/adm/目录下执行: 1db2licm -a /opt/ibm/db2/V9.7/license/db2ese_c.lic 执行后显示：LIC1402I License added successfully.再用db2licm -l查看，你会发现你的db2变为永久了,大功告成，以后就不怕db2数据库过期了 5.创建DB2运行所需要的用户组和用户123456groupadd -g 901 db2iadm1groupadd -g 902 db2fadm1groupadd -g 903 dasadm1useradd -g db2iadm1 -u 801 -d /home/db2inst1 -m db2inst1useradd -g db2fadm1 -u 802 -d /home/db2fenc1 -m db2fenc1useradd -g dasadm1 -u 803 -d /home/dasadm1 -m dasusr1 6.为db2inst1创建密码1passwd db2inst1 7.创建实例123456789101112[root@server]#cd /opt/ibm/db2/V9.7/instance[root@instance]#./dascrt -u dasusr1SQL4406W The DB2 Administration Server was started successfully.DBI1070I Program dascrt completed successfully.[root@instance]#./db2icrt -u db2inst1 db2inst1DBI1070I Program db2icrt completed successfully. 8.启动db2实例123456789101112[root@instance]#su - dasusr1[dasusr1@db2]$. das/dasprofile[dasusr1@db2]$db2admin start[dasusr1@db2]$su - db2inst1[db2inst1@db2]$. sqllib/db2profile[db2inst1@db2]$db2start 9.关闭、启动数据库12345[db2inst1@db2]$db2stop[db2inst1@db2]$db2 force applications all[db2inst1@db2]$db2start 10.创建样本库123[db2inst1@db2]$cd /opt/ibm/db2/V9.7/bin[db2inst1@db2]$./db2sampl 11.设置DB2自启动123[root@db2]#cd /opt/ibm/db2/V9.7/instance[root@instance]#./db2iauto -on db2inst1 12.配置TCPIP(这一步更改/etc/service建议在root用户下执行)12345678910111213141516171819202122232425262728293031323334353637383940[root@instance]#su - db2inst1[db2inst1@db2]$db2set DB2COMM=TCPIP[db2inst1@db2]$db2 get dbm cfg |grep SVCENAMETCP/IP Service name (SVCENAME) =SSL service name (SSL_SVCENAME) =[db2inst1@db2]$tail /etc/servicesDB2_db2inst1 60000/tcpDB2_db2inst1_1 60001/tcpDB2_db2inst1_2 60002/tcpDB2_db2inst1_END 60003/tcp[db2inst1@db2]$vim /etc/services修改成如下DB2_db2inst1 50000/tcpDB2_db2inst1_1 50001/tcpDB2_db2inst1_2 50002/tcpDB2_db2inst1_END 50003/tcp[db2inst1@db2]$db2 update dbm cfg using SVCENAME 50000[db2inst1@db2]$db2stop[db2inst1@db2]$db2start 附：如果系统为CENTOS7，可能会因为防火墙问题导致50000端口被禁用 解决方法： su - root systemctl stop firewalld.service 或者将50000端口加入防火墙信任： firewall-cmd –permanent –zone=public –add-port=50000/tcp 重启防火墙: 123systemctl stop firewalld.servicesystemctl start firewalld.service 注 阿里云需要在安全组添加","link":"/2021/05/13/DB2v9.7Linux%E5%AE%89%E8%A3%85/"},{"title":"Sklearn机器学习包对iris数据集建模并可视化","text":"前言 Sklearn机器学习包对iris数据集建模并可视化 ​ 2021-04-25 18:05:26 一. 决策树分析鸢尾花Sklearn机器学习包中，决策树实现类是DecisionTreeClassifier，能够执行数据集的多类分类。输入参数为两个数组X[n_samples,n_features]和y[n_samples],X为训练数据，y为训练数据的标记数据。DecisionTreeClassifier构造方法为： 1234sklearn.tree.DecisionTreeClassifier(criterion='gini',&nbsp;splitter='best' ,max_depth=None,&nbsp;min_samples_split=2,&nbsp;min_samples_leaf=1 ,max_features=None,&nbsp;random_state=None,&nbsp;min_density=None ,compute_importances=None,&nbsp;max_leaf_nodes=None) 鸢尾花数据集使用决策树的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierimport numpy as np# 使用sklearn自带的数据集iris = load_iris()#把一部分数据集作为训练，一部分作为预测，这里使用70%的训练，30%的进行预测，其中70%的训练集为0-40、50-90、100-140行，30%的预测集40-50、90-100、140-150行。同时输出准确率、召回率等# 训练集train_data = np.concatenate((iris.data[0:40, :], iris.data[50:90, :], iris.data[100:140, :]), axis=0)train_target = np.concatenate((iris.target[0:40], iris.target[50:90], iris.target[100:140]), axis=0)# 测试集test_data = np.concatenate((iris.data[40:50, :], iris.data[90:100, :], iris.data[140:150, :]), axis=0)test_target = np.concatenate((iris.target[40:50], iris.target[90:100], iris.target[140:150]), axis=0)# 训练clf = DecisionTreeClassifier()clf.fit(train_data, train_target)predict_target = clf.predict(test_data)print(predict_target)# 预测结果与真实结果比对print(sum(predict_target == test_target))from sklearn import metrics# 输出准确率 召回率 F值print(metrics.classification_report(test_target, predict_target))print(metrics.confusion_matrix(test_target, predict_target))X = test_dataL1 = [n[0] for n in X]print(L1)L2 = [n[1] for n in X]print(L2)import matplotlib as matplotlibimport matplotlib.pyplot as pltimport matplotlib.style as stylestyle.use(\"Solarize_Light2\")# plt.scatter中# c:表示的是颜色，也是一个可选项。默认是蓝色'b',表示的是标记的颜色，或者可以是一个表示颜色的字符，或者是一个长度为n的表示颜色的序列等等，感觉还没用到过现在不解释了。但是c不可以是一个单独的RGB数字，也不可以是一个RGBA的序列。可以是他们的2维数组（只有一行）。# marker表示标记符号 默认为oplt.scatter(L1, L2, c=predict_target, marker='o') # cmap=plt.cm.Pairedplt.title(\"DecisionTreeClassifier\")plt.show() 输出结果如下： 1234567891011121314[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]30 precision recall f1-score support 0 1.00 1.00 1.00 10 1 1.00 1.00 1.00 10 2 1.00 1.00 1.00 10 accuracy 1.00 30 准确度 macro avg 1.00 1.00 1.00 30 宏平均weighted avg 1.00 1.00 1.00 30 加权平均[[10 0 0] [ 0 10 0] [ 0 0 10]][5.0, 4.5, 4.4, 5.0, 5.1, 4.8, 5.1, 4.6, 5.3, 5.0, 5.5, 6.1, 5.8, 5.0, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9][3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0] 绘制出的图形如下： x轴为测试集第一列数据，y轴为第二列数据 二. Kmeans聚类分析鸢尾花KMeans聚类鸢尾花的代码如下，它则不需要类标（属于某一类鸢尾花），而是根据数据之间的相似性，按照“物以类聚，人以群分”进行聚类。 12345678910111213141516171819202122232425# -*- coding: utf-8 -*-from sklearn.datasets import load_irisfrom sklearn.cluster import KMeansiris = load_iris()clf = KMeans()clf.fit(iris.data, iris.target)print(clf)predicted = clf.predict(iris.data)# 获取花卉两列数据集X = iris.dataL1 = [x[0] for x in X]print(L1)L2 = [x[1] for x in X]print(L2)import numpy as npimport matplotlib.pyplot as plt# s:是一个实数或者是一个数组大小为(n,)，这个是一个可选的参数。# cmap:Colormap实体或者是一个colormap的名字，cmap仅仅当c是一个浮点数数组的时候才使用。如果没有申明就是image.cmapplt.scatter(L1, L2, c=predicted, marker='s', s=200, cmap=plt.cm.Paired)plt.title(\"Iris\")plt.show() 输出结果为：","link":"/2021/04/25/Sklearn%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8C%85%E5%AF%B9iris%E6%95%B0%E6%8D%AE%E9%9B%86%E5%BB%BA%E6%A8%A1%E5%B9%B6%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"Sql50题","text":"前言 Sql50题 ​ 2021-04-28 01:11:09 b站链接 解题思路 Sql50题1.查询课程编号为“01”的课程比“02”的课程成绩高的所有学生的学号（重点）SELECT a.s_id,c.s_name,a.s_score AS ascore,b.s_score AS bscoreFROM(SELECT s_id,c_id,s_scoreFROM ScoreWHERE c_id = ‘01’) AS aINNER JOIN(SELECT s_id,c_id,s_scoreFROM ScoreWHERE c_id = ‘02’) AS bON a.s_id = b.s_idINNER JOIN Student AS c ON c.s_id = a.s_idWHERE a.s_score &gt; b.s_score inner join(等值连接) 只返回两个表中联结字段相等的行 2.查询平均成绩大于60分的学生的学号和平均成绩SELECT s_id,AVG(s_score)FROM ScoreGROUP BY s_idHAVING AVG(s_score) &gt; 60 3、查询所有学生的学号、姓名、选课数、总成绩（不重要）SELECT st.s_id,st.s_name,COUNT(sc.s_id) AS xuankeshu,SUM(sc.s_score) AS zongchengjiFROM Student stJOIN Score scON st.s_id= sc.s_idGROUP BY sc.s_id,st.s_name 4、查询姓“猴”的老师的个数（不重要）SELECT COUNT(t.t_id)FROM Teacher tWHERE t.t_name LIKE “猴%” 注: 名字可能重复，所以用t_id 5、查询没学过“张三”老师课的学生的学号、姓名（重点）SELECT s_id,s_nameFROM StudentWHERE s_id NOT IN (SELECT s_idFROM ScoreWHERE c_id= ( SELECT c_idFROM CourseWHERE t_id = (SELECT t_idFROM TeacherWHERE t_name = “张三”) )) 错误答案； 因为te.t_name !=’张三’ 查找出另外两个老师，对应的学生 6、查询学过“张三”老师所教的所有课的同学的学号、姓名（重点）SELECT s_id,s_nameFROM StudentWHERE s_id IN (SELECT s_idFROM ScoreWHERE c_id= ( SELECT c_idFROM CourseWHERE t_id = (SELECT t_idFROM TeacherWHERE t_name = “张三”) )) 7、查询学过编号为“01”的课程并且也学过编号为“02”的课程的学生的学号、姓名（重点）我的答案 SELECT s_idFROM ScoreWHERE c_id = 2 IN(SELECT s_idFROM ScoreWHERE c_id = 1) 错的 ，这个还包含了学了3的 正确答案 SELECT *FROM StudentWHERE s_id IN(SELECT a.s_idFROM(SELECT s_idFROM ScoreWHERE c_id = 2)aINNER JOIN(SELECT s_idFROM ScoreWHERE c_id = 1)b ON a.s_id = b.s_id ) 8、查询课程编号为“02”的总成绩（不重点）SELECT SUM(s_score)FROM ScoreWHERE c_id = 2 9、查询所有课程成绩小于60分的学生的学号、姓名题目的意思是他所有课都小于60分而不是所有的有挂科的人 SELECT s_id,s_nameFROM StudentWHERE s_id IN(SELECT s_idFROM ScoreWHERE s_score &lt; 60) 10.查询没有学全所有课的学生的学号、姓名(重点)SET SESSION sql_mode=(SELECT REPLACE(@@sql_mode,’ONLY_FULL_GROUP_BY,’,’’));这个有效SET GLOBAL sql_mode=(SELECT REPLACE(@@sql_mode,’ONLY_FULL_GROUP_BY’,’’));这个无效 很奇怪 SELECT st.,sc.FROM Student AS stLEFT JOIN Score AS sc ON st.s_id = sc.s_idGROUP BY st.s_idHAVING COUNT(DISTINCT sc.c_id) &lt; (SELECT COUNT(DISTINCT c_id) FROM Course); 11、查询至少有一门课与学号为“01”的学生所学课程相同的学生的学号和姓名（重点）我的答案 SELECT DISTINCT st.s_id,st.s_nameFROM Student stLEFT JOIN Score sc ON st.s_id = sc.s_idWHERE sc.c_id IN( ) 漏了不能有01 示范答案 SELECT DISTINCT st.s_id,st.s_nameFROM Student stINNER JOIN Score sc ON st.s_id = sc.s_idWHERE sc.c_id IN(SELECT c_idFROM ScoreWHERE s_id = ‘01’)AND st.s_id!=’01’ 12.查询和“01”号同学所学课程完全相同的其他同学的学号(重点)SELECT s_id, s_nameFROM studentWHERE s_id IN ( SELECT s_id FROM score WHERE s_id != ‘01’ GROUP BY s_id HAVING COUNT( DISTINCT c_id ) = ( SELECT COUNT( DISTINCT c_id ) FROM score WHERE s_id = ‘01’ ) AND s_id NOT IN ( SELECT DISTINCT s_id FROM score WHERE c_id NOT IN ( SELECT c_id FROM score WHERE s_id = ‘01’ ) ) ) 13、查询没学过”张三”老师讲授的任一门课程的学生姓名 和47题一样（重点，能做出来）我的答案 SELECT s_id, s_nameFROM studentWHERE s_id NOT IN ( SELECT DISTINCT s_id FROM score WHERE c_id =( SELECT c_id FROM course WHERE t_id = ( SELECT t_id FROM teacher WHERE t_name = ‘张三’ ) ) ) 15、查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩（重点）SELECT *FROM studentWHERE s_id IN ( SELECT s_id FROM score WHERE NOT s_score &gt;= 60 GROUP BY s_id HAVING COUNT( c_id )&gt;= 2 ) 16、检索”01”课程分数小于60，按分数降序排列的学生信息（和34题重复，不重点）SELECT st.*FROM student AS st LEFT JOIN score AS sc ON st.s_id = sc.s_idWHERE sc.s_score &lt; 60 AND sc.c_id = ‘01’ORDER BY sc.s_score DESC 17、按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩(重重点与35一样)SELECT s_id,MAX(case when c_id=’02’ THEN s_score ELSE NULL END) ‘数学’,MAX(case when c_id=’01’ THEN s_score ELSE NULL END) ‘语文’,MAX(case when c_id=’03’ THEN s_score ELSE NULL END) ‘英语’,AVG(s_score)from scoreGROUP BY s_idORDER BY AVG(s_score) DESC 18.查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率–及格为&gt;=60，中等为：70-80，优良为：80-90，优秀为：&gt;=90 (超级重点) SELECT sc.c_id,co.c_name,MAX(sc.s_score) max,MIN(sc.s_score) min,AVG(sc.s_score) avg,avg(case when sc.s_score &gt; 60 THEN 1.0 ELSE 0.0 end),avg(case when sc.s_score &gt; 70 AND sc.s_score &lt; 80 THEN 1.0 ELSE 0.0 end),avg(case when sc.s_score &gt; 80 AND sc.s_score &lt;90 THEN 1.0 ELSE 0.0 end),avg(case when sc.s_score &gt; 90 THEN 1.0 ELSE 0.0 end)from score scLEFT JOIN course co on co.c_id = sc.c_idGROUP BY sc.c_id 19、按各科成绩进行排序，并显示排名(重点row_number)row_number(）over （order by 列） mysql8.0窗口函数 20、查询学生的总成绩并进行排名（不重点）SELECT s_id,sum(s_score)FROM scoreGROUP BY s_idORDER BY sum(s_score) DESC 21 、查询不同老师所教不同课程平均分从高到低显示(不重点)SELECT c_id,AVG(s_score)FROM scoreGROUP BY c_idORDER BY AVG(s_score) DESC 22、查询所有课程的成绩第2名到第3名的学生信息及该课程成绩（重要 25类似）mysql8.0窗口函数 23、使用分段[100-85],[85-70],[70-60],[&lt;60]来统计各科成绩，分别统计各分数段人数：课程ID和课程名称(重点和18题类似)24、查询学生平均成绩及其名次（同19题，重点）25、查询各科成绩前三名的记录（不考虑成绩并列情况）（重点 与22题类似）26、查询每门课程被选修的学生数(不重点)SELECT c_id,COUNT(s_id)FROM scoreGROUP BY c_id 27、 查询出只有两门课程的全部学生的学号和姓名(不重点)SELECT st.s_id,st.s_name,COUNT(sc.c_id) FROM student st INNER JOIN score scon st.s_id = sc.s_idGROUP BY st.s_idHAVING COUNT(sc.c_id) = 2 我的答案 SELECT s_id,s_nameFROM studentWHERE s_id in(SELECT s_idFROM scoreGROUP BY s_idHAVING COUNT(c_id) = 2) 28、查询男生、女生人数(不重点)SELECT s_sex,COUNT(s_sex)FROM studentGROUP BY s_sex 29 查询名字中含有”风”字的学生信息（不重点）SELECT *FROM studentWHERE s_name like “%风%” 31、查询1990年出生的学生名单（重点year）方法一 SELECT *FROM studentWHERE YEAR(s_birth) = 1990; 方法二 SELECT * FROM student WHERE s_birth LIKE “1990%” 32、查询平均成绩大于等于85的所有学生的学号、姓名和平均成绩（不重要）SELECT score.s_id, student.s_name, AVG( score.s_score )FROM score INNER JOIN student ON score.s_id = student.s_idGROUP BY s_idHAVING AVG( score.s_score )&gt;= 85 33、查询每门课程的平均成绩，结果按平均成绩升序排序，平均成绩相同时，按课程号降序排列（不重要）SELECT c_id, AVG( s_score )FROM scoreGROUP BY c_idORDER BY AVG( s_score ), c_id DESC 34、查询课程名称为”数学”，且分数低于60的学生姓名和分数（不重点）SELECT st.s_name, sc.s_scoreFROM score sc INNER JOIN student st ON st.s_id = sc.s_id INNER JOIN course co ON sc.c_id = co.c_idWHERE co.c_name = “数学” AND sc.s_score &lt; 60 35、查询所有学生的课程及分数情况（重点）备注:1.因为要选出需要的字段 用case when 当co.c_name=’数学’ then 可以得到对应的 sc.s_core 2.因为GROUP UP 要与select 列一致，所以case when 加修饰max 3.因为最后要展现出每个同学的各科成绩为一行，所以用到case SELECT sc.s_id, MAX( CASE WHEN co.c_name = “数学” THEN sc.s_score ELSE NULL END ), MAX( CASE WHEN co.c_name = “语文” THEN sc.s_score ELSE NULL END ), MAX( CASE WHEN co.c_name = “英语” THEN sc.s_score ELSE NULL END )FROM score sc INNER JOIN course co ON co.c_id = sc.c_idGROUP BY sc.s_id 36、查询任何一门课程成绩在70分以上的姓名、课程名称和分数（重点）SELECT st.s_name, co.c_name, sc.s_scoreFROM score sc LEFT JOIN student st ON sc.s_id = st.s_id LEFT JOIN course co ON sc.c_id = co.c_idWHERE sc.s_score &gt; 70 37、查询不及格的课程并按课程号从大到小排列(不重点)SELECT st.s_name, co.c_name, sc.s_score, sc.c_idFROM score sc LEFT JOIN student st ON sc.s_id = st.s_id LEFT JOIN course co ON sc.c_id = co.c_idWHERE sc.s_score &lt; 60ORDER BY sc.c_id asc 38、查询课程编号为03且课程成绩在80分以上的学生的学号和姓名（不重要）SELECT st.s_id, st.s_nameFROM score sc LEFT JOIN student st ON sc.s_id = st.s_idWHERE sc.c_id = ‘03’ AND sc.s_score &gt; 80 39、求每门课程的学生人数（不重要）SELECT c_id, COUNT( s_id )FROM scoreGROUP BY c_id 40、查询选修“张三”老师所授课程的学生中成绩最高的学生姓名及其成绩（重要top）SQL SERVER 中用top MYSQL 用 limit select 筛选的是orderby 后的数 SELECT st.s_id, st.s_name, sc.s_scoreFROM course co INNER JOIN score sc ON co.c_id = sc.c_id INNER JOIN student st ON sc.s_id = st.s_id INNER JOIN teacher te ON co.t_id = te.t_idWHERE te.t_name = “张三”ORDER BY sc.s_score DESC LIMIT 1,3 41.查询不同课程成绩相同的学生的学生编号、课程编号、学生成绩 （重点）SELECT DISTINCT sc1.s_id, sc1.c_id, sc1.s_scoreFROM score sc1 INNER JOIN score sc2 ON sc1.s_id = sc2.s_idWHERE sc1.s_score = sc2.s_score AND sc1.c_id != sc2.c_id 42、查询每门功成绩最好的前两名（同22和25题）43、统计每门课程的学生选修人数（超过5人的课程才统计）。要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列（不重要）SELECT c_id,count(c_id)from scoreGROUP BY c_idHAVING COUNT(c_id) &gt; 5ORDER BY COUNT(c_id) DESC,c_id 44、检索至少选修两门课程的学生学号（不重要）SELECT s_id,COUNT(c_id)from scoreGROUP BY s_idHAVING COUNT(c_id) &gt;= 2 45、 查询选修了全部课程的学生信息（重点划红线地方）SELECT s_id,COUNT(c_id)from scoreGROUP BY s_idHAVING COUNT(c_id) = (SELECT COUNT(c_id) FROM course) 47、查询没学过“张三”老师讲授的任一门课程的学生姓名（还可以，自己写的，答案中没有）SELECT s_nameFROM studentWHERE s_id NOT IN ( SELECT s_id FROM score WHERE c_id = ( SELECT c_id FROM course WHERE t_id = ( SELECT t_id FROM teacher WHERE t_name = “张三” ) ) ) 48、查询两门以上不及格课程的同学的学号及其平均成绩（还可以，自己写的，答案中没有） SELECT s_id,AVG(s_score)from scoreWHERE s_score &lt;60GROUP BY s_idHAVING COUNT(s_score)&gt;=2 46、查询各学生的年龄（精确到月份） 备注：年份转换成月份，比如结果是1.9，ditediff 最后取1年 SELECT s_id,s_birth,DATEDIFF(‘2020-11-30’,s_birth)/365FROM student 不对，但是mysql不知道用哪个函数 47、查询本月过生日的学生（无法使用week、date(now()） SELECT s_id,s_name,s_birth,MONTH(s_birth)FROM studentWHERE MONTH(s_birth) = MONTH(NOW())","link":"/2021/04/28/Sql50%E9%A2%98/"},{"title":"Vuex学习","text":"前言 Vuex学习 2021-04-22 13:07:08 Vuex学习State单一状态树 js浅拷贝与深拷贝的区别和实现方式 源链接 如何区分深拷贝与浅拷贝，简单点来说，就是假设B复制了A，当修改A时，看B是否会发生变化，如果B也跟着变了，说明这是浅拷贝，拿人手短，如果B没变，那就是深拷贝，自食其力。 如果是基本数据类型，名字和值都会储存在栈内存中 1234var a = 1;b = a; // 栈内存会开辟一个新的内存空间，此时b和a都是相互独立的b = 2;console.log(a); // 1 当然，这也算不上深拷贝，因为深拷贝本身只针对较为复杂的object类型数据。 如果是引用数据类型，名字存在栈内存中，值存在堆内存中，但是栈内存会提供一个引用的地址指向堆内存中的值","link":"/2021/04/22/Vuex%E5%AD%A6%E4%B9%A0/"},{"title":"db2window数据及结构迁移linux","text":"前言 db2window数据及结构迁移linux 环境 12win10 db2v9.5linux db2v9.7 一、操作系统相同之脱机备份原本准备使用备份backup 与 restore 但是有与系统不同失败了 123456--断开连接db2 force application all --备份数据库 db2 backup database &lt;db_name&gt; to &lt;location&gt; --恢复数据库，20161101134642 为备份文件时间戳 db2 restore database &lt;db_name&gt; from &lt;location&gt; taken at 20161101134642 据向下兼容原则，版本相同或者低版本的数据库可以还原在高版本上。 二、操作系统不同，或者源数据库版本较高DB2 提供了两个非常实用的工具： ★数据迁移工具 db2move ★数据字典获取工具 db2look 以下为Windows 环境迁移到Linux下操作小结： 1、登录Windows&gt;db2cmd,使用 db2move 命令将源数据库（TEST）数据导出至指定的文件夹D:\\db2move 下： 1D:\\db2move&gt;db2move TEST export -u db2inst1 -p 123456 123# 另可以将导出操作限制在特定的表(-tn)、表空间(-ts)、表创建者(-tc)、表模式 (-sn)的范围内。-- db2move TEST export -sn test -u db2inst1 -p 123456（密码）-- 将test模式下的所有数据导出。 执行成功后会显示 Disconnecting from database … successful! 2、使用 db2look 命令将数据库结构（DDL文件） 导出至指定的文件夹D:\\db2look下： 1D:\\db2look&gt; db2look -d TEST -e -a -o db2look_TEST.sql ps：参数说明： -d 为指定数据库，必须参数 -e 抽取数据库对象的DDL，必须参数 -a 所有用户和模式，(-u test01 可以指定用户， -a 和 -u 都没有时默认当前登录用户) -o 指定输出文件名称 3、将db2move和db2look文件上传至Linux系统下：4、Linux下db2用户登录，同步数据结构，载入数据： ★更新表结构： 123su - db2inst1:password:123456 db2inst1@localhost:~/db2back/db2look&gt; db2 -tvf db2look_TEST.sql 出现缺表可能是表空间太小 注意选择较大的表空间即可 参考 https://blog.csdn.net/weixin_33901641/article/details/90500494 具体指令为 1234567#创建pagesize为32K的bufferpooldb2 =&gt; create BUFFERPOOL bigbuffer SIZE 5000 PAGESIZE 32KDB20000I The SQL command completed successfully. #创建pagesize为32K的tablespace，同时使用新创建的bufferpooldb2 =&gt; CREATE TABLESPACE bigtablespace PAGESIZE 32K BUFFERPOOL bigbufferDB20000I The SQL command completed successfully. ★装载数据： 1db2inst1@localhost:~/db2back/db2move&gt; db2move TEST load 在这需要注意操作角色需要有对于文件的写入权限且 load指令在含有db2move.list的目录下 操作顺利的话，数据已迁移至linux&gt;db2。要注意几个问题：★关于表模式 关于表模式，Windows下面默认用户db2admin ,默认表模式也是db2admin ，而linux下面的默认用户是db2inst1 表模式也是db2inst1， 所以需要做以下处理： D:\\DBBack\\CNAS\\db2look\\TEST\\db2look_TEST.sql里面的db2admin字符全部替换成db2inst1 D:\\DBBack\\CNAS\\db2move\\TEST\\db2move.lst 也做同样的操作 ★CHECK表状态，修改暂挂状态的表 在db2move过程中会有些表因为检查约束可能会处于暂挂状态，需要执行SET INTEGRITY命令来恢复它的暂挂状态。可以从系统表中检索处于检查暂挂状态的表信息 Select tabname from syscat.tables where status=’C’ —暂挂状态的表信息对暂挂的表执行 1set integrity for usertbl ALLOW NO ACCESS immediate checked 遗漏的表 导出的时候，可能会有个别表的数据丢失，这时候只能对相应的表执行db2move命令重新load了，如果还是不行就重建表再loadload单个表的命令 1db2 load from tab11.ixf of ixf terminate into db2admin.tablename --tab11.ixf对应的是tablename表 12关键，可能需要call sysproc.admin_cmd (‘reorg table schema名.表名’) 三、Linux下数据导出参看: http://www.ibm.com/developerworks/cn/education/data/db2-cert7315/section6.html 和 http://www.ibm.com/developerworks/cn/data/library/techarticles/dm-0712xiam/index.html?ca=drs The below is how to do the db2 data migration. (1).Export source data from source DB(导出至指定数据库) 12db2move &lt;database_name&gt; export -sn &lt;database_schema&gt; -u &lt;DBA&gt; -p &lt;DBA_PASSWORD&gt; &gt; &lt;logFile&gt;db2move sample export -sn NPMM -u db2inst1 -p db2inst1 For example: 1db2move GEHGAL export -sn GEH_ADMIN -u GEH_ADMIN -p GEH_ADMIN &gt; mv.log If you want to export the db DDL schema from source DB, you should use the below db2 commands: 1db2look -d GEHGAL -e -o ddlfile -i GEH_ADMIN -w GEH_ADMIN or 12db2look -d GEHGAL -u GEH_ADMIN -e -o alltables.sqldb2look -d sample -z NPMM -u db2inst1 -e -o NP.sql 以上为导出 (2).Create a 32k normal or large tablespace 1db2 CREATE LARGE TABLESPACE LARGEGEHGAL32 PAGESIZE 32 K MANAGED BY DATABASE USING (FILE 'C:\\DB2\\NODE0000\\SQL00001\\largegehgal32' 20000) BUFFERPOOL IBMDEFAULT32K (3).Create the db2 user that should be same name as exported that is from source DB in target DB2 database, assign the 32k tablespace to the user, and delete the other tablespaces(4).Code page from target DB should be same as the code page of source DB. You should use the following command to check: 1db2 get db cfg If they are different, change the code page in target DB, use the following command: 12db2set db2codepage=1252 (1252 is the page code of source DB)db2 terminate db2 terminate command must be executed(**注意一定要进行terminate**)(5).Create a 16k or 32k temporary tablespace. It will be used when you view the Project Exception List from deployed GEH GUI 1DB2 CREATE TEMPORARY TABLESPACE gehgalsystemtmp32 IN DATABASE PARTITION GROUP IBMTEMPGROUP PAGESIZE 32K MANAGED BY SYSTEM USING ('C:\\DB2\\NODE0000\\SQL00001\\gehgalsystemtmp32') EXTENTSIZE 32 PREFETCHSIZE 16 BUFFERPOOL IBMDEFAULT32K (6).Import the data into the target DB 1db2move &lt;database_name&gt; import -io create -u &lt;DBA&gt; -p &lt;DBA_PASSWORD&gt; &gt; &lt;logFile&gt; For example: 1db2move GEHGAL import -io create -u GEH_MIGRATION -p Gal@pass &gt; imp.log Check the imp.log whether the import operation is successful. If the table schema has been created/existed in target DB, so you can use the following command: 1db2move GEHGAL import -io INSERT_UPDATE -u GEH_MIGRATION -p Gal@pass &gt; imp.log (7).If the export operation is successful, and you will find some tables in [color=green]userspace1 tablespace[/color], and the other tables in [color=green]LARGEGEHGAL32 tablespace[/color](8).If there are some procedures in your db schema, you should [color=green]execute the procedures that will not be imported into target database[/color](9).You should change the tables that have identify column if you want to store new messages in the target DB. The below is the commands that reset the identify value of EXCEPTION_MESSAGE table. 123select max(EXCEPTION_MESSAGE_ID) from EXCEPTION_MESSAGE$nextMessageId = max(EXCEPTION_MESSAGE_ID) + 1alter table EXCEPTION_MESSAGE alter column EXCEPTION_MESSAGE_ID restart with $nextMessageId import usage 1db2move &lt;database-name&gt; &lt;action&gt; [&lt;option&gt; &lt;value&gt;] 首先，您必须指定数据库名（想要移动的表所在的数据库）和要执行的操作(export和import或load)。然后指定一个选项来定义操作的范围。例如，可以将一个操作限制在特定的表（-tn）、表空间（-ts）、表创建者（-tc）或模式名（-sn）范围内。指定表、表空间或表的创建者的一个子集只对export操作有效。如果指定多个值，就必须使用逗号将其分隔开；在值列表项之间不允许有空格。可以指定的项最多为10个。 另外，也可以指定-tf选项，此时要使用一个文件名作为参数，其中列出了要导出的表名；在该文件中，每行只能列出一个完整的表名。您还可以指定以下内容：-io import-option指定DB2的import工具可以运行的一种模式。有效的选项有：CREATE、INSERT、INSERT_UPDATE、REPLACE和REPLACE_CREATE。缺省值为REPLACE_CREATE。 参看: http://publib.boulder.ibm.com/infocenter/db2luw/v8/index.jsp?topic=/com.ibm.db2.udb.doc/core/r0008304.html -lo load-option指定DB2的load工具可以运行的一种模式。有效的选项有：INSERT和REPLACE。缺省值为INSERT。 -l lobpaths指定要创建或查找的LOB文件的位置。必须指定一个或多个绝对路径名。如果指定了多个绝对路径，就必须使用逗号将其分隔开；值之间不允许有空格。缺省值是当前目录。 -u userid指定一个用户ID，该工具可以使用这个用户ID登录到远程系统上。 -p password指定对该用户进行认证的密码；该工具需要使用一个有效的用户ID和密码登录到远程系统上。 [b]db2codepage 设置[/b]1、db2 变量查看db2set -all(connect to dbanme ) get db cfgdb2pd -osinfo 2、db2c变量的设置用命令db2set 变量=value可以参考一下：客户端：db2codepage=1386(简体中文)db2country=86(中国)db2comm=tcpip 服务器端：db2codepage=1386(简体中文)db2country=86(中国)db2comm=tcpip一定要把缺省的db2codepage=819改为数据库的代码页设置","link":"/2021/05/14/db2window%E6%95%B0%E6%8D%AE%E5%8F%8A%E7%BB%93%E6%9E%84%E8%BF%81%E7%A7%BBlinux/"},{"title":"使用cloudflare免费加速github page","text":"前言 使用cloudflare免费加速github page ​ 2021-07-26 14:00:16 使用cloudflare免费加速github page前言github page 在国内访问速度非常慢，而且近期 github.io 的域名经常被干扰解析成127.0.0.1，迫于无奈在网上找到了一个能白嫖加速 github page 的办法，就是套一层 cloudflare CDN，虽然它在国内没有 CDN 节点，但是整体效果是完爆 github.io，不过要注意的是免费版本是有请求次数限制的，每天 10W 次，当然这足够我的小博客使用了，这里记录一下操作步骤。 准备准备域名随便到哪买一个 国内好像得备案 设置 github page保存之后 github 会自动的在仓库根目录里生成一个CNAME文件，里面存储着域名配置信息 设置域名解析可以使用这个 https://zijian.aliyun.com/?spm=a2c1d.8251892.content.11.7c5c5b76F5cVb1#/domainDetect 通过域名提供商，修改刚刚的域名解析，通过 A 记录分别解析到以下 4 个 IP： 添加到自己的 域名解析那里 当记录全部解析生效时，就可以通过你自己的设置的域名访问到博客了，这个时候再开启HTTPS，示例图： 然后 github 会自动签发提供给你自己的设置的域名域名使用的 SSL 证书，等待一段时间后，就可以通过HTTPS访问博客了。 使用 cloudflare CDN上面的步骤全部就绪之后，就可以开始白嫖之路了 先通过https://dash.cloudflare.com/sign-up链接进行注册 添加站点，把对应的域名填写进去： 提交之后会自动扫描域名对应的解析记录： 查看 cloudfalre 对应的 NS 记录 通过域名的运营商修改对应的 NS 记录，这里每个运营商的修改方式都不一样，我这里是用的阿里云的： 这样就设置完毕了 可以看到 dns 解析的 ip 已经变了，已经被 cloudflare 接管了，然后清除下浏览器 DNS 缓存，chrome 浏览器输入chrome://net-internals/#dns进入清除页： 再次访问你自己的设置的域名，F12 打开网络面板可以看到已经用上了 CDN 了： 后记一直白嫖一直爽，但是cloudflare不一定一直会提供免费版的，如果有一天它挂了，只需要把 DNS 的 NS 解析记录再还原回去就行了。","link":"/2021/07/26/%E4%BD%BF%E7%94%A8cloudflare%E5%8A%A0%E9%80%9FgithubPage/"},{"title":"改造eladmin为前后端一起部署","text":"前言 改造eladmin为前后端一起部署 2021-06-25 15:07:08 用了很多方法，不介绍失败的方法了，直接上成功的 首先改造前端1.更改.env.production 12VUE_APP_BASE_API = 'http://47.99.209.106:18061'VUE_APP_WS_API = 'ws://47.99.209.106:18061' 改成对应的后端接口 2.更改vue.config.js 1publicPath: process.env.NODE_ENV === 'development' ? '/' : './', 3.更改router.js 1mode: 'hash', 然后就是后端由这个地方可知 1、添加META-INF/resources/ 将前端打包好的dist放在下面 2、给SpringSecurityConfig添加图上四个 暂不确定是否都必须 注意部署后的项目web加载很慢 很奇怪 最后的入口为 后端端口/dist/index.html","link":"/2021/06/25/%E6%94%B9%E9%80%A0eladmin%E4%B8%BA%E5%89%8D%E5%90%8E%E7%AB%AF%E4%B8%80%E8%B5%B7%E9%83%A8%E7%BD%B2/"},{"title":"部署nginx二级目录","text":"前言 最近的nginx笔记 2021-06-25 13:07:08 项目为eladmin 前端 vue 主要的问题是前端更改配置 及部分nginx修改 参考链接: https://blog.csdn.net/qq_38319289/article/details/111867185 1.修改 router/router.js添加一行 1base: 'xinfuwu', 2、然后修改 vue.config.js更改一行 1publicPath: '/xinfuwu/', 3、部署时，通过NGINX的反向代理首先，给需要部署的项目定义一个 NGINX 的 server 1234567891011server { listen 4071; location / { #vue h5 history mode 时配置 try_files $uri $uri/ /xinfuwu_web/index.html; root html/xinfuwu_web; index index.html index.htm; } } 再到配置域名的主配置server上做反向代理 1234567891011121314location /xinfuwu/api/ { client_max_body_size 40M; proxy_pass http://127.0.0.1:10088/; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; } location ^~/xinfuwu/ { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:4071/; } 4.env.production也需要修改 baseapi修改为 ‘/xinfuwu/api’","link":"/2021/06/25/%E9%83%A8%E7%BD%B2nginx%E4%BA%8C%E7%BA%A7%E7%9B%AE%E5%BD%95/"},{"title":"路上的狗子","text":"​ 2021-08-03 23:40:07 有段时间每天晚上都跑步 跑到一个桥上的时候 都碰到了这只狗子 趴桥边围栏 看着下面 我当时就在想 他在等什么呢 时隔一星期 又见他 他tm在和另外一只狗子蹦蹦跳跳 喊他都不鸟我 可是狗子 所等的 是这只和他一起蹦蹦跳跳狗子吗","link":"/2021/08/03/%E8%B7%AF%E4%B8%8A%E7%9A%84%E7%8B%97%E5%AD%90/"},{"title":"晚上10点出门搞夜宵","text":"​ 2021-07-29 22:25:07 晚上10点出门搞夜宵 骑了辆共享单车 那家店没有堂食 所以有其他外卖员一起在等 我拿了我的外卖之后 在门外的共享单车上坐着玩了会手机 突然一个外卖员大叔 喊我一声 “你是是骑共享单车送外卖的吗？”","link":"/2021/07/29/%E6%99%9A%E4%B8%8A10%E7%82%B9%E5%87%BA%E9%97%A8%E6%90%9E%E5%A4%9C%E5%AE%B5/"},{"title":"鸵鸟蛋","text":"​ 2021-07-23 08:52:07 鸵鸟蛋 早餐店的阿姨问送货大叔能不能弄到鸵鸟蛋 大叔：“我那个圈子emmm好像没有诶 那个又大又厚的 你要那个干嘛” 阿姨：“买回来孵啊 变好多小鸵鸟” 然后我拿着包子走了","link":"/2021/07/23/%E9%B8%B5%E9%B8%9F%E8%9B%8B/"},{"title":"LeetCode刷题笔记","text":"前言 LeetCode刷题笔记 2021-06-01 15:15:56 LeetCode刷题笔记[7]整数反转1234567891011121314class Solution { public int reverse(int x) { int rev = 0; while (x!=0){ if (rev &lt; Integer.MIN_VALUE / 10 || rev &gt; Integer.MAX_VALUE/10){ return 0; } int digit = x % 10; x /= 10; rev = rev * 10 + digit; } return rev; }} [9]回文数1234567891011121314151617181920212223class Solution { public boolean isPalindrome(int x) { // 特殊情况： // 如上所述，当 x &lt; 0 时，x 不是回文数。 // 同样地，如果数字的最后一位是 0，为了使该数字为回文， // 则其第一位数字也应该是 0 // 只有 0 满足这一属性 if (x &lt; 0 || (x % 10 == 0 &amp;&amp; x != 0)) { return false; } int revertedNumber = 0; while (x &gt; revertedNumber) { revertedNumber = revertedNumber * 10 + x % 10; x /= 10; } // 当数字长度为奇数时，我们可以通过 revertedNumber/10 去除处于中位的数字。 // 例如，当输入为 12321 时，在 while 循环的末尾我们可以得到 x = 12，revertedNumber = 123， // 由于处于中位的数字不影响回文（它总是与自己相等），所以我们可以简单地将其去除。 return x == revertedNumber || x == revertedNumber / 10; }} [14]最长公共前缀12345678910111213141516171819202122232425class Solution { public String longestCommonPrefix(String[] strs) { if (strs == null || strs.length == 0){ return \"\"; } String prefix = strs[0]; int count = strs.length; for (int i = 1; i &lt; count; i++) { prefix = longestCommonPrefix(prefix, strs[i]); if (prefix.length() == 0){ break; } } return prefix; } public String longestCommonPrefix(String str1,String str2){ int length = Math.min(str1.length(), str2.length()); int index = 0; while (index &lt; length &amp;&amp; str1.charAt(index) == str2.charAt(index)){ index++; } return str1.substring(0, index); }} [20]有效的括号123456789101112131415161718192021222324252627class Solution { public boolean isValid(String s) { int n = s.length(); //偶数必错 if (n % 2 == 1) { return false; } Map&lt;Character, Character&gt; pairs = new HashMap&lt;Character, Character&gt;() {{ put(')', '('); put(']', '['); put('}', '{'); }}; Deque&lt;Character&gt; stack = new LinkedList&lt;Character&gt;(); for (int i = 0; i &lt; n; i++) { char ch = s.charAt(i); if (pairs.containsKey(ch)) { if (stack.isEmpty() || stack.peek() != pairs.get(ch)) { return false; } stack.pop(); } else { stack.push(ch); } } return stack.isEmpty(); }} [21]合并两个有序链表1234567891011121314151617181920212223242526class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { //使用带头结点的链表解决问题 //待输出链表的头部 ListNode head = new ListNode(); // 待输出链表的 last 结点 ListNode last = head; while (l1 != null &amp;&amp; l2 != null){ if (l1.val &gt; l2.val){ last.next = l2; l2 = l2.next; }else { last.next = l1; l1 = l1.next; } last = last.next; } // l1 或 l2 可能还有剩余结点没有合并， // 由于从上面的while循环中退出，那么链表 l1 和 l2 至少有一个已经遍历结束 if (l1 != null) last.next = l1; if (l2 != null) last.next = l2; return head.next; }} [26]删除有序数组中的重复项1234567891011121314151617class Solution { public int removeDuplicates(int[] nums) { int n = nums.length; if (n == 0){ return 0; } int fast = 1,slow = 1; while (fast &lt; n){ if (nums[fast] != nums[fast - 1]){ nums[slow] = nums[fast]; ++ slow; } ++fast; } return slow; }} [53]最大子序和123456789101112131415class Solution { public int maxSubArray(int[] nums) { int ans = nums[0]; int sum = 0; for (int num : nums) { if (sum &gt; 0) { sum += num; } else { sum = num; } ans = Math.max(ans, sum); } return ans; }} [70]爬楼梯12345678910111213// 动态规划class Solution { public int climbStairs(int n) { int p = 0, q = 0, r = 1; // 倒着推 f(x) = f(x-1) + f(x-2) for (int i = 0; i &lt; n; ++i) { p = q; q = r; r = p + q; } return r; }} [88]合并两个有序数组123456789//方法一：直接合并后排序class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { for (int i = 0; i != n; ++i) { nums1[m + i] = nums2[i]; } Arrays.sort(nums1); }} 1234567891011121314151617181920212223//方法二：双指针class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = 0, p2 = 0; int[] sorted = new int[m + n]; int cur; while (p1 &lt; m || p2 &lt; n) { if (p1 == m) { cur = nums2[p2++]; } else if (p2 == n) { cur = nums1[p1++]; } else if (nums1[p1] &lt; nums2[p2]) { cur = nums1[p1++]; } else { cur = nums2[p2++]; } sorted[p1 + p2 - 1] = cur; } for (int i = 0; i != m + n; i++) { nums1[i] = sorted[i]; } }} [101]对称二叉树12345678910111213141516// 方法一：递归class Solution { public boolean isSymmetric(TreeNode root) { return check(root, root); } public boolean check(TreeNode p, TreeNode q) { if (p == null &amp;&amp; q == null) { return true; } if (p == null || q == null) { return false; } return p.val == q.val &amp;&amp; check(p.left, q.right) &amp;&amp; check(p.right, q.left); }} 123456789101112131415161718192021222324252627// 迭代法class Solution { public boolean isSymmetric(TreeNode root) { return check(root, root); } public boolean check(TreeNode u,TreeNode v){ Queue&lt;TreeNode&gt; q = new LinkedList&lt;TreeNode&gt;(); q.offer(u); q.offer(v); while (!q.isEmpty()){ u = q.poll(); v = q.poll(); if (u == null &amp;&amp; v == null){ continue; } if (u == null || v == null || (u.val != v.val)){ return false; } q.offer(u.left); q.offer(v.right); q.offer(u.right); q.offer(v.left); } return true; }} [103]二叉树的锯齿形层序遍历1234567891011121314151617181920212223242526272829303132333435// 方法一: 层级遍历 双端队列class Solution { public List&lt;List&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; ans = new LinkedList&lt;List&lt;Integer&gt;&gt;(); if (root == null){ return ans; } Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;TreeNode&gt;(); nodeQueue.offer(root); boolean isOrderLeft = true; while (!nodeQueue.isEmpty()){ Deque&lt;Integer&gt; levelList = new LinkedList&lt;&gt;(); int size = nodeQueue.size(); for (int i = 0; i &lt; size; i++) { TreeNode curNode = nodeQueue.poll(); if (isOrderLeft){ levelList.offerLast(curNode.val); }else { levelList.offerFirst(curNode.val); } if (curNode.left != null){ nodeQueue.offer(curNode.left); } if (curNode.right != null){ nodeQueue.offer(curNode.right); } } ans.add(new LinkedList&lt;Integer&gt;(levelList)); isOrderLeft = !isOrderLeft; } return ans; }} 复杂度分析 时间复杂度：*O(N)*，其中 N 为二叉树的节点数。每个节点会且仅会被遍历一次。 空间复杂度：*O(N)*。我们需要维护存储节点的队列和存储节点值的双端队列，空间复杂度为 *O(N)*。 [104]二叉树的最大深度12345678910111213// 方法一：深度优先搜索 DFSclass Solution { public int maxDepth(TreeNode root) { if (root == null){ return 0; }else { int leftHeight = maxDepth(root.left); int rightHeight = maxDepth(root.right); //一直递归+1 return Math.max(leftHeight, rightHeight) + 1; } }} 12345678910111213141516171819202122232425262728// 方法二:BFSclass Solution { public int maxDepth(TreeNode root) { if (root == null){ return 0; } Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(root); int ans = 0; while (!queue.isEmpty()){ int size = queue.size(); while (size&gt;0){ TreeNode node = queue.poll(); if (node.left != null){ queue.offer(node.left); } //反复压栈 if (node.right != null){ queue.offer(node.right); } size --; }// //size = 0 时，最后+1 ans ++; } return ans; }} [121]买卖股票的最佳时机123456789101112131415// 方法一：暴力破解 超时class Solution { public int maxProfit(int[] prices) { int maxprofit = 0; for (int i = 0; i &lt; prices.length; i++) { for (int j = i+1; j &lt;prices.length ; j++) { int profit = prices[j] - prices[i]; if (profit &gt; maxprofit){ maxprofit = profit; } } } return maxprofit; }} 时间复杂度：*O(n^2)*。循环运行 n(n-1)/2 次。 空间复杂度：*O(1)*。只使用了常数个变量。 Integer.MAX_VALUE的含义 Integer.MAX_VALUE表示int数据类型的最大取值数：2 147 483 647Integer.MIN_VALUE表示int数据类型的最小取值数：-2 147 483 648 对应：Short.MAX_VALUE 为short类型的最大取值数 32 767Short.MIN_VALUE 为short类型的最小取值数 -32 768 Integer.MAX_VALUE+1=Integer.MIN_VALUE 12345678910111213141516// 方法二：一次遍历class Solution { public int maxProfit(int[] prices) { int minprice = Integer.MAX_VALUE; int maxprofit = 0; //一次遍历 不会出现前面-后面而出现利润错误的情况 for (int i = 0; i &lt; prices.length; i++) { if (prices[i] &lt; minprice){ minprice = prices[i]; }else if (prices[i] - minprice &gt; maxprofit){ maxprofit = prices[i] - minprice; } } return maxprofit; }} 时间复杂度：*O(n)*，只需要遍历一次。 空间复杂度：*O(1)*，只使用了常数个变量。 [141]环形链表1234567891011121314// 方法一：哈希表public class Solution { public boolean hasCycle(ListNode head) { Set&lt;ListNode&gt; seen = new HashSet&lt;ListNode&gt;(); while (head != null){ //这一步应该是表示不能添加 则返回true if (!seen.add(head)){ return true; } head = head.next; } return false; }} 时间复杂度：*O(N)*，其中 N 是链表中的节点数。最坏情况下我们需要遍历每个节点一次。 空间复杂度：*O(N)*，其中 N 是链表中的节点数。主要为哈希表的开销，最坏情况下我们需要将每个节点插入到哈希表中一次。 123456789101112131415161718192021// 方法二：快慢指针/*具体地，我们定义两个指针，一快一满。慢指针每次只移动一步，而快指针每次移动两步。初始时，慢指针在位置 head，而快指针在位置 head.next。这样一来，如果在移动的过程中，快指针反过来追上慢指针，就说明该链表为环形链表。否则快指针将到达链表尾部，该链表不为环形链表。*/public class Solution { public boolean hasCycle(ListNode head) { if (head == null || head.next == null){ return false; } ListNode slow = head; ListNode fast = head.next; while (slow!=fast){ if (fast == null || fast.next == null){ return false; } slow = slow.next; fast = fast.next.next; } return true; }} 时间复杂度：*O(N)*，其中 N 是链表中的节点数。 当链表中不存在环时，快指针将先于慢指针到达链表尾部，链表中每个节点至多被访问两次。 当链表中存在环时，每一轮移动后，快慢指针的距离将减小一。而初始距离为环的长度，因此至多移动 N 轮。 空间复杂度：*O(1)*。我们只使用了两个指针的额外空间。 [155]最小栈这道题没什么思路，基础太差的原因 123456789101112131415161718192021222324252627282930class MinStack { Deque&lt;Integer&gt; xStack; Deque&lt;Integer&gt; minStack; /** initialize your data structure here. */ public MinStack() { xStack = new LinkedList&lt;Integer&gt;(); minStack = new LinkedList&lt;Integer&gt;(); minStack.push(Integer.MAX_VALUE); } public void push(int val) { xStack.push(val); minStack.push(Math.min(minStack.peek(),val)); } public void pop() { xStack.pop(); minStack.pop(); } public int top() { return xStack.peek(); } public int getMin() { return minStack.peek(); }} [169]多数元素123456789101112131415161718192021222324// 方法一：哈希表class Solution { private Map&lt;Integer,Integer&gt; countNums(int[] nums){ Map&lt;Integer, Integer&gt; counts = new HashMap&lt;Integer, Integer&gt;(); for (int num : nums) { if (!counts.containsKey(num)){ counts.put(num, 1); }else { counts.put(num, counts.get(num) + 1); } } return counts; } public int majorityElement(int[] nums) { Map&lt;Integer, Integer&gt; counts = countNums(nums); Map.Entry&lt;Integer,Integer&gt; majorityEntry = null; for (Map.Entry&lt;Integer, Integer&gt; entry : counts.entrySet()) { if (majorityEntry == null || entry.getValue() &gt; majorityEntry.getValue()){ majorityEntry = entry; } } return majorityEntry.getKey(); }} 时间复杂度：O(n) 空间复杂度：*O(n)*。 1234567// 方法二：排序class Solution { public int majorityElement(int[] nums) { Arrays.sort(nums); return nums[nums.length/2]; }} 12345678910111213141516171819202122232425// 方法三：随机化class Solution { public int majorityElement(int[] nums) { Random rand = new Random(); int majorityCount = nums.length / 2; while (true){ int candidate = nums[randRange(rand,0,nums.length)]; if (countOccurences(nums,candidate) &gt; majorityCount){ return candidate; } } } private int randRange(Random rand,int min,int max){ return rand.nextInt(max-min) + min; } private int countOccurences(int[] nums,int num){ int count = 0; for (int i = 0; i &lt; nums.length; i++) { if (nums[i] == num){ count++; } } return count; }} 由于一个给定的下标对应的数字很有可能是众数，我们随机挑选一个下标，检查它是否是众数，如果是就返回，否则继续随机挑选。 12345678910111213141516171819202122232425262728293031323334353637383940// 方法四：分治class Solution { public int majorityElement(int[] nums) { return majorityElementRec(nums, 0, nums.length - 1); } private int countInRange(int[] nums, int num, int lo, int hi) { int count = 0; for (int i = 0; i &lt; lo; i++) { if (nums[i] == num) { count++; } } return count; } private int majorityElementRec(int[] nums, int lo, int hi) { //base case; the only element in an array of size 1 is the majority element if (lo == hi) { return nums[lo]; } //recurse on left and right halves of this slice int mid = (hi - lo) / 2 + lo; int left = majorityElementRec(nums, lo, mid); int right = majorityElementRec(nums, mid + 1, hi); //if the two halves agree on the majority element,return it if (left == right) { return left; } //otherwise,count each element and return the \"winner\" int leftCount = countInRange(nums, left, lo, hi); int rightCount = countInRange(nums, right, lo, hi); return leftCount &gt; rightCount ? left : right; }} [206]反转链表1234567891011121314// 方法一：迭代class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null){ ListNode next = curr.next; curr.next = prev; prev = curr; curr = next; } return prev; }} 复杂度分析 时间复杂度：*O(n)*，其中 n 是链表的长度。需要遍历链表一次。 空间复杂度：*O(1)*。 123456789101112// 方法二：递归class Solution { public ListNode reverseList(ListNode head) { if (head == null || head.next == null){ return head; } ListNode newHead = reverseList(head.next); head.next.next = head; head.next = null; return newHead; }} 复杂度分析 时间复杂度：*O(n)*，其中 n 是链表的长度。需要对链表的每个节点进行反转操作。 空间复杂度：*O(n)*，其中 n 是链表的长度。空间复杂度主要取决于递归调用的栈空间，最多为 n 层。 [226]翻转二叉树1234567891011121314// 方法一：递归class Solution { public TreeNode invertTree(TreeNode root) { if (root == null){ return null; } // 递归翻转 TreeNode left = invertTree(root.left); TreeNode right = invertTree(root.right); root.left = right; root.right = left; return root; }} 复杂度分析 时间复杂度：*O(N)*，其中 N 为二叉树节点的数目。我们会遍历二叉树中的每一个节点，对每个节点而言，我们在常数时间内交换其两棵子树。 空间复杂度：*O(N)*。使用的空间由递归栈的深度决定，它等于当前节点在二叉树中的高度。在平均情况下，二叉树的高度与节点个数为对数关系，即 O(logN)。而在最坏情况下，树形成链状，空间复杂度为 *O(N)*。 [234]回文链表123456789101112131415161718192021222324//方法一：将值复制到数组中后用双指针法class Solution { public boolean isPalindrome(ListNode head) { List&lt;Integer&gt; vals = new ArrayList&lt;Integer&gt;(); // 将链表的值复制到数组中 ListNode currentNode = head; while (currentNode != null){ vals.add(currentNode.val); currentNode = currentNode.next; } // int front = 0; int back = vals.size() - 1; while (front &lt; back){ if (!vals.get(front).equals(vals.get(back))){ return false; } front++; back--; } return true; }} 1234567891011121314151617181920212223//方法二：递归class Solution { private ListNode frontPointer; private boolean recursivelyCheck(ListNode currentNode){ if (currentNode!=null){ if (!recursivelyCheck(currentNode.next)){ return false; } if (currentNode.val != frontPointer.val){ return false; } frontPointer = frontPointer.next; } return true; } public boolean isPalindrome(ListNode head) { frontPointer = head; return recursivelyCheck(head); }} [283]移动零123456789101112131415161718// 方法一：双指针class Solution { public void moveZeroes(int[] nums) { int n = nums.length,left = 0,right = 0; while (right &lt; n){ if (nums[right] != 0){ swap(nums, left, right); left++; } right++; } } public void swap(int[] nums,int left,int right){ int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; }} 复杂度分析 时间复杂度：*O(n)*，其中 n 为序列长度。每个位置至多被遍历两次。 空间复杂度：*O(1)*。只需要常数的空间存放若干变量。 [448]找到所有数组中消失的数字123456789101112131415161718class Solution { public List&lt;Integer&gt; findDisappearedNumbers(int[] nums) { int n = nums.length; for (int num : nums) { // 取模 以免被增加过 int x = (num - 1) % n; nums[x] += n; } List&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; n; i++) { if (nums[i] &lt;= n) { ret.add(i + 1); } } return ret; }} [461]汉明距离123456// 内置位计数功能class Solution { public int hammingDistance(int x, int y) { return Integer.bitCount(x ^ y); }} 12345678910111213// 方法二：移位class Solution { public int hammingDistance(int x, int y) { int xor = x ^ y; int distance = 0; while (xor != 0) { if (xor % 2 == 1) distance += 1; xor = xor &gt;&gt; 1; } return distance; }} 12345678910111213//布赖恩·克尼根算法class Solution { public int hammingDistance(int x, int y) { int xor = x ^ y; int distance = 0; while (xor != 0) { distance += 1; // remove the rightmost bit of '1' xor = xor &amp; (xor - 1); } return distance; }} [543]二叉树的直径123456789101112131415161718192021class Solution { int ans; public int diameterOfBinaryTree(TreeNode root) { ans = 1; depth(root); return ans - 1; } public int depth(TreeNode node) { if (node == null) { return 0;//访问了空节点，返回0 } int L = depth(node.left);//左儿子为根的子树的深度 int R = depth(node.right);//右儿子为根的子树的深度 ans = Math.max(ans, L + R + 1);//计算d_node即L+R+1 并更新ans return Math.max(L, R) + 1;//返回该节点为根的子树的深度 }} [617]合并二叉树123456789101112131415//方法一：深度优先搜索class Solution { public TreeNode mergeTrees(TreeNode root1, TreeNode root2) { if (root1 == null){ return root2; } if (root2 == null){ return root1; } TreeNode merged = new TreeNode(root1.val+root2.val); merged.left = mergeTrees(root1.left, root2.left); merged.right = mergeTrees(root1.right, root2.right); return merged; }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//方法二：广度优先搜索class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null) { return t2; } if (t2 == null) { return t1; } TreeNode merged = new TreeNode(t1.val + t2.val); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); Queue&lt;TreeNode&gt; queue1 = new LinkedList&lt;TreeNode&gt;(); Queue&lt;TreeNode&gt; queue2 = new LinkedList&lt;TreeNode&gt;(); queue.offer(merged); queue1.offer(t1); queue2.offer(t2); while (!queue1.isEmpty() &amp;&amp; !queue2.isEmpty()) { TreeNode node = queue.poll(), node1 = queue1.poll(), node2 = queue2.poll(); TreeNode left1 = node1.left, left2 = node2.left, right1 = node1.right,right2 = node2.right; if (left1 != null || left2 != null) { if (left1 != null &amp;&amp; left2 != null) { TreeNode left = new TreeNode(left1.val + left2.val); node.left = left; queue.offer(left); queue1.offer(left1); queue2.offer(left2); } else if (left1 != null) { node.left = left1; } else if (left2 != null) { node.left = left2; } } if (right1 != null || right2 != null) { if (right1 != null &amp;&amp; right2 != null) { TreeNode right = new TreeNode(right1.val + right2.val); node.right = right; queue.offer(right); queue1.offer(right1); queue2.offer(right2); } else if (right1 != null) { node.right = right1; } else if (right2 != null) { node.right = right2; } } } return merged; }}","link":"/2021/06/01/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"},{"title":"最近的Redis笔记","text":"前言 最近的Redis笔记 ​ 2021-05-16 18:05:26 2.Redis 简介Redis 是我们在互联网应用中使用最广泛的一个 NoSQL 数据库，基于 C 开发的键值对存储数据库，Redis 这个名字是 Remote Dictionary Service 字母缩写。很多人想到 Redis，就想到缓存。但实际上 Redis 除了缓存之外，还有许多更加丰富的使用场景。比如分布式锁，限流。特点：支持数据持久化支持多种不同的数据结构类型之间的映射支持主从模式的数据备份自带了发布订阅系统定时器、计数器 3.Redis 安装四种方式获取一个 Redis： 直接编译安装（推荐使用） 使用 Docker 也可以直接安装 还有一个在线体验的方式，通过在线体验，可以直接使用 Redis 的功能http://try.redis.io/ 3.1 直接编译安装提前准备好 gcc 环境。 1yum install gcc-c++ 接下来下载并安装 Redis: 12345wget http://download.redis.io/releases/redis-5.0.7.tar.gztar -zxvf redis-5.0.7.tar.gzcd redis-5.0.7/makemake install 安装完成后，启动 Redis: redis-server redis.conf 启动成功页面如下： 3.2 通过 Docker 安装Docker 安装好之后，启动 Docker ，直接运行安装命令即可： 1docker run --name javaboy-redis -d -p 6379:6379 redis --requirepass 123 Docker 上的 Redis 启动成功之后，可以从宿主机上连接（前提是宿主机上存在 redis-cli）： 1redis-cli -a 123 如果宿主机上没有安装 Redis，那么也可以进入到 Docker 容器种去操作 Redis: docker exec -it javaboy-redis redis-cli -a 123 3.3 直接安装CentOS： 1yum install redis Ubuntu: 1apt-get install redis Mac: 1brew install redis 3.4 在线体验http://try.redis.io/ 4. Redis 五种基本数据类型4.1 Redis 启动首先，修改 redis.conf 配置文件： 配置完成后，保存退出，再次通过 redis-server redis.conf 命令启动 Redis，此时，就是在后台启动了。 4.2 StringString 是 Redis 里边最最简单的一种数据结构。在 Redis 中，所以的 key 都是字符串，但是，不同的key 对应的 value 则具备不同的数据结构，我们所说的五种不同的数据类型，主要是指 value 的数据类型不同。Redis 中的字符串是动态字符串，内部是可以修改的，像 Java 中的 StringBuffer，它采用分配冗余空间的方式来减少内存的频繁分配。在 Redis 内部结构中，一般实际分配的内存会大于需要的内存，当字符串小于 1M 的时候，扩容都是在现有的空间基础上加倍，扩容每次扩 1M 空间，最大 512M。 setset 就是给一个 key 赋值的。 append使用 append 命令时，如果 key 已经存在，则直接在对应的 value 后追加值，否则就创建新的键值对。 decr可以实现对 value 的减 1 操作（前提是 value 是一个数字），如果 value 不是数字，会报错，如果value 不存在，则会给一个默认的值为 0，在默认值的基础上减一。 decrby和 decr 类似，但是可以自己设置步长，该命令第二个参数就是步长。 getget 用来获取一个 key 的 value。 getrangegetrange 可以用来返回 key 对应的 value 的子串，这有点类似于 Java 里边的 substring。这个命令第二个和第三个参数就是截取的起始和终止位置，其中，-1 表示最后一个字符串，-2 表示倒数第二个字符串，以此类推… getset获取并更新某一个 key。 incr给某一个 key 的 value 自增。 incrby给某一个 key 的 value 自增，同时还可以设置步长。 incrbyfloat和 incrby 类似，但是自增的步长可以设置为浮点数。 mget 和 mset批量获取和批量存储 ttl查看 key 的有效期setex在给 key 设置 value 的同时，还设置过期时间。psetex和 setex 类似，只不过这里的时间单位是毫秒。setnx默认情况下， set 命令会覆盖已经存在的 key，setnx 则不会。msetnx批量设置。setrange覆盖一个已经存在的 key 的value。strlen查看字符串长度 4.2.1 BIT 命令在 Redis 中，字符串都是以二进制的方式来存储的。例如 set k1 a，a 对应的 ASCII 码是 97，97 转为二进制是 01100001，BIT 相关的命令就是对二进制进行操作的。getbitkey 对应的 value 在 offset 处的 bit 值。setbit修改 key 对应的 value 在 offset 处的 bit 值bitcount统计二进制数据中 1 的个数。 4.3 Listlpush将所有指定的值插入到存于 key 的列表的头部。如果 key 不存在，那么在进行 push 操作前会创建一个空列表。 如果 key 对应的值不是一个 list 的话，那么会返回一个错误。lrange返回列表指定区间内的元素。rpush向存于 key 的列表的尾部插入所有指定的值。rpop移除并返回列表的尾元素。lpop 移除并返回列表的头元素。lindex返回列表中，下标为 index 的元素。ltrimltrim 可以对一个列表进行修剪。blpop阻塞式的弹出，相当于 lpop 的阻塞版。 4.4 Setsadd添加元素到一个 key 中smembers获取一个 key 下的所有元素srem移除指定的元素sismemeber返回某一个成员是否在集合中scard返回集合的数量srandmember随机返回一个元素spop随机返回并且出栈一个元素。smove把一个元素从一个集合移到另一个集合中去。sdiff返回两个集合的差集。sinter返回两个集合的交集。sdiffstore这个类似于 sdiff ，不同的是，计算出来的结果会保存在一个新的集合中。sinterstore类似于 sinter，只是将计算出来的交集保存到一个新的集合中。sunion求并集。 sunionstore求并集并且将结果保存到新的集合中。 4.5 Hash在 hash 结构中，key 是一个字符串，value 则是一个 key/value 键值对。hset添加值。hget获取值hmset批量设置hmget批量获取hdel删除一个指定的 fieldhsetnx默认情况下，如果 key 和 field 相同，会覆盖掉已有的 value，hsetnx 则不会。hvals获取所有的 valuehkeys获取所有的 keyhgetall同时获取所有的 key 和 valuehexists返回 field 是否存在hincrby给指定的 value 自增hincrbyfloat可以自增一个浮点数hlen返回 某一个 key 中 value 的数量hstrlen返回某一个 key 中的某一个 field 的字符串长度 4.6 ZSetzadd将指定的元素添加到有序集合中。zscore返回 member 的 score 值zrange返回集合中的一组元素。zrevrange返回一组元素，但是是倒序。zcard返回元素个数zcount返回 score 在某一个区间内的元素。zrangebyscore按照 score 的范围返回元素。zrank返回元素的排名（从小到大zrevrank返回元素排名（从大到小zincrbyscore 自增zinterstore给两个集合求交集。zrem弹出一个元素zlexcount计算有序集合中成员数量zrangebylex返回指定区间内的成员。 4.7 keydel删除一个 key/valuedump序列化给定的 keyexists 判断一个 key 是否存在ttl查看一个 key 的有效期expire给一个 key 设置有效期，如果 key 在过期之前被重新 set 了，则过期时间会失效。persist移除一个 key 的过期时间keys *查看所有的 keypttl和 ttl 一样，只不过这里返回的是毫秒 4.8 补充 四种数据类型（list/set/zset/hash），在第一次使用时，如果容器不存在，就自动创建一个 四种数据类型（list/set/zset/hash），如果里边没有元素了，那么立即删除容器，释放内存。 5.Redis 的 Java 客户端5.1 开启远程连接Redis 默认是不支持远程连接的，需要手动开启。一共修改两个地方： 注释掉 bind: 127.0.0.1 开启密码校验，去掉 requirepass 的注释改完之后，保存退出，启动 Redis。 5.2 Jedis5.2.1 基本使用Jedis 的 GitHub 地址：https://github.com/xetorthio/jedis首先创建一个普通的 Maven 项目。项目创建成功后，添加 Jedis 依赖： 1234567&lt;dependency&gt;&lt;groupId&gt;redis.clients&lt;/groupId&gt;&lt;artifactId&gt;jedis&lt;/artifactId&gt;&lt;version&gt;3.2.0&lt;/version&gt;&lt;type&gt;jar&lt;/type&gt;&lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 然后创建一个测试方法。 123456789101112public class MyJedis {public static void main(String[] args) {//1.构造一个 Jedis 对象，因为这里使用的默认端口 6379，所以不用配置端口Jedis jedis = new Jedis(\"192.168.91.128\");//2.密码认证jedis.auth(\"javaboy\");//3.测试是否连接成功String ping = jedis.ping();//4.返回 pong 表示连接成功System.out.println(ping);}} 对于 Jedis 而言，一旦连接上 Redis 服务端，剩下的操作都很容易了。在 Jedis 中，由于方法的 API 和 Redis 的命令高度一致，所以，Jedis 中的方法见名知意，直接使用即可。 5.2.2 连接池在实际应用中，Jedis 实例我们一般都是通过连接池来获取，由于 Jedis 对象不是线城安全的，所以，当我们使用 Jedis 对象时，从连接池获取 Jedis，使用完成之后，再还给连接池。 12345678910111213public class JedisPoolTest {public static void main(String[] args) {//1. 构造一个 Jedis 连接池JedisPool pool = new JedisPool(\"192.168.91.128\", 6379);//2. 从连接池中获取一个 Jedis 连接Jedis jedis = pool.getResource();//3. Jedis 操作String ping = jedis.ping();System.out.println(ping);//4. 归还连接jedis.close();}} 如果第三步抛出异常的话，会导致第四步无法执行，所以，我们要对代码进行改进，确保第四步能够执行。 12345678910111213141516171819202122public class JedisPoolTest {public static void main(String[] args) {Jedis jedis = null;//1. 构造一个 Jedis 连接池JedisPool pool = new JedisPool(\"192.168.91.128\", 6379);//2. 从连接池中获取一个 Jedis 连接jedis = pool.getResource();jedis.auth(\"javaboy\");try {//3. Jedis 操作String ping = jedis.ping();System.out.println(ping);} catch (Exception e) {e.printStackTrace();} finally {//4. 归还连接if (jedis != null) {jedis.close();}}}} 通过 finally 我们可以确保 jedis 一定被关闭。利用 JDK1.7 中的 try-with-resource 特性，可以对上面的代码进行改造： 12345678910public class JedisPoolTest {public static void main(String[] args) {JedisPool pool = new JedisPool(\"192.168.91.128\");try(Jedis jedis = pool.getResource()) {jedis.auth(\"javaboy\");String ping = jedis.ping();System.out.println(ping);}}} 这段代码的作用和上面的是一致的。但是，上面这段代码无法实现强约束。我们可以做进一步的改进： 123456789101112131415161718192021222324252627282930313233public interface CallWithJedis {void call(Jedis jedis);}public class Redis {private JedisPool pool;public Redis() {GenericObjectPoolConfig config = new GenericObjectPoolConfig();//连接池最大空闲数config.setMaxIdle(300);//最大连接数config.setMaxTotal(1000);//连接最大等待时间，如果是 -1 表示没有限制config.setMaxWaitMillis(30000);//在空闲时检查有效性config.setTestOnBorrow(true);/*** 1. Redis 地址* 2. Redis 端口* 3. 连接超时时间* 4. 密码*/pool = new JedisPool(config, \"192.168.91.128\", 6379, 30000, \"javaboy\");}public void execute(CallWithJedis callWithJedis) {try (Jedis jedis = pool.getResource()) {callWithJedis.call(jedis);}}}Redis redis = new Redis();redis.execute(jedis -&gt; {System.out.println(jedis.ping());}); 5.3 LettuceGitHub：https://github.com/lettuce-io/lettuce-coreLettuce 和 Jedis 的一个比较： Jedis 在实现的过程中是直接连接 Redis 的，在多个线程之间共享一个 Jedis 实例，这是线城不安全的，如果想在多线程场景下使用 Jedis，就得使用连接池，这样，每个线城都有自己的 Jedis 实例。 Lettuce 基于目前很火的 Netty NIO 框架来构建，所以克服了 Jedis 中线程不安全的问题，Lettuce支持同步、异步 以及 响应式调用，多个线程可以共享一个连接实例。使用 Lettuce，首先创建一个普通的 Maven 项目，添加 Lettuce 依赖： 12345&lt;dependency&gt;&lt;groupId&gt;io.lettuce&lt;/groupId&gt;&lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&lt;version&gt;5.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后来一个简单的测试案例： 1234567891011public class LettuceTest {public static void main(String[] args) {RedisClient redisClient =RedisClient.create(\"redis://javaboy@192.168.91.128\");StatefulRedisConnection&lt;String, String&gt; connect = redisClient.connect();RedisCommands&lt;String, String&gt; sync = connect.sync();sync.set(\"name\", \"javaboy\");String name = sync.get(\"name\");System.out.println(name);}} 注意这里的密码传递方式，密码直接写在连接地址里边。 6.Redis 做分布式锁分布式锁也算是 Redis 比较常见的使用场景。问题场景：例如一个简单的用户操作，一个线城去修改用户的状态，首先从数据库中读出用户的状态，然后在内存中进行修改，修改完成后，再存回去。在单线程中，这个操作没有问题，但是在多线程中，由于读取、修改、存 这是三个操作，不是原子操作，所以在多线程中，这样会出问题。对于这种问题，我们可以使用分布式锁来限制程序的并发执行。 6.1 基本用法分布式锁实现的思路很简单，就是进来一个线城先占位，当别的线城进来操作时，发现已经有人占位了，就会放弃或者稍后再试。 在 Redis 中，占位一般使用 setnx 指令，先进来的线城先占位，线程的操作执行完成后，再调用 del 指令释放位子。根据上面的思路，我们写出的代码如下： 1234567891011121314151617public class LockTest {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis-&gt;{Long setnx = jedis.setnx(\"k1\", \"v1\");if (setnx == 1) {//没人占位jedis.set(\"name\", \"javaboy\");String name = jedis.get(\"name\");System.out.println(name);jedis.del(\"k1\");//释放资源}else{//有人占位，停止/暂缓 操作}});}} 上面的代码存在一个小小问题：如果代码业务执行的过程中抛异常或者挂了，这样会导致 del 指令没有被调用，这样，k1 无法释放，后面来的请求全部堵塞在这里，锁也永远得不到释放。要解决这个问题，我们可以给锁添加一个过期时间，确保锁在一定的时间之后，能够得到释放。改进后的代码如下： 12345678910111213141516171819public class LockTest {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis-&gt;{Long setnx = jedis.setnx(\"k1\", \"v1\");if (setnx == 1) {//给锁添加一个过期时间，防止应用在运行过程中抛出异常导致锁无法及时得到释放jedis.expire(\"k1\", 5);//没人占位jedis.set(\"name\", \"javaboy\");String name = jedis.get(\"name\");System.out.println(name);jedis.del(\"k1\");//释放资源}else{//有人占位，停止/暂缓 操作}});}} 这样改造之后，还有一个问题，就是在获取锁和设置过期时间之间如果如果服务器突然挂掉了，这个时候锁被占用，无法及时得到释放，也会造成死锁，因为获取锁和设置过期时间是两个操作，不具备原子性。为了解决这个问题，从 Redis2.8 开始，setnx 和 expire 可以通过一个命令一起来执行了，我们对上述代码再做改进： 12345678910111213141516171819public class LockTest {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis-&gt;{String set = jedis.set(\"k1\", \"v1\", new SetParams().nx().ex(5));if (set !=null &amp;&amp; \"OK\".equals(set)) {//给锁添加一个过期时间，防止应用在运行过程中抛出异常导致锁无法及时得到释放jedis.expire(\"k1\", 5);//没人占位jedis.set(\"name\", \"javaboy\");String name = jedis.get(\"name\");System.out.println(name);jedis.del(\"k1\");//释放资源}else{//有人占位，停止/暂缓 操作}});}} 6.2 解决超时问题为了防止业务代码在执行的时候抛出异常，我们给每一个锁添加了一个超时时间，超时之后，锁会被自动释放，但是这也带来了一个新的问题：如果要执行的业务非常耗时，可能会出现紊乱。举个例子：第一个线程首先获取到锁，然后开始执行业务代码，但是业务代码比较耗时，执行了 8 秒，这样，会在第一个线程的任务还未执行成功锁就会被释放了，此时第二个线程会获取到锁开始执行，在第二个线程刚执行了 3 秒，第一个线程也执行完了，此时第一个线程会释放锁，但是注意，它释放的第二个线程的锁，释放之后，第三个线程进来。对于这个问题，我们可以从两个角度入手：尽量避免在获取锁之后，执行耗时操作。可以在锁上面做文章，将锁的 value 设置为一个随机字符串，每次释放锁的时候，都去比较随机字符串是否一致，如果一致，再去释放，否则，不释放。对于第二种方案，由于释放锁的时候，要去查看锁的 value，第二个比较 value 的值是否正确，第三步释放锁，有三个步骤，很明显三个步骤不具备原子性，为了解决这个问题，我们得引入 Lua 脚本。Lua 脚本的优势：1.使用方便，Redis 中内置了对 Lua 脚本的支持。2.Lua 脚本可以在 Redis 服务端原子的执行多个 Redis 命令。 3.由于网络在很大程度上会影响到 Redis 性能，而使用 Lua 脚本可以让多个命令一次执行，可以有效解决网络给 Redis 带来的性能问题。 在 Redis 中，使用 Lua 脚本，大致上两种思路： 提前在 Redis 服务端写好 Lua 脚本，然后在 Java 客户端去调用脚本（推荐）。 可以直接在 Java 端去写 Lua 脚本，写好之后，需要执行时，每次将脚本发送到 Redis 上去执行。 首先在 Redis 服务端创建 Lua 脚本，内容如下： 12345if redis.call(\"get\",KEYS[1])==ARGV[1] thenreturn redis.call(\"del\",KEYS[1])elsereturn 0end 接下来，可以给 Lua 脚本求一个 SHA1 和，命令如下： 1cat lua/releasewherevalueequal.lua | redis-cli -a javaboy script load --pipe script load 这个命令会在 Redis 服务器中缓存 Lua 脚本，并返回脚本内容的 SHA1 校验和，然后在Java 端调用时，传入 SHA1 校验和作为参数，这样 Redis 服务端就知道执行哪个脚本了。 接下来，在 Java 端调用这个脚本。 12345678910111213141516171819202122232425public class LuaTest {public static void main(String[] args) {Redis redis = new Redis();for (int i = 0; i &lt; 2; i++) {redis.execute(jedis -&gt; {//1.先获取一个随机字符串String value = UUID.randomUUID().toString();//2.获取锁String k1 = jedis.set(\"k1\", value, new SetParams().nx().ex(5));//3.判断是否成功拿到锁if (k1 != null &amp;&amp; \"OK\".equals(k1)) {//4. 具体的业务操作jedis.set(\"site\", \"www.javaboy.org\");String site = jedis.get(\"site\");System.out.println(site);//5.释放锁jedis.evalsha(\"b8059ba43af6ffe8bed3db65bac35d452f8115d8\",Arrays.asList(\"k1\"), Arrays.asList(value));} else {System.out.println(\"没拿到锁\");}});}}} 12cat releasewherevalueequal.lua | redis-cli -a lunanboy script load --pipe-a后面是redis密码 根据实际情况填写 7.Redis 做消息队列我们平时说到消息队列，一般都是指 RabbitMQ、RocketMQ、ActiveMQ 以及大数据里边的 Kafka，这些是我们比较常见的消息中间件，也是非常专业的消息中间件，作为专业的中间件，它里边提供了许多功能。但是，当我们需要使用消息中间件的时候，并非每次都需要非常专业的消息中间件，假如我们只有一个消息队列，只有一个消费者，那就没有必要去使用上面这些专业的消息中间件，这种情况我们可以直接使用 Redis 来做消息队列。Redis 的消息队列不是特别专业，他没有很多高级特性，适用简单的场景，如果对于消息可靠性有着极高的追求，那么不适合使用 Redis 做消息队列。 7.1 消息队列Redis 做消息队列，使用它里边的 List 数据结构就可以实现，我们可以使用 lpush/rpush 操作来实现入队，然后使用 lpop/rpop 来实现出队。回顾一下：在客户端（例如 Java 端），我们会维护一个死循环来不停的从队列中读取消息，并处理，如果队列中有消息，则直接获取到，如果没有消息，就会陷入死循环，直到下一次有消息进入，这种死循环会造成大量的资源浪费，这个时候，我们可以使用之前讲的 blpop/brpop 。 7.2 延迟消息队列延迟队列可以通过 zset 来实现，因为 zset 中有一个 score，我们可以把时间作为 score，将 value 存到redis 中，然后通过轮询的方式，去不断的读取消息出来。首先，如果消息是一个字符串，直接发送即可，如果是一个对象，则需要对对象进行序列化，这里我们使用 JSON 来实现序列化和反序列化。所以，首先在项目中，添加 JSON 依赖： 12345&lt;dependency&gt;&lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;&lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;version&gt;2.10.3&lt;/version&gt;&lt;/dependency&gt; 接下来，构造一个消息对象： 1234567891011121314151617181920212223public class JavaboyMessage {private String id;private Object data;@Overridepublic String toString() {return \"JavaboyMessage{\" +\"id='\" + id + '\\'' +\", data=\" + data +'}';}public String getId() {return id;}public void setId(String id) {this.id = id;}public Object getData() {return data;}public void setData(Object data) {this.data = data;}} 接下来封装一个消息队列： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class DelayMsgQueue {private Jedis jedis;private String queue;public DelayMsgQueue(Jedis jedis, String queue) {this.jedis = jedis;this.queue = queue;}/*** 消息入队** @param data 要发送的消息*/public void queue(Object data) {//构造一个 JavaboyMessageJavaboyMessage msg = new JavaboyMessage();msg.setId(UUID.randomUUID().toString());msg.setData(data);//序列化try {String s = new ObjectMapper().writeValueAsString(msg);System.out.println(\"msg publish:\" + new Date());//消息发送，score 延迟 5 秒jedis.zadd(queue, System.currentTimeMillis() + 5000, s);} catch (JsonProcessingException e) {e.printStackTrace();}}/*** 消息消费*/public void loop() {while (!Thread.interrupted()) {//读取 score 在 0 到当前时间戳之间的消息Set&lt;String&gt; zrange = jedis.zrangeByScore(queue, 0,System.currentTimeMillis(), 0, 1);if (zrange.isEmpty()) {//如果消息是空的，则休息 500 毫秒然后继续try {Thread.sleep(500);} catch (InterruptedException e) {break;}continue;}//如果读取到了消息，则直接读取消息出来String next = zrange.iterator().next();if (jedis.zrem(queue, next) &gt; 0) {//抢到了，接下来处理业务try {JavaboyMessage msg = new ObjectMapper().readValue(next,JavaboyMessage.class);System.out.println(\"receive msg:\" + msg);} catch (JsonProcessingException e) {e.printStackTrace();}}}}} 测试： 1234567891011121314151617181920212223242526272829303132333435public class DelayMsgTest {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis -&gt; {//构造一个消息队列DelayMsgQueue queue = new DelayMsgQueue(jedis, \"javaboy-delayqueue\");//构造消息生产者Thread producer = new Thread(){@Overridepublic void run() {for (int i = 0; i &lt; 5; i++) {queue.queue(\"www.javaboy.org&gt;&gt;&gt;&gt;\" + i);}}};//构造一个消息消费者Thread consumer = new Thread(){@Overridepublic void run() {queue.loop();}};//启动producer.start();consumer.start();//休息 7 秒后，停止程序try {Thread.sleep(7000);consumer.interrupt();} catch (InterruptedException e) {e.printStackTrace();}});}} 8.再谈 Bit 操作8.1 基本介绍用户一年的签到记录，如果你用 string 类型来存储，那你需要 365 个 key/value，操作起来麻烦。通过位图可以有效的简化这个操作。它的统计很简单：01111000111每天的记录占一个位，365 天就是 365 个位，大概 46 个字节，这样可以有效的节省存储空间，如果有一天想要统计用户一共签到了多少天，统计 1 的个数即可。对于位图的操作，可以直接操作对应的字符串（get/set），可以直接操作位（getbit/setbit）. 8.2 基本操作Redis 的基本操作可以归为两大类： 8.2.1 零存整取例如存储一个 Java 字符串： 字符 ASCII 二进制 J 74 01001010 a 97 01100001 v 118 01110110 接下来去存储： 8.2.1 整存零取存一个字符串进去，但是通过位操作获取字符串。 8.3 统计例如签到记录：011110001111 表示签到的天，0 表示没签到，统计总的签到天数：可以使用 bitcount。 bitcount 中，可以统计的起始位置，但是注意，这个起始位置是指字符的起始位置而不是 bit 的起始位置。除了 bitcount 之外，还有一个 bitpos。bitpos 可以用来统计在指定范围内出现的第一个 1 或者 0 的位置，这个命令中的起始和结束位置都是字符索引，不是 bit 索引，一定要注意。 8.4 Bit 批处理在 Redis 3.2 之后，新加了一个功能叫做 bitfiled ，可以对 bit 进行批量操作。例如：BITFIELD name get u4 0表示获取 name 中的位，从 0 开始获取，获取 4 个位，返回一个无符号数字。 u 表示无符号数字 i 表示有符号数字，有符号的话，第一个符号就表示符号位，1 表示是一个负数。 bitfiled 也可以一次执行多个操作。 GET： SET： 用无符号的 98 转成的 8 位二进制数字，代替从第 8 位开始接下来的 8 位数字。 INCRBY： 对置顶范围进行自增操作，自增操作可能会出现溢出，既可能是向上溢出，也可能是向下溢出。Redis中对于溢出的处理方案是折返。8 位无符号数 255 加 1 溢出变为 0；8 位有符号数 127，加 1 变为 -128. 也可以修改默认的溢出策略，可以改为 fail ，表示执行失败。 1BITFIELD name overflow fail incrby u2 6 1 sat 表示留在在最大/最小值。 1BITFIELD name overflow sat incrby u2 6 1 9.HyperLogLog一般我们评估一个网站的访问量，有几个主要的参数： pv，Page View，网页的浏览量 uv，User View，访问的用户 一般来说，pv 或者 uv 的统计，可以自己来做，也可以借助一些第三方的工具，比如 cnzz，友盟 等。如果自己实现，pv 比较简单，可以直接通过 Redis 计数器就能实现。但是 uv 就不一样，uv 涉及到另外一个问题，去重。我们首先需要在前端给每一个用户生成一个唯一 id，无论是登录用户还是未登录用户，都要有一个唯一id，这个 id 伴随着请求一起到达后端，在后端我们通过 set 集合中的 sadd 命令来存储这个 id，最后通过 scard 统计集合大小，进而得出 uv 数据。如果是千万级别的 UV，需要的存储空间就非常惊人。而且，像 UV 统计这种，一般也不需要特别精确，800w 的 uv 和 803w 的 uv，其实差别不大。所以，我们要介绍今天的主角—HyperLogLogRedis 中提供的 HyperLogLog 就是专门用来解决这个问题的，HyperLogLog 提供了一套不怎么精确但是够用的去重方案，会有误差，官方给出的误差数据是 0.81%，这个精确度，统计 UV 够用了。HyperLogLog 主要提供了两个命令：pfadd 和 pfcount。pfadd 用来添加记录，类似于 sadd ，添加过程中，重复的记录会自动去重。pfcount 则用来统计数据。 数据量少的时候看不出来误差。在 Java 中，我们多添加几个元素： 123456789101112public class HyperLogLog {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis -&gt; {for (int i = 0; i &lt; 1000; i++) {jedis.pfadd(\"uv\", \"u\" + i, \"u\" + (i + 1));}long uv = jedis.pfcount(\"uv\");System.out.println(uv);//理论值是 1001});}} 理论值是 1001，实际打印出来 994，有误差，但是在可以接受的范围内。除了 pfadd 和 pfcount 之外，还有一个命令 pfmerge ，合并多个统计结果，在合并的过程中，会自动去重多个集合中重复的元素。 10.布隆过滤器10.1 场景重现我们用 HyperLogLog 来估计一个数，有偏差但是也够用。HyperLogLog 主要提供两个方法：pfaddpfcount但是 HyperLogLog 没有判断是否包含的方法，例如 pfexists 、pfcontains 等。没有这样的方法存在，但是我们有这样的业务需求。例如刷今日头条，推送的内容有相似的，但是没有重复的。这就涉及到如何在推送的时候去重？解决方案很多，例如将用户的浏览历史记录下来，然后每次推送时去比较该条消息是否已经给用户推送了。但是这种方式效率极低，不推荐。解决这个问题，就要靠我们今天要说的布隆过滤器。 10.2 Bloom Filter 介绍Bloom Filter 专门用来解决我们上面所说的去重问题的，使用 Bloom Filter 不会像使用缓存那么浪费空间。当然，他也存在一个小小问题，就是不太精确。Bloom Filter 相当于是一个不太精确的 set 集合，我们可以利用它里边的 contains 方法去判断某一个对象是否存在，但是需要注意，这个判断不是特别精确。一般来说，通过 contains 判断某个值不存在，那就一定不存在，但是判断某个值存在的话，则他可能不存在。 以今日头条为例，假设我们将用户的浏览记录用 B 表示，A 表示用户没有浏览的新闻，现在要给用户推送消息，先去 B 里边判断这条消息是否已经推送过，如果判断结果说没推送过（B 里边没有这条记录），那就一定没有推送过。如果判断结果说有推送过（B 里边也有可能没有这条消息），这个时候该条消息就不会推送给用户，导致用户错过该条消息，当然这是概率极低的。 10.3 Bloom Filter 原理每一个布隆过滤器，在 Redis 中都对应了一个大型的位数组以及几个不同的 hash 函数。 所谓的 add 操作是这样的：首先根据几个不同的 hash 函数给元素进行 hash 运算一个整数索引值，拿到这个索引值之后，对位数组的长度进行取模运算，得到一个位置，每一个 hash 函数都会得到一个位置，将位数组中对应的位置设置位 1 ，这样就完成了添加操作。 当判断元素是否粗存在时，依然先对元素进行 hash 运算，将运算的结果和位数组取模，然后去对应的位置查看是否有相应的数据，如果有，表示元素可能存在（因为这个有数据的地方也可能是其他元素存进来的），如果没有表示元素一定不存在。Bloom Filter 中，误判的概率和位数组的大小有很大关系，位数组越大，误判概率越小，当然占用的存储空间越大；位数组越小，误判概率越大，当然占用的存储空间就小。 10.4 Bloom Filter 安装https://oss.redislabs.com/redisbloom/Quick_Start/ 这里给大家介绍两种安装方式： Docker： 1docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest 自己编译安装： 123456cd redis-5.0.7git clone https://github.com/RedisBloom/RedisBloom.gitcd RedisBloom/makecd ..redis-server redis.conf --loadmodule ./RedisBloom/redisbloom.so 安装完成后，执行 bf.add 命令，测试安装是否成功。每次启动时都输入 redis-server redis.conf --loadmodule ./RedisBloom/redisbloom.so 比较麻烦，我们可以将要加载的模块在 redis.conf 中提前配置好。 1234567################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.soloadmodule /root/redis-5.0.7/RedisBloom/redisbloom.so 最下面这一句，配置完成后，以后只需要 redis-server redis.conf 来启动 Redis 即可。 10.5 基本用法主要是两类命令，添加和判断是否存在。bf.add\\bf.madd 添加和批量添加bf.exists\\bf.mexists 判断是否存在和批量判断 使用 Jedis 操作布隆过滤器，首先添加依赖： 12345&lt;dependency&gt;&lt;groupId&gt;com.redislabs&lt;/groupId&gt;&lt;artifactId&gt;jrebloom&lt;/artifactId&gt;&lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 然后进行测试： 12345678910111213141516171819public class BloomFilter {public static void main(String[] args) {GenericObjectPoolConfig config = new GenericObjectPoolConfig();config.setMaxIdle(300);config.setMaxTotal(1000);config.setMaxWaitMillis(30000);config.setTestOnBorrow(true);JedisPool pool = new JedisPool(config, \"192.168.91.128\", 6379, 30000,\"javaboy\");Client client = new Client(pool);//存入数据for (int i = 0; i &lt; 100000; i++) {client.add(\"name\", \"javaboy-\" + i);}//检查数据是否存在boolean exists = client.exists(\"name\", \"javaboy-9999999\");System.out.println(exists);}} 默认情况下，我们使用的布隆过滤器它的错误率是 0.01 ，默认的元素大小是 100。但是这两个参数也是可以配置的。我们可以调用 bf.reserve 方法进行配置。 1BF.RESERVE k1 0.0001 1000000 第一个参数是 key，第二个参数是错误率，错误率越低，占用的空间越大，第三个参数预计存储的数量，当实际数量超出预计数量时，错误率会上升。 10.6 典型场景前面所说的新闻推送过滤算是一个应用场景。解决 Redis 穿透或者又叫缓存击穿问题。假设我有 1亿 条用户数据，现在查询用户要去数据库中查，效率低而且数据库压力大，所以我们会把请求首先在 Redis 中处理（活跃用户存在 Redis 中），Redis 中没有的用户，再去数据库中查询。现在可能会存在一种恶意请求，这个请求携带上了很多不存在的用户，这个时候 Redis 无法拦截下来请求，所以请求会直接跑到数据库里去。这个时候，这些恶意请求会击穿我们的缓存，甚至数据库，进而引起“雪崩效应”。为了解决这个问题，我们就可以使用布隆过滤器。将 1亿条用户数据存在 Redis 中不现实，但是可以存在布隆过滤器中，请求来了，首先去判断数据是否存在，如果存在，再去数据库中查询，否则就不去数据库中查询。 11.Redis 限流11.1 预备知识Pipeline（管道）本质上是由客户端提供的一种操作。Pipeline 通过调整指令列表的读写顺序，可以大幅度的节省 IO 时间，提高效率。 11.2 简单限流1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class RateLimiter {private Jedis jedis;public RateLimiter(Jedis jedis) {this.jedis = jedis;}/*** 限流方法* @param user 操作的用户，相当于是限流的对象* @param action 具体的操作* @param period 时间窗，限流的周期* @param maxCount 限流的次数* @return*/public boolean isAllowed(String user, String action, int period, intmaxCount) {//1.数据用 zset 保存，首先生成一个 keyString key = user + \"-\" + action;//2.获取当前时间戳long nowTime = System.currentTimeMillis();//3.建立管道Pipeline pipelined = jedis.pipelined();pipelined.multi();//4.将当前的操作先存储下来pipelined.zadd(key, nowTime, String.valueOf(nowTime));//5.移除时间窗之外的数据pipelined.zremrangeByScore(key, 0, nowTime - period * 1000);//6.统计剩下的 keyResponse&lt;Long&gt; response = pipelined.zcard(key);//7.将当前 key 设置一个过期时间，过期时间就是时间窗pipelined.expire(key, period + 1);//关闭管道pipelined.exec();pipelined.close();//8.比较时间窗内的操作数return response.get() &lt;= maxCount;}public static void main(String[] args) {Redis redis = new Redis();redis.execute(j -&gt; {RateLimiter rateLimiter = new RateLimiter(j);for (int i = 0; i &lt; 20; i++) {System.out.println(rateLimiter.isAllowed(\"javaboy\", \"publish\",5, 3));}});}} 11.3 深入限流操作Redis4.0 开始提供了一个 Redis-Cell 模块，这个模块使用漏斗算法，提供了一个非常好用的限流指令。漏斗算法就像名字一样，是一个漏斗，请求从漏斗的大口进，然后从小口出进入到系统中，这样，无论是多大的访问量，最终进入到系统中的请求，都是固定的。使用漏斗算法，需要我们首先安装 Redis-Cell 模块：https://github.com/brandur/redis-cell安装步骤： 123456wget https://github.com/brandur/redis-cell/releases/download/v0.2.4/redis-cellv0.2.4-x86_64-unknown-linux-gnu.tar.gztar -zxvf redis-cell-v0.2.4-x86_64-unknown-linux-gnu.tar.gzmkdir redis-cellmv libredis_cell.d ./redis-cellmv libredis_cell.so ./redis-cell 接下来修改 redis.conf 文件，加载额外的模块： 1loadmodule /root/redis-5.0.7/redis-cell/libredis_cell.so 然后，启动 Redis： 1redis-server redis.conf Redis 中加入 Redis-Cell 模块，配置文件出错 version GLIBC_2.18 not found添加 Redis-Cell 模块到 redis.conf 配置文件中，启动就会报下面的错误。 最后配置了 redis.log 的日志发现是缺少 CLIBC_2.18 这个文件，经过一系列下载就解决了这个问题。 参考链接 ： https://blog.csdn.net/qq_43647359/article/details/105864565 下载 glibc 文件 1234567891011121314151617// 下载 glibc 压缩包wget http://ftp.gnu.org/gnu/glibc/glibc-2.18.tar.gz // 解压 glibc 压缩包tar -zxvf glibc-2.18.tar.gz// 进入解压后的目录cd glibc-2.18// 创建编译目录mkdir build// 进入到创建好的目录cd build/// 编译、安装../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin// 这一步比较慢make -j 8make install redis 启动成功后，如果存在 CL.THROTTLE 命令，说明 redis-cell 已经安装成功了。 CL.THROTTLE 命令一共有五个参数 第一个参数是 key 第二个参数是漏斗的容量 时间窗内可以操作的次数 时间窗 每次漏出数量 执行完成后，返回值也有五个： 第一个 0 表示允许，1表示拒绝 第二个参数是漏斗的容量 第三个参数是漏斗的剩余空间 如果拒绝了，多长时间后，可以再试 多长时间后，漏斗会完全空出来 11.4 Lettuce扩展1cl.throttle &lt;key&gt; &lt;max_burst&gt; &lt;count per period&gt; &lt;period&gt; [&lt;quantity&gt;] 首先定义一个命令接口： 12345public interface RedisCommandInterface extends Commands {@Command(\"CL.THROTTLE ?0 ?1 ?2 ?3 ?4\")List&lt;Object&gt; throttle(String key, Long init, Long count, Long period, Longquota);} 定义完成后，接下来，直接调用即可： 12345678910111213public class ThrottleTest {public static void main(String[] args) {RedisClient redisClient =RedisClient.create(\"redis://javaboy@192.168.91.128\");StatefulRedisConnection&lt;String, String&gt; connect = redisClient.connect();RedisCommandFactory factory = new RedisCommandFactory(connect);RedisCommandInterface commands =factory.getCommands(RedisCommandInterface.class);List&lt;Object&gt; list = commands.throttle(\"javaboy-publish\", 10L, 10L, 60L,1L);System.out.println(list);}} 12.Redis 之 GeoRedis3.2 开始提供了 GEO 模块。该模块也使用了 GeoHash 算法。 1.GeoHash核心思想：GeoHash 是一种地址编码方法，使用这种方式，能够将二维的空间经纬度数据编码成一个一维字符串。地球上经纬度的划分： 以经过伦敦格林尼治天文台旧址的经线为 0 度经线，向东就是东经，向西就是西经。如果我们将西经定义负，经度的范围就是 [-180,180]。纬度北纬 90 度到南纬 90 度，如果我们将南纬定义负，则纬度的范围就是 [-90,90]。接下来，以本初子午线和赤道为界，我们可以将地球上的点分配到一个二维坐标中： GeoHash 算法就是基于这样的思想，划分的次数越多，区域越多，每个区域中的面积就更小了，精确度就会提高。GeoHash 具体算法：以北京天安门广场为例（39.9053908600,116.3980007200）： 纬度的范围在 (-90,90) 之间，中间值为 0，对于 39.9053908600 值落在 (0,90),因此得到的值为 1 (0,90) 的中间值为 45，39.9053908600 落在 (0,45) 之间，因此得到一个 0 (0,45) 的中间值为 22.5，39.9053908600 落在 (22.5,45)之间，因此得到一个 1 …. 这样，我们得到的纬度二进制是 101按照同样的步骤，我们可以算出来经度的二进制是 110接下来将经纬度合并（经度占偶数位，纬度占奇数位）：111001按照 Base32 （0-9,b-z,去掉 a i l 0）对合并后的二进制数据进行编码，编码的时候，先将二进制转换为十进制，然后进行编码。 将编码得到的字符串，可以拿去 geohash.org 网站上解析。GeoHash 有哪些特点： 用一个字符串表示经纬度 GeoHash 表示的是一个区域，而不是一个点。 编码格式有规律，例如一个地址编码之后的格式是 123，另一个地址编码之后的格式是 123456，从字符串上就可以看出来，123456 处于 123 之中。 Redis 中使用添加地址： 12GEOADD city 116.3980007200 39.9053908600 beijingGEOADD city 114.0592002900 22.5536230800 shenzhen 查看两个地址之间的距离： 12127.0.0.1:6379&gt; GEODIST city beijing shenzhen km\"1942.5435\" 获取元素的位置： 123127.0.0.1:6379&gt; GEOPOS city beijing1) 1) \"116.39800339937210083\" 2) \"39.90539144357683909\" 获取元素 hash 值： 12127.0.0.1:6379&gt; GEOHASH city beijing1) \"wx4g08w3y00\" 通过 hash 值可以查看定位。http://geohash.org/wx4g08w3y00查看附近的人： 12127.0.0.1:6379&gt; GEORADIUSBYMEMBER city beijing 200 km count 3 asc1) \"beijing\" 以北京为中心，方圆 200km 以内的城市找出来 3 个，按照远近顺序排列，这个命令不会排除 北京。当然，也可以根据经纬度来查询（将 member 换成对应的经纬度）： 12127.0.0.1:6379&gt; GEORADIUS city 116.3980007200 39.9053908600 2000 km withdistwithhash withcoord count 4 desc http://www.gpsspg.com/maps.htm 13.Redis 之 Scan1.简单介绍scan 实际上是 keys 的一个升级版。可以用 keys 来查询 key，在查询的过程中，可以使用通配符。keys 虽然用着还算方便，但是没有分页功能。同时因为 Redis 是单线程，所以 key 的执行会比较消耗时间，特别是当数据量大的时候，影响整个程序的运行。为了解决 keys 存在的问题，从 Redis2.8 中开始，引入了 scan。scan 具备 keys 的功能，但是不会阻塞线程，而且可以控制每次返回的结果数。 2.基本用法首先准备 10000 条测试数据： 12345678910public class ScanTest {public static void main(String[] args) {Redis redis = new Redis();redis.execute(jedis -&gt; {for (int i = 0; i &lt; 10000; i++) {jedis.set(\"k\" + i, \"v\" + i);}});}} scan 命令一共提供了三个参数，第一个 cursor，第二个参数是 key，第三个参数是 limit。cursor 实际上是指一维数组的位置索引，limit 则是遍历的一维数组个数（所以每次返回的数据大小可能不确定）。 1scan 0 match k8* count 1000 3.原理SCAN 的遍历顺序。假设目有三条数据： 1234567891011121314151617127.0.0.1:6379&gt; keys *1) \"key1\"2) \"db_number\"3) \"myKey\"127.0.0.1:6379&gt; scan 0 match * count 11) \"2\"2) 1) \"key1\"127.0.0.1:6379&gt; scan 2 match * count 11) \"1\"2) 1) \"myKey\"127.0.0.1:6379&gt; scan 1 match * count 11) \"3\"2) 1) \"db_number\"127.0.0.1:6379&gt; scan 3 match * count 11) \"0\"2) (empty list or set)127.0.0.1:6379&gt; 在遍历的过程中，大家发现游标的顺序是 0 2 1 3，从十进制来看好像没有规律，但是从转为二进制，则是有规律的： 00-&gt;10-&gt;01-&gt;11 这种规律就是高位进1，传统的二进制加法，是从右往左加，这里是从左往右加。实际上，在 Redis 中，它的具体计算流程给是这样： 将要计算的数字反转 给反转后的数字加 1 再反转 那么为什么不是按照 0、1、2、3、4…这样的顺序遍历呢？因为主要考虑到两个问题： 字典扩容 字典缩容 假如我们将要访问 110 时，发生了扩容，此时 scan 就会从 0110 开始遍历，之前已经被遍历过的元素就不会被重复遍历了。假如我们将要访问 110 时，发生缩容，此时 scan 就会从 10 开始遍历，这个时候，也会遍历到 010，但是 010 之前的不会再被遍历了。所以，在发生缩容的时候，可能返回重复的元素。 4.其他用法scan 是一系列的指令，除了遍历所有的 key 之外，也可以遍历某一个类型的 key，对应的命令有：zscan–&gt;zsethscan–&gt;hashsscan–&gt;set 14.Redis 单线程如何处理高并发1.阻塞 IO 与非阻塞 IOJava 在 JDK1.4 中引入 NIO，但是也有很多人在使用阻塞 IO，这两种 IO 有什么区别？在阻塞模式下，如果你从数据流中读取不到指定大小的数据两，IO 就会阻塞。比如已知会有 10 个字节发送过来，但是我目前只收到 4 个，还剩六个，此时就会发生阻塞。如果是非阻塞模式，虽然此时只收到 4 个字节，但是读到 4 个字节就会立即返回，不会傻傻等着，等另外 6 个字节来的时候，再去继续读取。所以阻塞 IO 性能低于 非阻塞 IO。如果有一个 Web 服务器，使用阻塞 IO 来处理请求，那么每一个请求都需要开启一个新的线程；但是如果使用了非阻塞 IO，基本上一个小小线程池就够用了，因为不会发生阻塞，每一个线程都能够高效利用。 Java 在 JDK1.4 中引入 NIO，但是也有很多人在使用阻塞 IO，这两种 IO 有什么区别？在阻塞模式下，如果你从数据流中读取不到指定大小的数据两，IO 就会阻塞。比如已知会有 10 个字节发送过来，但是我目前只收到 4 个，还剩六个，此时就会发生阻塞。如果是非阻塞模式，虽然此时只收到 4 个字节，但是读到 4 个字节就会立即返回，不会傻傻等着，等另外 6 个字节来的时候，再去继续读取。所以阻塞 IO 性能低于 非阻塞 IO。如果有一个 Web 服务器，使用阻塞 IO 来处理请求，那么每一个请求都需要开启一个新的线程；但是如果使用了非阻塞 IO，基本上一个小小线程池就够用了，因为不会发生阻塞，每一个线程都能够高效利用。 2.Redis 的线程模型首先一点，Redis 是单线程。单线程如何解决高并发问题的？实际上，能够处理高并发的单线程应用不仅仅是 Redis，除了 Redis 之外，还有 NodeJS、Nginx 等等也是单线程。 Redis 虽然是单线程，但是运行很快，主要有如下几方面原因： Redis 中的所有数据都是基于内存的，所有的计算也都是内存级别的计算，所以快。 Redis 是单线程的，所以有一些时间复杂度高的指令，可能会导致 Redis 卡顿，例如 keys。 Redis 在处理并发的客户端连接时，使用了非阻塞 IO。 在使用非阻塞 IO 时，有一个问题，就是线程如何知道剩下的数据来了？这里就涉及到一个新的概念叫做多路复用，本质上就是一个事件轮询 API。 Redis 会给每一个客户端指令通过队列来排队进行顺序处理。 Redis 做出响应时，也会有一个响应的队列。 15.Redis 通信协议Redis 通信使用了文本协议，文本协议比较费流量，但是 Redis 作者认为数据库的瓶颈不在于网络流量，而在于内部逻辑，所以采用了这样一个费流量的文本协议。这个文本协议叫做 Redis Serialization Protocol，简称 RESP。Redis 协议将传输的数据结构分为 5 种最小单元，单元结束时，加上回车换行符 \\r\\n。 单行字符串以 + 开始，例如 +javaboy.org\\r\\n 多行字符串以 $ 开始，后面加上字符串长度，例如 $11\\r\\njavaboy.org\\r\\n 整数值以: 开始，例如 :1024\\r\\n 错误消息以 - 开始 数组以 * 开始，后面加上数组长度。需要注意的是，如果是客户端连接服务端，只能使用第 5 种。 1.准备工作做两件事情：为了方便客户端连接 Redis，我们关闭 Redis 种的保护模式(在 redis.conf 文件中)protected no同时关闭密码 1# requirepass xxxx 配置完成后，重启 Redis。 2. 实战接下来，我们通过 Socket+RESP 来定义两个最最常见的命令 set 和 get。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class JavaboyRedisClient {private Socket socket;public JavaboyRedisClient() {try {socket = new Socket(\"192.168.91.128\", 6379);} catch (IOException e) {e.printStackTrace();System.out.println(\"Redis 连接失败\");}}/*** 执行 Redis 中的 set 命令 [set,key,value]* @param key* @param value* @return*/public String set(String key, String value) throws IOException {StringBuilder sb = new StringBuilder();sb.append(\"*3\").append(\"\\r\\n\").append(\"$\").append(\"set\".length()).append(\"\\r\\n\").append(\"set\").append(\"\\r\\n\").append(\"$\").append(key.getBytes().length).append(\"\\r\\n\").append(key).append(\"\\r\\n\").append(\"$\").append(value.getBytes().length).append(\"\\r\\n\").append(value).append(\"\\r\\n\");System.out.println(sb.toString());socket.getOutputStream().write(sb.toString().getBytes());byte[] buf = new byte[1024];socket.getInputStream().read(buf);return new String(buf);}/*** 执行 Redis 中的 get 命令 [get,key]* @param key* @return*/public String get(String key) throws IOException {StringBuilder sb = new StringBuilder();sb.append(\"*2\").append(\"\\r\\n\").append(\"$\").append(\"get\".length()).append(\"\\r\\n\").append(\"get\").append(\"\\r\\n\").append(\"$\").append(key.getBytes().length).append(\"\\r\\n\").append(key).append(\"\\r\\n\");socket.getOutputStream().write(sb.toString().getBytes());byte[] buf = new byte[1024];socket.getInputStream().read(buf);return new String(buf);}public static void main(String[] args) throws IOException {String set = new JavaboyRedisClient().set(\"k1\", \"江南一点雨\");System.out.println(set);String k1 = new JavaboyRedisClient().get(\"k1\");System.out.println(k1);}} 16.Redis 持久化Redis 是一个缓存工具，也叫做 NoSQL 数据库，既然是数据库，必然支持数据的持久化操作。在 Redis中，数据库持久化一共有两种方案： 快照方式 AOF 日志 1.快照1.1 原理Redis 使用操作系统的多进程机制来实现快照持久化：Redis 在持久化时，会调用 glibc 函数 fork 一个子进程，然后将快照持久化操作完全交给子进程去处理，而父进程则继续处理客户端请求。在这个过程中，子进程能够看到的内存中的数据在子进程产生的一瞬间就固定下来了，再也不会改变，也就是为什么 Redis 持久化叫做 快照。 1.2 具体配置在 Redis 中，默认情况下，快照持久化的方式就是开启的。默认情况下会产生一个 dump.rdb 文件，这个文件就是备份下来的文件。当 Redis 启动时，会自动的去加载这个 rdb 文件，从该文件中恢复数据。具体的配置，在 redis.conf 中： 123456789101112# 表示快照的频率，第一个表示 900 秒内如果有一个键被修改，则进行快照save 900 1save 300 10save 60 10000# 快照执行出错后，是否继续处理客户端的写命令stop-writes-on-bgsave-error yes# 是否对快照文件进行压缩rdbcompression yes# 表示生成的快照文件名dbfilename dump.rdb# 表示生成的快照文件位置dir ./ 1.3 备份流程 在 Redis 运行过程中，我们可以向 Redis 发送一条 save 命令来创建一个快照。但是需要注意，save 是一个阻塞命令，Redis 在收到 save 命令开始处理备份操作之后，在处理完成之前，将不再处理其他的请求。其他命令会被挂起，所以 save 使用的并不多。 我们一般可以使用 bgsave，bgsave 会 fork 一个子进程去处理备份的事情，不影响父进程处理客户端请求。 我们定义的备份规则，如果有规则满足，也会自动触发 bgsave。 另外，当我们执行 shutdown 命令时，也会触发 save 命令，备份工作完成后，Redis 才会关闭。 用 Redis 搭建主从复制时，在 从机连上主机之后，会自动发送一条 sync 同步命令，主机收到命令之后，首先执行 bgsave 对数据进行快照，然后才会给从机发送快照数据进行同步。 2.AOF与快照持久化不同，AOF 持久化是将被执行的命令追加到 aof 文件末尾，在恢复时，只需要把记录下来的命令从头到尾执行一遍即可。默认情况下，AOF 是没有开启的。我们需要手动开启： 123456789101112# 开启 aof 配置appendonly yes# AOF 文件名appendfilename \"appendonly.aof\"# 备份的时机，下面的配置表示每秒钟备份一次appendfsync everysec# 表示 aof 文件在压缩时，是否还继续进行同步操作no-appendfsync-on-rewrite no# 表示当目前 aof 文件大小超过上一次重写时的 aof 文件大小的百分之多少的时候，再次进行重写auto-aof-rewrite-percentage 100# 如果之前没有重写过，则以启动时的 aof 大小为依据，同时要求 aof 文件至少要大于 64Mauto-aof-rewrite-min-size 64mb 同时为了避免快照备份的影响，记得将快照备份关闭： 1234save \"\"#save 900 1#save 300 10#save 60 10000 17.Redis 事务正常来说，一个可以商用的数据库往往都有比较完善的事务支持，Redis 当然也不例外。相对于 关系型数据库中的事务模型，Redis 中的事务要简单很多。因为简单，所以 Redis 中的事务模型不太严格，所以我们不能像使用关系型数据库中的事务那样来使用 Redis。 在关系型数据库中，和事务相关的三个指令分别是：begincommitrollback 在 Redis 中，当然也有对应的指令：multiexecdiscard 1.原子性注意，Redis 中的事务并不能算作原子性。它仅仅具备隔离性，也就是说当前的事务可以不被其他事务打断。由于每一次事务操作涉及到的指令还是比较多的，为了提高执行效率，我们在使用客户端的时候，可以通过 pipeline 来优化指令的执行。Redis 中还有一个 watch 指令，watch 可以用来监控一个 key，通过这种监控，我们可以确保在 exec之前，watch 的键的没有被修改过。 2.Java代码实现","link":"/2021/05/16/redis%E7%AC%94%E8%AE%B0/"},{"title":"linux韩顺平2021笔记","text":"前言 最近的linux学习笔记 linux韩顺平2021p1 课程内容基础篇Linux入门 vm和linux的安装 Linux目录结构 实际操作篇远程登陆（Xshell XFtp） 实用指令 进程管理 用户管理 Vi和Vim管理 定时任务调度 RPM和YUM 开机，重启和用户登陆注销 磁盘分区，挂载 网络配置 2021高级篇日志管理 Linux内核源码&amp;内核升级 定制自己的Linux Linux备份和恢复 Linux可视化管理webmin和bt运维工具 Linux入侵检测&amp;权限划分&amp;系统优化 Linux面试题（腾讯，百度，美团，滴滴 ） p2 应用领域p3 概述p4 Linux和Unixp5 vmware15.5安装下载链接：https://www.nocmd.com/windows/740.html p6 centOS7.6安装下载链接：http://mirrors.163.com/centos/7.6.1810/isos/x86_64/CentOS-7-x86_64-DVD-1810.iso linux分区：3个区 boot 引导分区 1g swap 交换分区 和内存大小一致2g 可以临时充当内存 根分区 17g p7 网络连接的三种方式桥接模式 会直接占用网段，会造成256个不够用 虚拟系统可以和外部系统通讯，但是容易造成IP冲突 NAT模式 网络地址转换模式 虚拟系统可以和外部系统通讯，而且不造成IP冲突 主机模式 不和外部通讯 p8 虚拟机克隆用于快速构建集群 方式一 直接拷贝一份安装好的虚拟机文件 方式二 使用vmware的克隆操作（需要先关闭linux系统） ​ 克隆方法 ​ 1.创建链接克隆（这只是引用） ​ 2.创建完整克隆（这个是拷贝） p9 虚拟机快照在进行一些不确定的操作时，用于恢复原先的某个状态，也叫快照管理 快照会占用一定空间 p10 虚拟机迁移和删除p11 vmtools安装后，在windows下更容易管理vm虚拟机，可以设置windows和centos的共享文件夹 1.进入centOS 2.点击vm菜单的-&gt;install vmware tools 3.centos会出现一个vm安装包，xx.tar.gz 4.拷贝到/opt 5.使用解压命令tar,得到一个安装文件 cd /opt tar -zxvf ./ 进行安装 可能会出现一些问题 参考链接 6.进入该vm解压的目录，/opt目录下 7.安装./vmware-install.pl 8.全部使用默认设置即可安装成功 9.注意：安装vmtools需要有gcc 主机的共享文件夹需要在vmware中设置 共享文件夹位置在/mnt/hgfs/ p12 第4章 linux目录结构linux采用层级树状结构，最上层根目录/ /root root用户的目录 /home 每创建一个用户都会出现一个用户的主目录 /bin 常用指令 环境设置 之类的文件 Binary /sbin s代表Super user的意思 /etc 系统管理所需要的配置文件和子目录 比如安装了mysql数据库 my.conf /boot 系统启动相关 核心文件，包括一些连接文件以及镜像文件 /dev 设备管理器 linux会把所有的硬件映射成一个文件管理 一切皆文件 /media 自动识别设备挂载到这个目录下 /lib 系统开机所需要的最基本的动态连接共享库，作用类似Windows里的DDL文件。几乎所有的应用程序都需要用到这些共享库 /lost+found 一般是空的，当系统非法关机后，这里就存放了一些文件 /usr 用户很多应用程序和文件都放在这个目录下，类似windows下的program files 目录 /proc 这个目录是一个虚拟的目录，它是系统内存的映射，访问这个目录来获取系统信息 /srv service缩写，存放一些服务启动之后需要提取的数据 /sys linux2.6内核很大的一个变化 。安装了新出现的文件系统 sysfs /tmp 存放临时文件 /mnt 为了让用户临时挂载别的文件系统，我们可以把外部存储挂载在/mnt/上，然后进入该目录就可以查看里面的内容了。d:/myshare /opt 这是主机额外安装软件（约定俗成）所摆放的目录。如安装Oracle数据库就可摆放在该目录下 挂载：例如将myshare文件夹挂载在/mnt/hgfs目录下 /usr/local 额外安装软件所安装的目录，一般通过编译源码的方式安装的程序 /var 这个目录存放着不断扩充着的东西，习惯将经常被修改的目录放在这个目录下。包括各种日志文件 /selinux[security-enhanced linux] SELinux是一种安全子系统，它能控制程序只能访问特定文件；三种工作模式，可以自行设置，需要启用 p13 第5章 远程登录到Linux服务器p14 远程登录xshell6 p15 远程文件传输xftp6 p16 vi和vim编辑器常用三种模式正常模式 插入模式 iIoOaArR 命令行模式 输入”esc” + “:” 或 “/“ 再输入:wq “wq”代表写入并退出 p17 vi和vim快捷键命令行模式输入 :wq(保存退出) :q(退出) :q!(强制退出，不保存) 拷贝当前行 yy 拷贝当前行向下五行 5yy 粘贴 p 删除当前行 dd 删除当前行向下五行 5dd 查找 / + 所需的字段 n键用来切换 :setnu 显示行号 :setnonu 关闭显示行号 文档最末行 G 最首行 gg 这些快捷键在一般模式下使用即可 指定行数 输入行号 + shift +g 撤销操作 一般模式下 按 u p18 vi vim 内容整理p19 第七章 开机、重启和用户注销shutdown -h now 立刻进行关机 shutdown -h 1 “hello,1分钟后会关机了” shutdown -r now 现在重新启动计算机 halt 关机，作用和上面一样 reboot 现在重启 sync 内存同步到磁盘 不论重启还是关闭系统，首先要运行sync指令，同步内存至磁盘 目前的shutdown/reboot/halt命令均已经在关机前进行了sync 建议还是先运行sync命令 p20 登录注销su - 用户名 为切换用户 logout在图形级界面运行级别是无效的 在运行级别3下有效 p21 用户管理添加用户useradd 用户名 1.创建用户成功后，会自动创建和用户名同名的home目录 2.也可以通过useradd -d 指定目录 新的用户名，给新创建的用户指定家目录 指定/修改密码 passwd 用户名（不写用户名会给当前登录的用户更改密码） 显示当前用户 pwd 删除用户但是不删除家目录 userdel 用户名 删除用户以及家目录 userdel -r 用户名 操作慎重 这样删除会把用户家目录所有内容删除 一般情况下建议保留家目录 p22 查询用户信息指令基本语法id 用户名 切换用户su - 切换用户名 权限高的用户切换到权限低的不需要输入密码，反之需要 返回到原来的用户 exit/logout 查看当前用户/登录用户基本语法whoami/Who am I p23 用户组介绍类似于角色，系统可对有共性的多个用户进行统一的管理 新增组指令：groupadd 组名 删除组groupdel 组名 增加用户时直接加上组useradd -g 用户组 用户名 修改用户的组usermod -g 用户组 用户名 用户和组相关文件/etc/password 文件用户的配置文件 用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录shell /etc/shadow 文件口令配置文件 登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间标志 /etc/group 文件 组配置文件 组名:口令:组标志号:组内用户列表 p24 用户管理总结p25 第九章 实用指令指定运行级别基本介绍 0：关机 1：单用户【找回丢失密码】 2：多用户状态没有网络服务 3：多用户状态有网络服务 4：系统未使用保留给用户 5：图形界面 6：系统重启 常用运行级别为3和5，也可以指定默认运行级别 init[0123456] 在centos7之前,/etc/inittab文件中指定 简化为 multi-user.target:analogous to runlevel 3 graphical.target:analagous to runlevel 5 当前运行级别 systemctl get-default systemctl set-default TARGET.target p26 如何找回root密码参考链接 p27 帮助指令man 命令或配置文件 Linux下,隐藏文件以.开头 选项可以组合使用 help 命令 p28 文件目录类pwd 指令显示当前工作目录绝对路径 ls 指令ls 目录或文件 常用选项 -a 所有 -l 列表 应用实例 查看当前目录所有内容信息 cd指令cd ~ 或者cd :回到自己的家目录 cd.. 回到当前目录的上一级目录 p28 文件目录类（2）mkdir指令创建目录 mkdir 要创建的目录 常见选项 -p：创建多级目录 案例一：创建一个目录 /home/dog mkdir /home/dog 案例二：创建一个多级目录 /home/animal/tiger mkdir -p /home/animal/tiger rmdir指令删除空目录 rmdir 要删除的空目录 案例：删除一个目录 /home/dog 细节注意：删除的是空目录，有内容则无法删除 如果要删除非空目录，需要使用 rm-rf 要删除的目录 例：rm -rf /home/animal touch 指令创建空文件 touch 文件名称 案例：创建一个空文件 hello.txt p30 文件目录指令（3）cp指令拷贝文件到指定目录 cp [选项] source dest 常用选项 -r：递归复制整个文件夹 cp hello.txt /home/bbb cp -r /home/bbb /opt/ \\cp 表示强制覆盖不提示 rm指令移除文件或目录 rm [选项] 要删除的文件或目录 常用选项： -r：递归删除整个文件夹 -f：强制删除不提示 案例一：将/home/hello.txt 删除，rm /home/hello.txt 案例二：递归删除整个文件夹 /home/bbb，rm -rf /home/bbb p31 文件目录指令（4）mv指令mv移动文件与目录或重命名 基本语法 mv oldNameFile newNameFile （功能描述：重命名） mv /temp/movefile /targetFolder (功能描述：移动文件) 实例 案例一：将/home/cat.txt 文件 重新命名为pig.txt 案例二：将/home/pig.txt 文件 移动到/root目录下 案例三：移动整个目录 cat指令cat 查看文件内容 基本用法 ​ cat [选项] 要查看的文件 常用选项 ​ -n：显示行号 cat只能浏览文件，而不能修改文件，为了浏览方便，一般会带上 管道命令| more 管道指的是 将前面得到的结果交给后面的指令来完成 more 指令基于vi编辑器的文本过滤器，全屏幕按页显示文本文件内容。more指令中内置了若干快捷键 基本语法： more 要查看的文件 less指令分屏查看文件内容，功能与more类似，但比more更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于大型文件具有更高的效率。 基本语法 less 要查看的文件 应用实例 案例: 采用less 查看一个大文件文件 /opt/杂文.txt less /opt/杂文.txt echo 指令echo 输出内容到控制台 基本语法 echo [选项] [输出内容] 应用实例 案例: 使用echo 指令输出环境变量, 比如输出 $PATH $HOSTNAME, echo $HOSTNAME 案例: 使用echo 指令输出hello,world! head 指令head 用于显示文件的开头部分内容，默认情况下head 指令显示文件的前10 行内容 基本语法 head 文件(功能描述：查看文件头10 行内容) head -n 5 文件(功能描述：查看文件头5 行内容，5 可以是任意行数) 应用实例 案例: 查看/etc/profile 的前面5 行代码 head -n 5 /etc/profile tail 指令tail 用于输出文件中尾部的内容，默认情况下tail 指令显示文件的前10 行内容。 基本语法 tail 文件（功能描述：查看文件尾10 行内容） tail -n 5 文件（功能描述：查看文件尾5 行内容，5 可以是任意行数） tail -f 文件（功能描述：实时追踪该文档的所有更新） 应用实例 案例1: 查看/etc/profile 最后5 行的代码 tail -n 5 /etc/profile 案例2: 实时监控mydate.txt , 看看到文件有变化时，是否看到， 实时的追加hello,world tail -f /home/mydate.txt &gt; 指令和&gt;&gt; 指令 &gt; 输出重定向（覆盖）和&gt;&gt; 追加 基本语法 ls -l &gt;文件（功能描述：列表的内容写入文件a.txt 中（覆盖写）） ls -al &gt;&gt;文件（功能描述：列表的内容追加到文件aa.txt 的末尾） cat 文件1 &gt; 文件2 （功能描述：将文件1 的内容覆盖到文件2） echo “内容”&gt;&gt; 文件(追加) 应用实例 案例1: 将/home 目录下的文件列表写入到/home/info.txt 中, 覆盖写入 ls -l /home &gt; /home/info.txt [如果info.txt 没有，则会创建] 案例2: 将当前日历信息追加到/home/mycal 文件中 指令为： cal &gt;&gt; /home/mycal ln 指令link 软链接也称为符号链接，类似于windows 里的快捷方式，主要存放了链接其他文件的路径 基本语法 ln -s [原文件或目录] [软链接名] （功能描述：给原文件创建一个软链接） 应用实例 案例1: 在/home 目录下创建一个软连接myroot，连接到/root 目录 history 指令查看已经执行过历史命令,也可以执行历史指令 基本语法 history （功能描述：查看已经执行过历史命令） 应用实例 案例1: 显示所有的历史命令 history 案例2: 显示最近使用过的10 个指令。 history 10 案例3：执行历史编号为5 的指令 !5 p34 时间日期类时间日期类date 指令-显示当前日期基本语法 date （功能描述：显示当前时间） date +%Y （功能描述：显示当前年份） date +%m（功能描述：显示当前月份） date +%d （功能描述：显示当前是哪一天） date “+%Y-%m-%d %H:%M:%S”（功能描述：显示年月日时分秒） 应用实例案例1: 显示当前时间信息date案例2: 显示当前时间年月日date “+%Y-%m-%d” 案例3: 显示当前时间年月日时分秒date “+%Y-%m-%d %H:%M:%S” date 指令-设置日期基本语法date -s 字符串时间 应用实例案例1: 设置系统当前时间， 比如设置成2020-11-03 20:02:10date -s “2020-11-03 20:02:10” cal 指令查看日历指令cal 基本语法cal [选项] （功能描述：不加选项，显示本月日历） 应用实例案例1: 显示当前日历cal案例2: 显示2020 年日历: cal 2020 p35 查找指令（1）搜索查找类find指令find 指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端。 基本语法find [搜索范围] [选项] 选项说明 应用实例案例1: 按文件名：根据名称查找/home 目录下的hello.txt 文件find /home -name hello.txt案例2：按拥有者：查找/opt 目录下，用户名称为nobody 的文件find /opt -user nobody案例3：查找整个linux 系统下大于200M 的文件（+n 大于-n 小于n 等于, 单位有k,M,G）find / -size +200M ls -lh h表示大小用k，m之类表示 locate 指令locate 指令可以快速定位文件路径。locate 指令利用事先建立的系统中所有文件名称及路径的locate 数据库实现快速定位给定的文件。Locate 指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate 时刻 基本语法locate 搜索文件 特别说明由于locate 指令基于数据库进行查询，所以第一次运行前，必须使用updatedb 指令创建locate 数据库。 应用实例案例1: 请使用locate 指令快速定位hello.txt 文件所在目录which 指令，可以查看某个指令在哪个目录下，比如ls 指令在哪个目录which ls p36 查找指令（2）grep 指令和管道符号 |grep 过滤查找， 管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理。 基本语法grep [选项] 查找内容源文件 常用选项 应用实例案例1: 请在hello.txt 文件中，查找”yes” 所在行，并且显示行号写法1: cat /home/hello.txt | grep “yes”写法2: grep -n “yes” /home/hello.txt p37 压缩和解压类gzip/gunzip 指令gzip 用于压缩文件， gunzip 用于解压的 基本语法gzip 文件（功能描述：压缩文件，只能将文件压缩为*.gz 文件）gunzip 文件.gz （功能描述：解压缩文件命令） 应用实例案例1: gzip 压缩， 将/home 下的hello.txt 文件进行压缩gzip /home/hello.txt案例2: gunzip 压缩， 将/home 下的hello.txt.gz 文件进行解压缩gunzip /home/hello.txt.gz zip/unzip 指令zip 用于压缩文件， unzip 用于解压的，这个在项目打包发布中很有用的 基本语法zip [选项] XXX.zip 将要压缩的内容（功能描述：压缩文件和目录的命令）unzip [选项] XXX.zip （功能描述：解压缩文件） zip 常用选项-r：递归压缩，即压缩目录 unzip 的常用选项-d&lt;目录&gt; ：指定解压后文件的存放目录 p38 压缩和解压类（2）tar 指令tar 指令是打包指令，最后打包后的文件是.tar.gz 的文件。 基本语法tar [选项] XXX.tar.gz 打包的内容(功能描述：打包目录，压缩后的文件格式.tar.gz) 选项说明 应用实例案例1: 压缩多个文件，将/home/pig.txt 和/home/cat.txt 压缩成pc.tar.gztar -zcvf pc.tar.gz /home/pig.txt /home/cat.txt案例2: 将/home 的文件夹压缩成myhome.tar.gztar -zcvf myhome.tar.gz /home/案例3: 将pc.tar.gz 解压到当前目录tar -zxvf pc.tar.gz案例4: 将myhome.tar.gz 解压到/opt/tmp2 目录下(1) mkdir /opt/tmp2 (2) tar -zxvf /home/myhome.tar.gz -C /opt/tmp2 p39 实用指令小结p40 第十章 Linux组的介绍Linux 组基本介绍在linux 中的每个用户必须属于一个组，不能独立于组外。在linux 中每个文件有所有者、所在组、其它组的概念。 所有者 所在组 其它组 改变用户所在的组 p41 所有者文件/目录所有者一般为文件的创建者,谁创建了该文件，就自然的成为该文件的所有者。 查看文件的所有者指令：ls –ahl应用实例 修改文件所有者（change owner） 指令：chown 用户名 文件名应用案例要求：使用root 创建一个文件apple.txt ，然后将其所有者修改成tomchown tom apple.txt 组的创建基本指令groupadd 组名 应用实例创建一个组, ,monstergroupadd monster 创建一个用户fox ，并放入到monster 组中useradd -g monster fox p42 所在组文件/目录所在组当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组(默认)。 查看文件/目录所在组基本指令ls –ahl应用实例, 使用fox 来创建一个文件，看看该文件属于哪个组? -rw-r–r–. 1 fox monster 0 12月 30 15:08 ok.txt 修改文件/目录所在的组基本指令chgrp 组名文件名 应用实例使用root 用户创建文件orange.txt ,看看当前这个文件属于哪个组，然后将这个文件所在组，修改到fruit 组。 groupadd fruit touch orange.txt 看看当前这个文件属于哪个组-&gt; root 组 chgrp fruit orange.txt p43 修改所在组其它组除文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 改变用户所在组在添加用户时，可以指定将该用户添加到哪个组中，同样的用root 的管理权限可以改变某个用户所在的组。 改变用户所在组usermod –g 新组名用户名usermod –d 目录名用户名改变该用户登陆的初始目录。特别说明：用户需要有进入到新目录的权限。 应用实例将zwj 这个用户从原来所在组，修改到wudang 组usermod -g wudang zwj p44 rwx权限权限的基本介绍ls -l 中显示的内容如下：-rwxrw-r– 1 root root 1213 Feb 2 09:39 abc 0-9 位说明第0 位确定文件类型(d, - , l , c , b)-代表是一个普通文件l 是链接，相当于windows 的快捷方式 linkd 是目录，相当于windows 的文件夹c 是字符设备文件，鼠标，键盘b 是块设备，比如硬盘第1-3 位确定所有者（该文件的所有者）拥有该文件的权限。—User第4-6 位确定所属组（同用户组的）拥有该文件的权限，—Group第7-9 位确定其他用户拥有该文件的权限—Other rwx 权限详解，难点rwx 作用到文件 [ r ]代表可读(read): 可以读取,查看 [ w ]代表可写(write): 可以修改,但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被执行 rwx 作用到目录 [ r ]代表可读(read): 可以读取，ls 查看目录内容 [ w ]代表可写(write): 可以修改, 对目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 p45 权限说明案例ls -l 中显示的内容如下：-rwxrw-r– 1 root root 1213 Feb 2 09:39 abc 10 个字符确定不同用户能对文件干什么第一个字符代表文件类型： - l d c b其余字符每3 个一组(rwx) 读(r) 写(w) 执行(x)第一组rwx : 文件拥有者的权限是读、写和执行第二组rw- : 与文件拥有者同一组的用户的权限是读、写但不能执行第三组r– : 不与文件拥有者同组的其他用户的权限是读不能写和执行 可用数字表示为: r=4,w=2,x=1 因此rwx=4+2+1=7 , 数字可以进行组合 其它说明1 文件：硬连接数或目录：子目录数root 用户root 组1213 文件大小(字节)，如果是文件夹，显示4096 字节Feb 2 09:39 最后修改日期abc 文件名 p46 修改权限修改权限-chmod基本说明：通过chmod 指令，可以修改文件或者目录的权限。 第一种方式：+ 、-、= 变更权限u:所有者g:所有组o:其他人a:所有人(u、g、o 的总和) chmod u=rwx,g=rx,o=x 文件/目录名 chmod o+w 文件/目录名 chmod a-x 文件/目录名 案例演示 给abc 文件的所有者读写执行的权限，给所在组读执行权限，给其它组读执行权限。chmod u=rwx,g=rx,o=rx abc 给abc 文件的所有者除去执行的权限，增加组写的权限chmod u-x,g+w abc 给abc 文件的所有用户添加读的权限chmod a+r abc 第二种方式：通过数字变更权限r=4 w=2 x=1 rwx=4+2+1=7chmod u=rwx,g=rx,o=x 文件目录名相当于chmod 751 文件/目录名 案例演示要求：将/home/abc.txt 文件的权限修改成rwxr-xr-x, 使用给数字的方式实现： chmod 755 /home/abc.txt p47 修改所在组和所有者基本介绍chown newowner 文件/目录改变所有者chown newowner:newgroup 文件/目录改变所有者和所在组 -R 如果是目录则使其下所有子文件或目录递归生效 案例演示 请将/home/abc.txt 文件的所有者修改成tomchown tom /home/abc.txt 请将/home/test 目录下所有的文件和目录的所有者都修改成tomchown -R tom /home/test 修改文件/目录所在组-chgrp基本介绍chgrp newgroup 文件/目录【改变所在组】 案例演示请将/home/abc .txt 文件的所在组修改成shaolin (少林)groupadd shaolinchgrp shaolin /home/abc.txt请将/home/test 目录下所有的文件和目录的所在组都修改成shaolin(少林)chgrp -R shaolin /home/test p48 最佳实践-警察和土匪游戏police ， banditjack, jerry: 警察xh, xq: 土匪 创建组groupadd police ; groupadd bandit 创建用户useradd -g police jack ; useradd -g police jerryuseradd -g bandit xh; useradd -g bandit xq jack 创建一个文件，自己可以读r 写w，本组人可以读，其它组没人任何权限首先jack 登录； vim jack.txt ; chmod 640 jack.txt jack 修改该文件，让其它组人可以读, 本组人可以读写chmod o=r,g=r jack.txt xh 投靠警察，看看是否可以读写.usermod -g police xh 测试，看看xh 是否可以读写，xq 是否可以, 小结论，就是如果要对目录内的文件进行操作，需要要有对该目录的相应权限 p49文件权限管理[课堂练习1] 建立两个组（神仙(sx),妖怪(yg)） 建立四个用户(唐僧,悟空，八戒，沙僧) 设置密码 把悟空，八戒放入妖怪唐僧沙僧在神仙 用悟空建立一个文件（monkey.java 该文件要输出i am monkey） 给八戒一个可以r w 的权限 八戒修改monkey.java 加入一句话( i am pig) 唐僧沙僧对该文件没有权限 把沙僧放入妖怪组 让沙僧修改该文件monkey, 加入一句话(“我是沙僧，我是妖怪!”); 对文件夹rwx 的细节讨论和测试!!!x: 表示可以进入到该目录, 比如cdr: 表示可以ls , 将目录的内容显示w: 表示可以在该目录，删除或者创建文件 示意图 课堂练习2，完成如下操作 用root 登录，建立用户mycentos,自己设定密码 用mycentos 登录，在主目录下建立目录test/t11/t1 在t1 中建立一个文本文件aa,用vi 编辑其内容为ls –al 改变aa 的权限为可执行文件[可以将当前日期追加到一个文件],运行该文件./aa 删除新建立的目录test/t11/t1 删除用户mycentos 及其主目录中的内容 将linux 设置成进入到图形界面的 重新启动linux 或关机 p50 第10章总结p51 第11章 定时任务调度crond 任务调度crontab 进行 定时任务的设置 概述任务调度：是指系统在某个时间执行的特定的命令或程序。任务调度分类：1.系统工作：有些重要的工作必须周而复始地执行。如病毒扫描等个别用户工作：个别用户可能希望执行某些程序，比如对mysql 数据库的备份。示意图 p52 crontab基本语法crontab [选项] 常用选项 快速入门设置任务调度文件：/etc/crontab设置个人任务调度。执行crontab –e 命令。接着输入任务到调度文件如：*/1 * * * * ls –l /etc/ &gt; /tmp/to.txt意思说每小时的每分钟执行ls –l /etc/ &gt; /tmp/to.txt 命令 参数细节说明cron表达式 5 个占位符的说明 p53 crond 时间规则特殊时间执行案例 每天的凌晨4点，每10分钟的时间段为 4-5点之内 p54 crond应用实例案例1：每隔1 分钟，就将当前的日期信息，追加到/tmp/mydate 文件中 */1 * * * * date &gt;&gt; /tmp/mydate 案例2：每隔1 分钟， 将当前日期和日历都追加到/home/mycal 文件中步骤: (1) vim /home/my.sh 写入内容date &gt;&gt; /home/mycal 和cal &gt;&gt; /home/mycal(2) 给my.sh 增加执行权限，chmod u+x /home/my.sh(3) crontab -e 增加*/1 * * * * /home/my.sh crond 相关指令conrtab –r：终止任务调度。其实就是删除crondtab -e 中的任务crontab –l：列出当前有那些任务调度service crond restart [重启任务调度] p55 at定时任务基本介绍 at 命令是一次性定时计划任务，at 的守护进程atd 会以后台模式运行，检查作业队列来运行。 默认情况下，atd 守护进程每60 秒检查作业队列（任务队列），有作业时，会检查作业运行时间，如果时间与当前时间匹配，则运行此作业。 at 命令是一次性定时计划任务，执行完一个任务后不再执行此任务了 在使用at 命令的时候，一定要保证atd 进程的启动, 可以使用相关指令来查看ps -ef | grep atd //可以检测atd 是否在运行 ps -ef 检测现在有哪些进程在运行 | grep 过滤 画一个示意图 at 命令格式at [选项] [时间]Ctrl + D 结束at 命令的输入， 输出两次 at 命令选项 at 时间定义at 指定时间的方法： 接受在当天的hh:mm（小时:分钟）式的时间指定。假如该时间已过去，那么就放在第二天执行。例如：04:00 使用midnight（深夜），noon（中午），teatime（饮茶时间，一般是下午4 点）等比较模糊的词语来指定时间。 采用12 小时计时制，即在时间后面加上AM（上午）或PM（下午）来说明是上午还是下午。例如：12pm 指定命令执行的具体日期，指定格式为month day（月日）或mm/dd/yy（月/日/年）或dd.mm.yy（日.月.年），指定的日期必须跟在指定时间的后面。例如：04:00 2021-03-1 使用相对计时法。指定格式为：now + count time-units ，now 就是当前时间，time-units 是时间单位，这里能够是minutes（分钟）、hours（小时）、days（天）、weeks（星期）。count 是时间的数量，几天，几小时。例如：now + 5 minutes 直接使用today（今天）、tomorrow（明天）来指定完成命令的时间。 p56 at任务调度实例案例1：2 天后的下午5 点执行/bin/ls /home 案例2：atq 命令来查看系统中没有执行的工作任务案例3：明天17 点钟，输出时间到指定文件内比如/root/date100.log 案例4：2 分钟后，输出时间到指定文件内比如/root/date200.log 案例5：删除已经设置的任务, atrm 编号atrm 4 //表示将job 队列，编号为4 的job 删除. 默认删除键变^H，只要按住ctrl键，删除键就可以使用了~p57 任务调度小结p58 磁盘分区机制Linux 分区原理介绍 Linux 来说无论有几个分区，分给哪一目录使用，它归根结底就只有一个根目录，一个独立且唯一的文件结构, Linux中每个分区都是用来组成整个文件系统的一部分。 Linux 采用了一种叫“载入”的处理方法，它的整个文件系统中包含了一整套的文件和目录，且将一个分区和一个目录联系起来。这时要载入的一个分区将使它的存储空间在一个目录下获得。 示意图 硬盘说明 Linux 硬盘分IDE 硬盘和SCSI 硬盘，目前基本上是SCSI 硬盘 对于IDE 硬盘，驱动器标识符为“hdx”,其中“hd”表明分区所在设备的类型，这里是指IDE 硬盘了。“x”为盘号（a 为基本盘，b 为基本从属盘，c 为辅助主盘，d 为辅助从属盘）,“”代表分区，前四个分区用数字1 到4 表示，它们是主分区或扩展分区，从5 开始就是逻辑分区。例，hda3 表示为第一个IDE 硬盘上的第三个主分区或扩展分区,hdb2表示为第二个IDE 硬盘上的第二个主分区或扩展分区。 对于SCSI 硬盘则标识为“sdx~”，SCSI 硬盘是用“sd”来表示分区所在设备的类型的，其余则和IDE 硬盘的表示方法一样 查看所有设备挂载情况命令：lsblk 或者lsblk -f p59 增加磁盘应用实例挂载的经典案例说明：下面我们以增加一块硬盘为例来熟悉下磁盘的相关指令和深入理解磁盘分区、挂载、卸载的概念。 如何增加一块硬盘 虚拟机添加硬盘 分区 格式化 挂载 设置可以自动挂载 虚拟机增加硬盘步骤1在【虚拟机】菜单中，选择【设置】，然后设备列表里添加硬盘，然后一路【下一步】，中间只有选择磁盘大小的地方需要修改，至到完成。然后重启系统（才能识别）！ 虚拟机增加硬盘步骤2分区命令fdisk /dev/sdb 开始对/sdb 分区m 显示命令列表p 显示磁盘分区同fdisk –ln 新增分区d 删除分区w 写入并退出 说明： 开始分区后输入n，新增分区，然后选择p ，分区类型为主分区。两次回车默认剩余全部空间。最后输入w写入分区并退出，若不保存退出输入q。 虚拟机增加硬盘步骤3格式化磁盘分区命令:mkfs -t ext4 /dev/sdb1其中ext4 是分区类型 虚拟机增加硬盘步骤4挂载: 将一个分区与一个目录联系起来，mount 设备名称挂载目录例如： mount /dev/sdb1 /newdisk umount 设备名称或者挂载目录 例如： umount /dev/sdb1 或者umount /newdisk 老师注意: 用命令行挂载,重启后会失效 问题：1.能否在一个目录下挂载多个分区 不能，只能挂载一个 如果切换挂载 已经写入的文件位置仍然不变 虚拟机增加硬盘步骤5永久挂载: 通过修改/etc/fstab 实现挂载添加完成后执行mount –a 即刻生效 p60 磁盘情况查询查询系统整体磁盘使用情况基本语法df -h 应用实例查询系统整体磁盘使用情况 查询指定目录的磁盘占用情况基本语法du -h 查询指定目录的磁盘占用情况，默认为当前目录-s 指定目录占用大小汇总-h 带计量单位-a 含文件–max-depth=1 子目录深度-c 列出明细的同时，增加汇总值 应用实例查询/opt 目录的磁盘占用情况，深度为1 p61 磁盘情况-工作实用指令 统计/opt 文件夹下文件的个数ls -l /opt | grep “^-“ | wc -l 统计/opt 文件夹下目录的个数ls -l /opt | grep “^d” | wc -l 统计/opt 文件夹下文件的个数，包括子文件夹里的ls -lR /opt | grep “^-“ | wc -l 统计/opt 文件夹下目录的个数，包括子文件夹里的ls -lR /opt | grep “^d” | wc -l 以树状显示目录结构tree 目录， 注意，如果没有tree ,则使用yum install tree 安装 p62 磁盘挂载小结p63 NAT网络原理图 p64 网络配置指令查看网络IP 和网关ip自动分配与指定ip 查看网关 查看windows 环境的中VMnet8 网络配置(ipconfig 指令) 查看linux 的网络配置ifconfig ping 测试主机之间网络连通性基本语法ping 目的主机（功能描述：测试当前服务器是否可以连接目的主机） 应用实例测试当前服务器是否可以连接百度ping www.baidu.com p65 网络配置实例linux 网络环境配置第一种方法(自动获取)：说明：登陆后，通过界面的来设置自动获取ip，特点：linux 启动后会自动获取IP,缺点是每次自动获取的ip 地址可能不一样 第二种方法(指定ip)说明直接修改配置文件来指定IP,并可以连接到外网(程序员推荐) 编辑vi /etc/sysconfig/network-scripts/ifcfg-ens33要求：将ip 地址配置的静态的，比如: ip 地址为192.168.200.130 ifcfg-ens33 文件说明DEVICE=eth0 #接口名（设备,网卡） HWADDR=00:0C:2x:6x:0x:xx #MAC 地址 TYPE=Ethernet #网络类型（通常是Ethemet） UUID=926a57ba-92c6-4231-bacb-f27e5e6a9f44 #随机id #系统启动的时候网络接口是否有效（yes/no）ONBOOT=yes #IP 的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP 协议|DHCP 协议）BOOTPROTO=static#IP 地址IPADDR=192.168.200.130#网关GATEWAY=192.168.200.2#域名解析器DNS1=192.168.200.2重启网络服务或者重启系统生效service network restart 、reboot p66 主机名和hosts映射设置主机名 为了方便记忆，可以给linux 系统设置主机名, 也可以根据需要修改主机名 指令hostname ： 查看主机名 修改文件在/etc/hostname 指定 修改后，重启生效 设置hosts 映射思考：如何通过主机名能够找到(比如ping) 某个linux 系统? windows在C:\\Windows\\System32\\drivers\\etc\\hosts 文件指定即可 win10无法修改host文件参考 案例: 192.168.200.130 hspedu100 linux在/etc/hosts 文件指定 案例: 192.168.200.1 ThinkPad-PC 主机名解析过程分析(Hosts、DNS)Hosts 是什么一个文本文件，用来记录IP 和Hostname(主机名)的映射关系 DNSDNS，就是Domain Name System 的缩写，翻译过来就是域名系统是互联网上作为域名和IP 地址相互映射的一个分布式数据库 应用实例: 用户在浏览器输入了www.baidu.com 浏览器先检查浏览器缓存中有没有该域名解析IP 地址，有就先调用这个IP 完成解析；如果没有，就检查DNS 解析器缓存，如果有直接返回IP 完成解析。这两个缓存，可以理解为本地解析器缓存 一般来说，当电脑第一次成功访问某一网站后，在一定时间内，浏览器或操作系统会缓存他的IP 地址（DNS 解析记录）.如在cmd 窗口中输入ipconfig /displaydns //DNS 域名解析缓存ipconfig /flushdns //手动清理dns 缓存 如果本地解析器缓存没有找到对应映射，检查系统中hosts 文件中有没有配置对应的域名IP 映射，如果有，则完成解析并返回。 如果本地DNS 解析器缓存和hosts 文件中均没有找到对应的IP，则到域名服务DNS 进行解析域 示意图 p67 网络配置小结p68 进程基本介绍 在LINUX 中，每个执行的程序都称为一个进程。每一个进程都分配一个ID 号(pid,进程号)。=&gt;windows =&gt; linux 每个进程都可能以两种方式存在的。前台与后台，所谓前台进程就是用户目前的屏幕上可以进行操作的。后台进程则是实际在操作，但由于屏幕上无法看到的进程，通常使用后台方式执行。 一般系统的服务都是以后台进程的方式存在，而且都会常驻在系统中。直到关机才才结束。 示意图 p69 ps指令详解显示系统执行的进程基本介绍ps 命令是用来查看目前系统中，有哪些正在执行，以及它们执行的状况。可以不加任何参数. ps 详解 指令：ps –aux|grep xxx ，比如我看看有没有sshd 服务 指令说明 System V 展示风格 USER：用户名称 PID：进程号 %CPU：进程占用CPU 的百分比 %MEM：进程占用物理内存的百分比 VSZ：进程占用的虚拟内存大小（单位：KB） RSS：进程占用的物理内存大小（单位：KB） TT：终端名称,缩写. STAT：进程状态，其中S-睡眠，s-表示该进程是会话的先导进程，N-表示进程拥有比普通优先级更 低的优先级，R- 正在运行，D-短期等待，Z-僵死进程，T-被跟踪或者被停止等等 STARTED：进程的启动时间 TIME：CPU 时间，即进程使用CPU 的总时间 COMMAND：启动进程所用的命令和参数，如果过长会被截断显示 p70 父子进程应用实例要求：以全格式显示当前所有的进程，查看进程的父进程。查看sshd 的父进程信息ps -ef 是以全格式显示当前所有的进程-e 显示所有进程。-f 全格式ps -ef|grep sshd 是BSD 风格UID：用户IDPID：进程IDPPID：父进程IDC：CPU 用于计算执行优先级的因子。数值越大，表明进程是CPU 密集型运算，执行优先级会降低；数值越小，表明进程是I/O 密集型运算，执行优先级会提高STIME：进程启动的时间TTY：完整的终端名称TIME：CPU 时间CMD：启动进程所用的命令和参数 p71 终止进程kill 和killall介绍:若是某个进程执行一半需要停止时，或是已消了很大的系统资源时，此时可以考虑停止该进程。使用kill 命令来完成此项任务。 基本语法kill [选项] 进程号（功能描述：通过进程号杀死/终止进程）killall 进程名称（功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用） 常用选项-9 :表示强迫进程立即停止 最佳实践 案例1：踢掉某个非法登录用户kill 进程号, 比如kill 11421 案例2: 终止远程登录服务sshd, 在适当时候再次重启sshd 服务kill sshd 对应的进程号; /bin/systemctl start sshd.service 案例3: 终止多个gedit , 演示killall gedit 案例4：强制杀掉一个终端, 指令kill -9 bash 对应的进程号 p72 查看进程树14.4.1 基本语法pstree [选项] ,可以更加直观的来看进程信息 14.4.2 常用选项-p :显示进程的PID-u :显示进程的所属用户 14.4.3 应用实例： 案例1：请你树状的形式显示进程的pidpstree -p 案例2：请你树状的形式进程的用户pstree -u p73 服务管理14.5.1 介绍:服务(service) 本质就是进程，但是是运行在后台的，通常都会监听某个端口，等待其它程序的请求，比如(mysqld , sshd防火墙等)，因此我们又称为守护进程，是Linux 中非常重要的知识点。【原理图】 14.5.2 service 管理指令 service 服务名[start | stop | restart | reload | status] 在CentOS7.0 后很多服务不再使用 service ,而是 systemctl (后面专门讲) service 指令管理的服务在/etc/init.d 查看 service 管理指令案例请使用service 指令，查看，关闭，启动network [注意：在虚拟系统演示，因为网络连接会关闭]指令:service network statusservice network stopservice network start 14.5.4 查看服务名:方式1：使用setup -&gt; 系统服务就可以看到全部。setup 按tab会进入图形化界面的下面的菜单 ，利于退出 方式2: /etc/init.d 看到service 指令管理的服务ls -l /etc/init.d p74 服务管理（2）14.5.5 服务的运行级别(runlevel):Linux 系统有7 种运行级别(runlevel)：常用的是级别3 和5运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root 权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)，不支持网络运行级别3：完全的多用户状态(有NFS)，无界面，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11 控制台，登陆后进入图形GUI 模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动开机的流程说明： p75 服务管理（3）14.5.7 chkconfig 指令介绍通过chkconfig 命令可以给服务的各个运行级别设置自启动/关闭chkconfig 指令管理的服务在/etc/init.d 查看注意: Centos7.0 后，很多服务使用systemctl 管理(后面马上讲) chkconfig 基本语法 查看服务chkconfig –list [| grep xxx] chkconfig 服务名–list chkconfig –level 5 服务名on/off 案例演示: 对network 服务进行各种操作, 把network 在3 运行级别,关闭自启动chkconfig –level 3 network offchkconfig –level 3 network on 使用细节chkconfig 重新设置服务后自启动或关闭，需要重启机器reboot 生效. p76 服务管理（4）14.5.8 systemctl 管理指令基本语法： systemctl [start | stop | restart | status] 服务名systemctl 指令管理的服务在/usr/lib/systemd/system 查看 14.5.9 systemctl 设置服务的自启动状态systemctl list-unit-files [ | grep 服务名] (查看服务开机启动状态, grep 可以进行过滤)systemctl enable 服务名(设置服务开机启动)systemctl disable 服务名(关闭服务开机启动)systemctl is-enabled 服务名(查询某个服务是否是自启动的) 14.5.10 应用案例：查看当前防火墙的状况，关闭防火墙和重启防火墙。=&gt; firewalld.servicesystemctl status firewalld; systemctl stop firewalld; systemctl start firewalld 14.5.11 细节讨论：关闭或者启用防火墙后，立即生效。[telnet 测试某个端口即可]这种方式只是临时生效，当重启系统后，还是回归以前对服务的设置。如果希望设置某个服务自启动或关闭永久生效，要使用systemctl [enable|disable] 服务名. [演示] p77 服务管理（5）14.5.12 打开或者关闭指定端口在真正的生产环境，往往需要将防火墙打开，但问题来了，如果我们把防火墙打开，那么外部请求数据包就不能跟服务器监听端口通讯。这时，需要打开指定的端口。比如80、22、8080 等，这个又怎么做呢？老韩给给大家讲一讲。[示意图] 14.5.13 firewall 指令 打开端口: firewall-cmd –permanent –add-port=端口号/协议 关闭端口: firewall-cmd –permanent –remove-port=端口号/协议 重新载入,才能生效: firewall-cmd –reload 查询端口是否开放: firewall-cmd –query-port=端口/协议 14.5.14 应用案例： 启用防火墙， 测试111 端口是否能telnet , 不行 开放111 端口firewall-cmd –permanent –add-port=111/tcp ; 需要firewall-cmd –reload 再次关闭111 端口firewall-cmd –permanent –remove-port=111/tcp ; 需要firewall-cmd –reload p78 动态监控进程介绍：top 与ps 命令很相似。它们都用来显示正在执行的进程。Top 与ps 最大的不同之处，在于top 在执行一段时间可以更新正在运行的的进程。 14.6.2 基本语法top [选项] 14.6.3 选项说明： p79 交互操作说明 14.6.5 应用实例 案例1.监视特定用户, 比如我们监控tom 用户top：输入此命令，按回车键，查看执行的进程。u：然后输入“u”回车，再输入用户名，即可, 案例2：终止指定的进程, 比如我们要结束tom 登录top：输入此命令，按回车键，查看执行的进程。k：然后输入“k”回车，再输入要结束的进程ID 号 案例3:指定系统状态更新的时间(每隔10 秒自动更新), 默认是3 秒top -d 10 p80 监控网络状态14.7.1 查看系统网络情况netstat基本语法netstat [选项] 选项说明-an 按一定顺序排列输出-p 显示哪个进程在调用 应用案例请查看服务名为sshd 的服务的信息。netstat -anp | grep sshd 14.7.2 检测主机连接命令ping：是一种网络检测工具，它主要是用检测远程主机是否正常，或是两部主机间的网线或网卡故障。如: ping 对方ip 地址 p81 进程管理小结p82 rpm管理（1）15.1 rpm 包的管理15.1.1 介绍rpm 用于互联网下载包的打包及安装工具，它包含在某些Linux 分发版中。它生成具有.RPM 扩展名的文件。RPM是RedHat Package Manager（RedHat 软件包管理工具）的缩写，类似windows 的setup.exe，这一文件格式名称虽然打上了RedHat 的标志，但理念是通用的。Linux 的分发版本都有采用（suse,redhat, centos 等等），可以算是公认的行业标准了。 15.1.2 rpm 包的简单查询指令查询已安装的rpm 列表rpm –qa|grep xx举例： 看看当前系统，是否安装了firefox指令: rpm -qa | grep firefox 15.1.3 rpm 包名基本格式一个rpm 包名：firefox-60.2.2-1.el7.centos.x86_64名称:firefox版本号：60.2.2-1适用操作系统: el7.centos.x86_64表示centos7.x 的64 位系统如果是i686、i386 表示32 位系统，noarch 表示通用 15.1.4 rpm 包的其它查询指令：rpm -qa :查询所安装的所有rpm 软件包rpm -qa | morerpm -qa | grep X [rpm -qa | grep firefox ] rpm -q 软件包名:查询软件包是否安装案例：rpm -q firefoxrpm -qi 软件包名：查询软件包信息案例: rpm -qi firefoxrpm -ql 软件包名:查询软件包中的文件比如： rpm -ql firefoxrpm -qf 文件全路径名查询文件所属的软件包rpm -qf /etc/passwdrpm -qf /root/install.log p83 rpm的卸载15.1.5 卸载rpm 包：基本语法rpm -e RPM 包的名称//erase 应用案例删除firefox 软件包rpm -e firefox 细节讨论 如果其它软件包依赖于您要卸载的软件包，卸载时则会产生错误信息。如： $ rpm -e fooremoving these packages would break dependencies:foo is needed by bar-1.0-1 如果我们就是要删除foo 这个rpm 包，可以增加参数–nodeps ,就可以强制删除，但是一般不推荐这样做，因为依赖于该软件包的程序可能无法运行如：$ rpm -e –nodeps foo 15.1.6 安装rpm 包基本语法rpm -ivh RPM 包全路径名称 参数说明i=install 安装v=verbose 提示h=hash 进度条 应用实例演示卸载和安装firefox 浏览器rpm -e firefoxrpm -ivh firefox 😢 2021年2月20日10点59分 p84 yum15.2.1 介绍：Yum 是一个Shell 前端软件包管理器。基于RPM 包管理，能够从指定的服务器自动下载RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包。示意图 15.2.2 yum 的基本指令查询yum 服务器是否有需要安装的软件yum list|grep xx 软件列表 15.2.3 安装指定的yum 包yum install xxx 下载安装 15.2.4 yum 应用实例：案例：请使用yum 的方式来安装firefoxrpm -e firefoxyum list | grep firefoxyum install firefox p85 软件包管理小结p86 安装配置JDK1.816.1 概述如果需要在Linux 下进行JavaEE 的开发，我们需要安装如下软件 16.2 安装JDK16.2.1 安装步骤 mkdir /opt/jdk 通过xftp6 上传到/opt/jdk 下 cd /opt/jdk 解压tar -zxvf jdk-8u261-linux-x64.tar.gz mkdir /usr/local/java mv /opt/jdk/jdk1.8.0_261 /usr/local/java 配置环境变量的配置文件vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_261 export PATH=$JAVA_HOME/bin:$PATH source /etc/profile [让新的环境变量生效] 刷新系统环境变量 16.2.2 测试是否安装成功编写一个简单的Hello.java 输出”hello,world!” p87 tomcat 的安装16.3.1 步骤: 上传安装文件，并解压缩到/opt/tomcat 进入解压目录/bin , 启动tomcat ./startup.sh 开放端口8080 , 回顾firewall-cmd 16.3.2 测试是否安装成功：在windows、Linux 下访问http://linuxip:8080 p88 idea2020 的安装16.4.1 步骤 下载地址: https://www.jetbrains.com/idea/download/#section=windows 解压缩到/opt/idea 启动idea bin 目录下./idea.sh，配置jdk 编写Hello world 程序并测试成功！ p89 mysql5.7 的安装(!!) 新建文件夹/opt/mysql，并cd进去 运行wget http://dev.mysql.com/get/mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar，下载mysql安装包 PS：centos7.6自带的类mysql数据库是mariadb，会跟mysql冲突，要先删除。 运行tar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar 运行rpm -qa|grep mari，查询mariadb相关安装包 运行rpm -e –nodeps mariadb-libs，卸载 然后开始真正安装mysql，依次运行以下几条 rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm 运行systemctl start mysqld.service，启动mysql 然后开始设置root用户密码 Mysql自动给root用户设置随机密码，运行grep “password” /var/log/mysqld.log可看到当前密码 运行mysql -u root -p，用root用户登录，提示输入密码可用上述的，可以成功登陆进入mysql命令行 设置root密码，对于个人开发环境，如果要设比较简单的密码（生产环境服务器要设复杂密码），可以运行 set global validate_password_policy=0; 提示密码设置策略 （validate_password_policy默认值1，） set password for ‘root’@’localhost’ =password(‘xjxj1109’); 运行flush privileges;使密码设置生效 p90 小结p91 shell编程快速入门17.1 为什么要学习Shell 编程 Linux 运维工程师在进行服务器集群管理时，需要编写Shell 程序来进行服务器管理。 对于JavaEE 和Python 程序员来说，工作的需要，你的老大会要求你编写一些Shell 脚本进行程序或者是服务器的维护，比如编写一个定时备份数据库的脚本。 对于大数据程序员来说，需要编写Shell 程序来管理集群 17.2 Shell 是什么Shell 是一个命令行解释器，它为用户提供了一个向Linux 内核发送请求以便运行程序的界面系统级程序，用户可以用Shell 来启动、挂起、停止甚至是编写一些程序。看一个示意图 17.3 Shell 脚本的执行方式17.3.1 脚本格式要求 脚本以#!/bin/bash 开头 脚本需要有可执行权限 chmod u+x [file] 17.3.2 编写第一个Shell 脚本需求说明：创建一个Shell 脚本，输出hello world!vim hello.sh#!/bin/bashecho “hello,world~” 可以使用绝对也可以使用相对路径来执行这个脚本，前提是有可执行权限 ./hello.sh 相对路径 /root/shcode/hello.sh 绝对路径 17.3.3 脚本的常用执行方式方式1(输入脚本的绝对路径或相对路径)说明：首先要赋予helloworld.sh 脚本的+x 权限， 再执行脚本比如./hello.sh 或者使用绝对路径/root/shcode/hello.sh方式2(sh+脚本)说明：不用赋予脚本+x 权限，直接执行即可。比如sh hello.sh , 也可以使用绝对路径 p92 shell变量17.4 Shell 的变量17.4.1 Shell 变量介绍 Linux Shell 中的变量分为，系统变量和用户自定义变量。 系统变量：$HOME、$PWD、$SHELL、$USER 等等，比如： echo $HOME 等等.. 显示当前shell 中所有变量：set 17.4.2 shell 变量的定义基本语法 定义变量：变量名=值 中间不要空格 撤销变量：unset 变量 声明静态变量：readonly 变量，注意：不能unset快速入门 案例1：定义变量A 案例2：撤销变量A 案例3：声明静态的变量B=2，不能unset 12345678910111213141516171819202122#!/bin/bash#案例1：定义变量AA=100#输出变量需要加上$echo A=$Aecho \"A=$A\"#案例2：撤销变量Aunset Aecho \"A=$A\"#案例3：声明静态的变量B=2，不能unsetreadonly B=2echo \"B=$B\"#unset B#将指令返回的结果赋给变量:&lt;&lt;!C=`date`D=$(date)echo \"C=$C\"echo \"D=$D\"!#使用环境变量TOMCAT_HOMEecho \"tomcat_home=$TOMCAT_HOME\" 案例4：可把变量提升为全局环境变量，可供其他shell 程序使用[该案例后面讲] 17.4.3 shell 变量的定义定义变量的规则 变量名称可以由字母、数字和下划线组成，但是不能以数字开头。5A=200(×) 等号两侧不能有空格 变量名称一般习惯为大写， 这是一个规范，我们遵守即可将命令的返回值赋给变量 A=date反引号，运行里面的命令，并把结果返回给变量A A=$(date) 等价于反引号 p93 设置环境变量17.5 设置环境变量17.5.1 基本语法 export 变量名=变量值（功能描述：将shell 变量输出为环境变量/全局变量） source 配置文件（功能描述：让修改后的配置信息立即生效） echo $变量名（功能描述：查询环境变量的值） 示意 17.5.2 快速入门 在/etc/profile 文件中定义TOMCAT_HOME 环境变量 查看环境变量TOMCAT_HOME 的值 在另外一个shell 程序中使用TOMCAT_HOME注意：在输出TOMCAT_HOME 环境变量前，需要让其生效source /etc/profile shell 脚本的多行注释:&lt;&lt;! 内容! p94 位置参数变量17.6.1 介绍当我们执行一个shell 脚本时，如果希望获取到命令行的参数信息，就可以使用到位置参数变量比如： ./myshell.sh 100 200 , 这个就是一个执行shell 的命令行，可以在myshell 脚本中获取到参数信息 17.6.2 基本语法$n （功能描述：n 为数字，$0 代表命令本身，$1-$9 代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@（功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$#（功能描述：这个变量代表命令行中所有参数的个数） 17.6.3 位置参数变量案例：编写一个shell 脚本position.sh ， 在脚本中获取到命令行的各个参数信息。 p95 预定义变量17.7 预定义变量17.7.1 基本介绍就是shell 设计者事先已经定义好的变量，可以直接在shell 脚本中使用 17.7.2 基本语法 $$ （功能描述：当前进程的进程号（PID）） $! （功能描述：后台运行的最后一个进程的进程号（PID）） $？（功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。） 17.7.3 应用实例在一个shell 脚本中简单使用一下预定义变量preVar.sh#!/bin/bashecho “当前执行的进程id=$$”#以后台的方式运行一个脚本，并获取他的进程号/root/shcode/myshell.sh &amp;echo “最后一个后台方式运行的进程id=$!”echo “执行的结果是=$?” p96 运算符17.8.1 基本介绍学习如何在shell 中进行各种运算操作。 17.8.2 基本语法12341) “$((运算式))”或“$[运算式]”或者expr m + n //expression 表达式2) 注意expr 运算符间要有 空格 , 如果希望将expr 的结果赋给某个变量，使用``3) expr m - n4) expr \\*, /, % 乘，除，取余 17.8.3 应用实例oper.sh123456789101112131415161718案例1：计算（2+3）X4 的值案例2：请求出命令行的两个参数[整数]的和20 50#!/bin/bash#案例1：计算（2+3）X4 的值#使用第一种方式RES1=$(((2+3)*4))echo \"res1=$RES1\"#使用第二种方式, 推荐使用RES2=$[(2+3)*4]echo \"res2=$RES2\"#使用第三种方式exprTEMP=`expr 2 + 3`RES4=`expr $TEMP \\* 4`echo \"temp=$TEMP\"echo \"res4=$RES4\"#案例2：请求出命令行的两个参数[整数]的和20 50SUM=$[$1+$2]echo \"sum=$SUM\" p97 条件判断17.9.1 判断语句基本语法 😢[ condition ]（注意condition 前后要有空格）#非空返回true，可使用$?验证（0 为true，&gt;1 为false） 应用实例 🔕[ hspEdu ] 返回true[ ] 返回false[ condition ] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 判断语句 🖕常用判断条件 = 字符串比较 两个整数的比较 -lt 小于-le 小于等于little equal-eq 等于-gt 大于-ge 大于等于-ne 不等于 按照文件权限进行判断 -r 有读的权限-w 有写的权限-x 有执行的权限 按照文件类型进行判断 -f 文件存在并且是一个常规的文件-e 文件存在-d 文件存在并是一个目录 应用实例🚀案例1：”ok”是否等于”ok”判断语句：使用=案例2：23 是否大于等于22判断语句：使用-ge案例3：/root/shcode/aaa.txt 目录中的文件是否存在判断语句： 使用-f代码如下: p98 流程控制17.10.1 if 判断基本语法12345678910111213if [ 条件判断式]then代码fi或者, 多分支if [ 条件判断式]then代码elif [条件判断式]then代码fi 注意事项：[ 条件判断式]，中括号和条件判断式之间必须有空格 应用实例ifCase.sh案例：请编写一个shell 程序，如果输入的参数，大于等于60，则输出”及格了”，如果小于60,则输出”不及格” 😗😏 p99 流程控制（2）17.10.2 case 语句基本语法case $变量名in“值1”）如果变量的值等于值1，则执行程序1;;“值2”）如果变量的值等于值2，则执行程序2;;…省略其他分支… *）如果变量的值都不是以上的值，则执行此程序;;esac 应用实例 testCase.sh案例1 ：当命令行参数是1 时，输出”周一”, 是2 时，就输出”周二”， 其它情况输出”other” p100 for 循环基本语法1🤕for 变量in 值1 值2 值3…do程序/代码done 应用实例testFor1.sh🏭案例1 ：打印命令行输入的参数[这里可以看出$* 和$@ 的区别] 基本语法2🤒for (( 初始值;循环控制条件;变量变化))do程序/代码done 应用实例testFor2.sh👊案例1 ：从1 加到100 的值输出显示 p101 while循环基本语法1while [ 条件判断式]do程序/代码done注意：while 和[有空格，条件判断式和[也有空格 应用实例testWhile.sh案例1 ：从命令行输入一个数n，统计从1+..+ n 的值是多少？ 1234567891011#!/bin/bash#案例1 ：从命令行输入一个数n，统计从1+..+ n 的值是多少？SUM=0i=0while [ $i -le $1 ]doSUM=$[$SUM+$i]#i 自增i=$[$i+1]doneecho \"执行结果=$SUM\" p102 read 读取控制台输入17.11.1 基本语法read(选项)(参数)选项：-p：指定读取值时的提示符；-t：指定读取值时等待的时间（秒），如果没有在指定的时间内输入，就不再等待了。。参数变量：指定读取值的变量名 17.11.2 应用实例testRead.sh案例1：读取控制台输入一个NUM1 值案例2：读取控制台输入一个NUM2 值，在10 秒内输入。代码: 1234567#!/bin/bash#案例1：读取控制台输入一个NUM1 值read -p \"请输入一个数NUM1=\" NUM1echo \"你输入的NUM1=$NUM1\"#案例2：读取控制台输入一个NUM2 值，在10 秒内输入。read -t 10 -p \"请输入一个数NUM2=\" NUM2echo \"你输入的NUM2=$NUM2\" p103 函数17.12.1 函数介绍shell 编程和其它编程语言一样，有系统函数，也可以自定义函数。系统函数中，我们这里就介绍两个。 17.12.2 系统函数basename 基本语法功能：返回完整路径最后/ 的部分，常用于获取文件名basename [pathname] [suffix]basename [string] [suffix] （功能描述：basename 命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。选项：suffix 为后缀，如果suffix 被指定了，basename 会将pathname 或string 中的suffix 去掉。 应用实例案例1：请返回/home/aaa/test.txt 的”test.txt” 部分basename /home/aaa/test.txt dirname 基本语法功能：返回完整路径最后/ 的前面的部分，常用于返回路径部分dirname 文件绝对路径（功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 应用实例案例1：请返回/home/aaa/test.txt 的/home/aaadirname /home/aaa/test.txt p104 自定义函数基本语法[ function ] funname[()]{Action;[return int;]}调用直接写函数名：funname [值]应用实例案例1：计算输入两个参数的和(动态的获取)， getSum代码 123456789101112#!/bin/bash#案例1：计算输入两个参数的和(动态的获取)， getSum#定义函数getSumfunction getSum() {SUM=$[$n1+$n2]echo \"和是=$SUM\"}#输入两个值read -p \"请输入一个数n1=\" n1read -p \"请输入一个数n2=\" n2#调用自定义函数getSum $n1 $n2 p105 Shell 编程综合案例17.13.1 需求分析 每天凌晨2:30 备份数据库hspedu 到/data/backup/db 备份开始和备份结束能够给出相应的提示信息 备份后的文件要求以备份时间为文件名，并打包成.tar.gz 的形式，比如：2021-03-12_230201.tar.gz 在备份的同时，检查是否有10 天前备份的数据库文件，如果有就将其删除。 画一个思路分析图 17.13.2 代码/usr/sbin/mysql_db.backup.sh1234567891011121314151617181920212223242526#备份目录BACKUP=/data/backup/db#当前时间DATETIME=$(date +%Y-%m-%d_%H%M%S)echo $DATETIME#数据库的地址HOST=localhost#数据库用户名DB_USER=root#数据库密码DB_PW=hspedu100#备份的数据库名DATABASE=hspedu#创建备份目录, 如果不存在，就创建[ ! -d \"${BACKUP}/${DATETIME}\" ] &amp;&amp; mkdir -p \"${BACKUP}/${DATETIME}\"#备份数据库mysqldump -u${DB_USER} -p${DB_PW} --host=${HOST} -q -R --databases ${DATABASE} | gzip &gt;${BACKUP}/${DATETIME}/$DATETIME.sql.gz#将文件处理成tar.gzcd ${BACKUP}tar -zcvf $DATETIME.tar.gz ${DATETIME}#删除对应的备份目录rm -rf ${BACKUP}/${DATETIME}#删除10 天前的备份文件find ${BACKUP} -atime +10 -name \"*.tar.gz\" -exec rm -rf {} \\;echo \"备份数据库${DATABASE} 成功~\" p106 备份数据库p107 小结p108 Ubuntu安装p109 中文包p110 ubuntu的root18.4.1 介绍安装ubuntu 成功后，都是普通用户权限，并没有最高root 权限，如果需要使用root 权限的时候，通常都会在命令前面加上sudo 。有的时候感觉很麻烦。(演示)我们一般使用su 命令来直接切换到root 用户的，但是如果没有给root 设置初始密码，就会抛出su : Authenticationfailure 这样的问题。所以，我们只要给root 用户设置一个初始密码就好了。 18.4.2 给root 用户设置密码并使用 输入sudo passwd 命令，设定root 用户密码。 设定root 密码成功后，输入su 命令，并输入刚才设定的root 密码，就可以切换成root 了。提示符$代表一般用户，提示符#代表root 用户。 以后就可以使用root 用户了 输入exit 命令，退出root 并返回一般用户 p111 Ubuntu 下开发Python18.5.1 说明安装好Ubuntu 后，默认就已经安装好Python 的开发环境。 18.5.2 在Ubuntu 下开发一个Python 程序vi hello.py [编写hello.py]python3 hello.py [运行hello.py] p112 APT 软件管理和远程登录19.1 apt 介绍apt 是Advanced Packaging Tool 的简称，是一款安装包管理工具。在Ubuntu 下，我们可以使用apt 命令进行软件包的安装、删除、清理等，类似于Windows 中的软件管理工具。unbuntu 软件管理的原理示意图： 19.2 Ubuntu 软件操作的相关命令sudo apt-get update 更新源 sudo apt-get install package 安装包 sudo apt-get remove package 删除包 sudo apt-cache search package 搜索软件包 sudo apt-cache show package 获取包的相关信息，如说明、大小、版本等 sudo apt-get install package –reinstall 重新安装包 sudo apt-get -f install 修复安装 sudo apt-get remove package –purge 删除包，包括配置文件等 sudo apt-get build-dep package 安装相关的编译环境 sudo apt-get upgrade 更新已安装的包 sudo apt-get dist-upgrade 升级系统 sudo apt-cache depends package 了解使用该包依赖那些包 sudo apt-cache rdepends package 查看该包被哪些包依赖 sudo apt-get source package 更新Ubuntu 软件下载地址 19.3.1 原理介绍(画出示意图) 19.3.2 寻找国内镜像源https://mirrors.tuna.tsinghua.edu.cn/所谓的镜像源：可以理解为提供下载软件的地方，比如Android 手机上可以下载软件的安卓市场；iOS 手机上可以下载软件的AppStore 19.3.3 寻找国内镜像源19.3.4 备份Ubuntu 默认的源地址sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup 19.3.5 更新源服务器列表先清空sources.list 文件复制镜像网站的地址 复制镜像网站的地址， 拷贝到sources.list 文件 p113 更新源和实例19.3.6 更新源更新源地址：sudo apt-get update 19.4 Ubuntu 软件安装，卸载的最佳实践案例说明：使用apt 完成安装和卸载vim 软件，并查询vim 软件的信息：（因为使用了镜像网站， 速度很快）sudo apt-get remove vim //删除sudo apt-get install vim //安装sudo apt-cache show vim //获取软件信息 p114 ubuntu远程登录和集群19.5.1 ssh 介绍SSH 为Secure Shell 的缩写，由IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议。 SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录。几乎所有UNIX/LInux平台都可运行SSH。使用SSH 服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A 机器想被B 机器远程控制，那么，A 机器需要安装SSH 服务器，B 机器需要安装SSH 客户端。和CentOS 不一样，Ubuntu 默认没有安装SSHD 服务(使用netstat 指令查看: apt install net-tools)，因此，我们不能进行远程登录。 19.5.2 原理示意图 19.5.3 安装SSH 和启用sudo apt-get install openssh-server执行上面指令后，在当前这台Linux 上就安装了SSH 服务端和客户端。service sshd restart执行上面的指令，就启动了sshd 服务。会监听端口22 19.5.4 在Windows 使用XShell6/XFTP6 登录Ubuntu前面我们已经安装了XShell6，直接使用即可。注意：使用hspEdu 用户登录，需要的时候再su - 切换成root 用户 19.5.5 从一台linux 系统远程登陆另外一台linux 系统在创建服务器集群时，会使用到该技术 基本语法：ssh 用户名@IP例如：ssh hspedu@192.168.200.130使用ssh 访问，如访问出现错误。可查看是否有该文件～/.ssh/known_ssh 尝试删除该文件解决，一般不会有问题 登出登出命令：exit 或者logout p115 小结p116 CentOS8.1/8.2的使用安装Centos8.1/8.220.1.1 Centos 下载地址CentOS-8.1.1911-x86_64-dvd1.iso CentOS 8.1/8.2 DVD 版8G (未来的主流.)https://mirrors.aliyun.com/centos/8.1.1911/isos/x86_64/ p117 日志管理21.1 基本介绍 日志文件是重要的系统信息文件，其中记录了许多重要的系统事件，包括用户的登录信息、系统的启动信息、系统的安全信息、邮件相关信息、各种服务相关信息等。 日志对于安全来说也很重要，它记录了系统每天发生的各种事情，通过日志来检查错误发生的原因，或者受到攻击时攻击者留下的痕迹。 可以这样理解日志是用来记录重大事件的工具 21.2 系统常用的日志 /var/log/ 目录就是系统日志文件的保存位置，看张图 系统常用的日志 应用案例使用root 用户通过xshell6 登陆, 第一次使用错误的密码，第二次使用正确的密码登录成功看看在日志文件/var/log/secure 里有没有记录相关信息 p118 日志管理服务rsyslogd21.3 日志管理服务rsyslogdCentOS7.6 日志服务是rsyslogd ， CentOS6.x 日志服务是syslogd 。rsyslogd 功能更强大。rsyslogd 的使用、日志文件的格式，和syslogd 服务兼容的。原理示意图 p119 日志服务配置文件查询Linux 中的rsyslogd 服务是否启动ps aux | grep “rsyslog” | grep -v “grep” -v 表示反向匹配 表示查询不包含grep的服务 查询rsyslogd 服务的自启动状态systemctl list-unit-files | grep rsyslog 配置文件：/etc/rsyslog.conf编辑文件时的格式为： . 存放日志文件其中第一个代表日志类型，第二个代表日志级别 日志类型分为：auth ##pam 产生的日志 authpriv ##ssh、ftp 等登录信息的验证信息corn ##时间任务相关kern ##内核lpr ##打印mail ##邮件mark(syslog)-rsyslog##服务内部的信息，时间标识news ##新闻组user ##用户程序产生的相关信息uucp ##unix to nuix copy 主机之间相关的通信local 1-7 ##自定义的日志设备2) 日志级别分为：debug ##有调试信息的，日志通信最多info ##一般信息日志，最常用notice ##最具有重要性的普通条件的信息warning ##警告级别err ##错误级别，阻止某个功能或者模块不能正常工作的信息crit ##严重级别，阻止整个系统或者整个软件不能正常工作的信息alert ##需要立刻修改的信息emerg ##内核崩溃等重要信息none ##什么都不记录注意：从上到下，级别从低到高，记录信息越来越少 由日志服务rsyslogd 记录的日志文件，日志文件的格式包含以下4 列： 事件产生的时间 产生事件的服务器的主机名 产生事件的服务名或程序名 事件的具体信息 日志如何查看实例查看一下/var/log/secure 日志，这个日志中记录的是用户验证和授权方面的信息来分析如何查看 p120 自定义日志服务日志管理服务应用实例在/etc/rsyslog.conf 中添加一个日志文件/var/log/hsp.log,当有事件发送时(比如sshd 服务相关事件)，该文件会接收到信息并保存. 给小伙伴演示重启，登录的情况，看看是否有日志保存 p121 日志轮替21.4.1 基本介绍日志轮替就是把旧的日志文件移动并改名，同时建立新的空日志文件，当旧日志文件超出保存的范围之后，就会进行删除 21.4.2 日志轮替文件命名 centos7 使用logrotate 进行日志轮替管理，要想改变日志轮替文件名字，通过/etc/logrotate.conf 配置文件中“dateext”参数： 如果配置文件中有“dateext”参数，那么日志会用日期来作为日志文件的后缀，例如“secure-20201010”。这样日志文件名不会重叠，也就不需要日志文件的改名， 只需要指定保存日志个数，删除多余的日志文件即可。 如果配置文件中没有“dateext”参数，日志文件就需要进行改名了。当第一次进行日志轮替时，当前的“secure”日志会自动改名为“secure.1”，然后新建“secure”日志， 用来保存新的日志。当第二次进行日志轮替时，“secure.1”会自动改名为“secure.2”， 当前的“secure”日志会自动改名为“secure.1”，然后也会新建“secure”日志，用来保存新的日志，以此类推。 21.4.3 logrotate 配置文件/etc/logrotate.conf 为logrotate 的全局配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243# rotate log files weekly, 每周对日志文件进行一次轮替weekly# keep 4 weeks worth of backlogs, 共保存4 份日志文件，当建立新的日志文件时，旧的将会被删除rotate 4# create new (empty) log files after rotating old ones, 创建新的空的日志文件，在日志轮替后create# use date as a suffix of the rotated file, 使用日期作为日志轮替文件的后缀dateext# uncomment this if you want your log files compressed, 日志文件是否压缩。如果取消注释，则日志会在转储的同时进行压缩#compress#RPM packages drop log rotation information into this directoryinclude /etc/logrotate.d# 包含/etc/logrotate.d/ 目录中所有的子配置文件。也就是说会把这个目录中所有子配置文件读取进来，#下面是单独设置，优先级更高。# no packages own wtmp and btmp -- we'll rotate them here/var/log/wtmp {monthly # 每月对日志文件进行一次轮替create 0664 root utmp # 建立的新日志文件，权限是0664 ，所有者是root ，所属组是utmp 组minsize 1M # 日志文件最小轮替大小是1MB 。也就是日志一定要超过1MB 才会轮替，否则就算时间达到一个月，也不进行日志转储rotate 1 # 仅保留一个日志备份。也就是只有wtmp 和wtmp.1 日志保留而已}/var/log/btmp {missingok # 如果日志不存在，则忽略该日志的警告信息monthlycreate 0600 root utmprotate 1} p122 自定义日志轮替参数说明1234567891011121314151617参数参数说明daily 日志的轮替周期是每天weekly 日志的轮替周期是每周monthly 日志的轮替周期是每月rotate 数字保留的日志文件的个数。0 指没有备份compress 日志轮替时，旧的日志进行压缩create mode owner group 建立新日志，同时指定新日志的权限与所有者和所属组。mail address 当日志轮替时，输出内容通过邮件发送到指定的邮件地址。missingok 如果日志不存在，则忽略该日志的警告信息notifempty 如果日志为空文件，则不进行日志轮替minsize 大小日志轮替的最小值。也就是日志一定要达到这个最小值才会轮替，否则就算时间达到也不轮替size 大小日志只有大于指定大小才进行日志轮替，而不是按照时间轮替。dateext 使用日期作为日志轮替文件的后缀。sharedscripts 在此关键字之后的脚本只执行一次。prerotate/endscript 在日志轮替之前执行脚本命令。postrotate/endscript 在日志轮替之后执行脚本命令。 21.4.4 把自己的日志加入日志轮替 第一种方法是直接在/etc/logrotate.conf 配置文件中写入该日志的轮替策略 第二种方法是在/etc/logrotate.d/目录中新建立该日志的轮替文件，在该轮替文件中写入正确的轮替策略，因为该目录中的文件都会被“include”到主配置文件中，所以也可以把日志加入轮替。 推荐使用第二种方法，因为系统中需要轮替的日志非常多，如果全都直接写入/etc/logrotate.conf 配置文件，那么这个文件的可管理性就会非常差，不利于此文件的维护。 在/etc/logrotate.d/ 配置轮替文件一览 21.4.5 应用实例看一个案例, 在/etc/logrotate.conf 进行配置, 或者直接在/etc/logrotate.d/ 下创建文件hsplog 编写如下内容, 具体轮替的效果可以参考/var/log 下的boot.log 情况. p123 日志轮替机制原理21.5 日志轮替机制原理日志轮替之所以可以在指定的时间备份日志，是依赖系统定时任务。在/etc/cron.daily/目录，就会发现这个目录中是有logrotate 文件(可执行)，logrotate 通过这个文件依赖定时任务执行的。 p124 查看内存日志journalctl 可以查看内存日志, 这里我们看看常用的指令journalctl ##查看全部journalctl -n 3 ##查看最新3 条journalctl –since 19:00 –until 19:10:10 #查看起始时间到结束时间的日志可加日期journalctl -p err ##报错日志journalctl -o verbose ##日志详细内容journalctl _PID=1245 _COMM=sshd ##查看包含这些参数的日志（在详细日志查看）或者journalctl | grep sshd 注意: journalctl 查看的是内存日志, 重启清空演示案例:使用journalctl | grep sshd 来看看用户登录清空, 重启系统，再次查询，看看日志有什么变化没有 p125 小结p126 定制自己的linux系统22.1 基本介绍通过裁剪现有Linux 系统(CentOS7.6)，创建属于自己的min Linux 小系统，可以加深我们对linux 的理解。老韩利用centos7.6，搭建一个小小linux 系统, 很有趣。 22.2 基本原理启动流程介绍：制作Linux 小系统之前，再了解一下Linux 的启动流程：1、首先Linux 要通过自检，检查硬件设备有没有故障2、如果有多块启动盘的话，需要在BIOS 中选择启动磁盘3、启动MBR 中的bootloader 引导程序4、加载内核文件5、执行所有进程的父进程、老祖宗systemd6、欢迎界面在Linux 的启动流程中，加载内核文件时关键文件：1）kernel 文件: vmlinuz-3.10.0-957.el7.x86_642）initrd 文件: initramfs-3.10.0-957.el7.x86_64.img 22.3 制作min linux 思路分析 在现有的Linux 系统(centos7.6)上加一块硬盘/dev/sdb，在硬盘上分两个分区，一个是/boot，一个是/，并将其格式化。需要明确的是，现在加的这个硬盘在现有的Linux 系统中是/dev/sdb，但是，当我们把东西全部设置好时，要把这个硬盘拔除，放在新系统上，此时，就是/dev/sda 在/dev/sdb 硬盘上，将其打造成独立的Linux 系统，里面的所有文件是需要拷贝进去的 作为能独立运行的Linux 系统，内核是一定不能少，要把内核文件和initramfs 文件也一起拷到/dev/sdb 上 以上步骤完成，我们的自制Linux 就完成, 创建一个新的linux 虚拟机，将其硬盘指向我们创建的硬盘，启动即可 示意图 制作自己的min linux(基于CentOS7.6) 首先，我们在现有的linux添加一块大小为20G的硬盘 点击完成，就OK了， 可以使用 lsblk 查看，需要重启 添加完成后，点击确定，然后启动现有的linux(centos7.6)。 通过fdisk来给我们的/dev/sdb进行分区 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 1 [root@localhost ~]# fdisk /dev/sdb 2 Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel 3 Building a new DOS disklabel with disk identifier 0x4fde4cd0. 4 Changes will remain in memory only, until you decide to write them. 5 After that, of course, the previous content won't be recoverable. 6 7 8 Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite) 9 10 11 WARNING: DOS-compatible mode is deprecated. It's strongly recommended to12 switch off the mode (command 'c') and change display units to13 sectors (command 'u').14 15 16 Command (m for help): n17 Command action18 e extended19 p primary partition (1-4)20 p21 Partition number (1-4): 122 First cylinder (1-2610, default 1):23 Using default value 124 Last cylinder, +cylinders or +size{K,M,G} (1-2610, default 2610): +500M25 26 27 Command (m for help): n28 Command action29 e extended30 p primary partition (1-4)31 p32 Partition number (1-4): 233 First cylinder (15-2610, default 15):34 Using default value 1535 Last cylinder, +cylinders or +size{K,M,G} (15-2610, default 2610):36 Using default value 261037 #查看分区38 Command (m for help): p39 40 41 Disk /dev/sdb: 21.5 GB, 21474836480 bytes42 255 heads, 63 sectors/track, 2610 cylinders43 Units = cylinders of 16065 * 512 = 8225280 bytes44 Sector size (logical/physical): 512 bytes / 512 bytes45 I/O size (minimum/optimal): 512 bytes / 512 bytes46 Disk identifier: 0x4fde4cd047 48 49 Device Boot Start End Blocks Id System50 /dev/sdb1 1 14 112423+ 83 Linux51 /dev/sdb2 15 2610 20852370 83 Linux52 #保存并退出53 Command (m for help): w54 The partition table has been altered! 接下来，我们对/dev/sdb的分区进行格式化 12[root@localhost ~]# mkfs.ext4 /dev/sdb1[root@localhost ~]# mkfs.ext4 /dev/sdb2 创建目录，并挂载新的磁盘 123#mkdir -p /mnt/boot /mnt/sysroot #mount /dev/sdb1 /mnt/boot #mount /dev/sdb2 /mnt/sysroot/ 安装grub, 内核文件拷贝至目标磁盘 1234#grub2-install --root-directory=/mnt /dev/sdb#我们可以来看一下二进制确认我们是否安装成功#hexdump -C -n 512 /dev/sdb #cp -rf /boot/* /mnt/boot/ 修改 grub2/grub.cfg 文件, 标红的部分 是需要使用 指令来查看的 在grub.cfg文件中 , 红色部分用 上面 sdb1 的 UUID替换，蓝色部分用 sdb2的UUID来替换, 紫色部分是添加的，表示 selinux给关掉，同时设定一下init，告诉内核不要再去找这个程序了，不然开机的时候会出现错误的 123456789101112131415161718192021222324252627282930313233### BEGIN /etc/grub.d/10_linux ###menuentry 'CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-957.el7.x86_64-advanced-2eef594e-68fc-49a0-8b23-07cf87dda424' { load_video set gfxpayload=keep insmod gzio insmod part_msdos insmod ext2 set root='hd0,msdos1' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint-bios=hd0,msdos1 --hint-efi=hd0,msdos1 --hint-baremetal=ahci0,msdos1--hint='hd0,msdos1' 6ba72e9a-19ec-4552-ae54-e35e735142d4 else search --no-floppy --fs-uuid --set=root 6ba72e9a-19ec-4552-ae54-e35e735142d4 fi linux16 /vmlinuz-3.10.0-957.el7.x86_64 root=UUID=d2e0ce0f-e209-472a-a4f1-4085f777d9bb ro crashkernel=auto rhgb quiet LANG=zh_CN.UTF-8 selinux=0 init=/bin/bash initrd16 /initramfs-3.10.0-957.el7.x86_64.img}menuentry 'CentOS Linux (0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d-advanced-2eef594e-68fc-49a0-8b23-07cf87dda424' { load_video insmod gzio insmod part_msdos insmod ext2 set root='hd0,msdos1' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint-bios=hd0,msdos1 --hint-efi=hd0,msdos1 --hint-baremetal=ahci0,msdos1 --hint='hd0,msdos1' 6ba72e9a-19ec-4552-ae54-e35e735142d4 else search --no-floppy --fs-uuid --set=root 6ba72e9a-19ec-4552-ae54-e35e735142d4 fi linux16 /vmlinuz-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d root=UUID=d2e0ce0f-e209-472a-a4f1-4085f777d9bb ro crashkernel=auto rhgb quiet selinux=0 init=/bin/bash initrd16 /initramfs-0-rescue-5bd4fb8d8e9d4198983fc1344f652b5d.img}### END /etc/grub.d/10_linux ### 创建目标主机根文件系统 1#mkdir -pv /mnt/sysroot/{etc/rc.d,usr,var,proc,sys,dev,lib,lib64,bin,sbin,boot,srv,mnt,media,home,root} 拷贝需要的bash(也可以拷贝你需要的指令)和库文件给新的系统使用 12#cp /lib64/*.* /mnt/sysroot/lib64/ #cp /bin/bash /mnt/sysroot/bin/ 现在我们就可以创建一个新的虚拟机，然后将默认分配的硬盘 移除掉，指向我们刚刚创建的磁盘即可. 这时，很多指令都不能使用，比如 ls , reboot 等，可以将需要的指令拷贝到对应的目录即可 如果要拷贝指令，重新进入到原来的 linux****系统拷贝相应的指令即可，比较将 /bin/ls 拷贝到 /mnt/sysroot/bin 将/sbin/reboot 拷贝到 /mnt/sysroot/sbin 1234root@hspedu100 ~]# mount /dev/sdb2 /mnt/sysroot/[root@hspedu100 ~]# cp /bin/ls /mnt/sysroot/bin/[root@hspedu100 ~]# cp /bin/systemctl /mnt/sysroot/bin/[root@hspedu100 ~]# cp /sbin/reboot /mnt/sysroot/sbin/ 再重新启动新的min linux系统，就可以使用 ls , reboot 指令了 p127 定制自己的linux系统(2)p128 小结p129 Linux 内核源码介绍&amp;内核升级23.1 为什么要阅读linux 内核? 爱好，就是喜欢linux(黑客精神) 想深入理解linux 底层运行机制，对操作系统有深入理解 阅读Linux 内核，你会对整个计算机体系有一个更深刻的认识。作为开发者，不管你从事的是驱动开发，应用开发还是后台开发，你都需要了解操作系统内核的运行机制，这样才能写出更好的代码。 作为开发人员不应该只局限在自己的领域，你设计的模块看起来小，但是你不了解进程的调用机制，你不知道进程为什么会阻塞、就绪、执行几个状态。那么很难写出优质的代码。 找工作面试的需要😏 老韩忠告，作为有追求的程序员，还是应该深入的了解一个操作系统的底层机制,(比如linux/unix) 最好是源码级别的，这样你写多线程高并发程序，包括架构，优化，算法等，高度不一样的，当然老韩也不是要求小伙伴儿把一个非常庞大的Linux 内核每一行都读懂。我觉得。你至少能看几个核心的模块。 23.2 linux0.01 内核源码23.2.1 基本介绍Linux 的内核源代码可以从网上下载, 解压缩后文件一般也都位于linux 目录下。内核源代码有很多版本，可以从linux0.01 内核入手，总共的代码1w 行左右， 最新版本5.9.8 总共代码超过700w 行，非常庞大.内核地址：https://www.kernel.org/ 很多人害怕读Linux 内核，Linux 内核这样大而复杂的系统代码，阅读起来确实有很多困难，但是也不象想象的那么高不可攀。老韩建议可以从linux0.01 入手。 p130 linux0.01 内核源码目录&amp;阅读23.2.2 linux0.01 内核源码目录&amp;阅读老韩提示阅读内核源码技巧 linux0.01 的阅读需要懂c 语言 阅读源码前，应知道Linux 内核源码的整体分布情况。现代的操作系统一般由进程管理、内存管理、文件系统、驱动程序和网络等组成。Linux 内核源码的各个目录大致与此相对应. 在阅读方法或顺序上，有纵向与横向之分。所谓纵向就是顺着程序的执行顺序逐步进行；所谓横向，就是按模块进行。它们经常结合在一起进行。 对于Linux 启动的代码可顺着Linux 的启动顺序一步步来阅读；对于像内存管理部分，可以单独拿出来进行阅读分析。实际上这是一个反复的过程，不可能读一遍就理解linux 内核源码阅读&amp;目录介绍&amp;main.c 说明 p131 linux 内核最新版和内核升级23.3.1 内核地址：https://www.kernel.org/ 查看23.3.2 下载&amp;解压最新版wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.11.2.tar.gztar -zxvf linux-5.8.16.tar.gz 23.3.3 linux 内核升级应用实例将Centos 系统从7.6 内核升级到7.8 版本内核(兼容性问题) 23.3.4 具体步骤，看老师演示uname -a // 查看当前的内核版本yum info kernel -q //检测内核版本，显示可以升级的内核yum update kernel //升级内核yum list kernel -q //查看已经安装的内核 p132 linux 内核最新版和内核升级(2)p133 第24 章linux 系统-备份与恢复24.1 基本介绍实体机无法做快照，如果系统出现异常或者数据损坏，后果严重， 要重做系统，还会造成数据丢失。所以我们可以使用备份和恢复技术linux 的备份和恢复很简单， 有两种方式： 把需要的文件(或者分区)用TAR 打包就行，下次需要恢复的时候，再解压开覆盖即可 使用dump 和restore 命令 示意图 24.2 安装dump 和restore如果linux 上没有dump 和restore 指令，需要先按照yum -y install dumpyum -y install restore 24.3 使用dump 完成备份24.3.1 基本介绍dump 支持分卷和增量备份（所谓增量备份是指备份上次备份后修改/增加过的文件，也称差异备份）。 24.3.2 dump 语法说明dump [ -cu] [-123456789] [ -f &lt;备份后文件名&gt;] [-T &lt;日期&gt;] [ 目录或文件系统]dump []-wW-c ： 创建新的归档文件，并将由一个或多个文件参数所指定的内容写入归档文件的开头。-0123456789： 备份的层级。0 为最完整备份，会备份所有文件。若指定0 以上的层级，则备份至上一次备份以来修改或新增的文件, 到9 后，可以再次轮替.-f &lt;备份后文件名&gt;： 指定备份后文件名-j : 调用bzlib 库压缩备份文件，也就是将备份后的文件压缩成bz2 格式，让文件更小-T &lt;日期&gt;： 指定开始备份的时间与日期-u ： 备份完毕后，在/etc/dumpdares 中记录备份的文件系统，层级，日期与时间等。-t ： 指定文件名，若该文件已存在备份文件中，则列出名称-W ：显示需要备份的文件及其最后一次备份的层级，时间，日期。-w ：与-W 类似，但仅显示需要备份的文件。 24.3.3 dump 应用案例1将/boot 分区所有内容备份到/opt/boot.bak0.bz2 文件中，备份层级为“0”􀀃dump -0uj -f /opt/boot.bak0.bz2 /boot 24.3.4 dump 应用案例2在/boot 目录下增加新文件，备份层级为“1”(只备份上次使用层次“0”备份后发生过改变的数据), 注意比较看看这次生成的备份文件boot1.bak 有多大dump -1uj -f /opt/boot.bak1.bz2 /boot老韩提醒: 通过dump 命令在配合crontab 可以实现无人值守备份 只有分区支持增量备份 24.3.5 dump -W显示需要备份的文件及其最后一次备份的层级，时间，日期 24.3.6 查看备份时间文件cat /etc/dumpdates 24.3.7 dump 备份文件或者目录前面我们在备份分区时，是可以支持增量备份的，如果备份文件或者目录，不再支持增量备份, 即只能使用0 级别备份案例， 使用dump 备份/etc 整个目录dump -0j -f /opt/etc.bak.bz2 /etc/#下面这条语句会报错，提示DUMP: Only level 0 dumps are allowed on a subdirectorydump -1j -f /opt/etc.bak.bz2 /etc/ 24.3.8 老韩提醒如果是重要的备份文件， 比如数据区，建议将文件上传到其它服务器保存，不要将鸡蛋放在同一个篮子. p134 数据备份与恢复24.4 使用restore 完成恢复24.4.1 基本介绍restore 命令用来恢复已备份的文件，可以从dump 生成的备份文件中恢复原文件 24.4.2 restore 基本语法restore [模式选项] [选项] 说明下面四个模式， 不能混用，在一次命令中， 只能指定一种。-C ：使用对比模式，将备份的文件与已存在的文件相互对比。-i：使用交互模式，在进行还原操作时，restors 指令将依序询问用户-r：进行还原模式-t : 查看模式，看备份文件有哪些文件选项-f &lt;备份设备&gt;：从指定的文件中读取备份数据，进行还原操作 24.4.3 应用案例1restore 命令比较模式，比较备份文件和原文件的区别测试mv /boot/hello.java /boot/hello100.javarestore -C -f boot.bak1.bz2 //注意和最新的文件比较 mv /boot/hello100.java /boot/hello.javarestore -C -f boot.bak1.bz2 24.4.4 应用案例2restore 命令查看模式，看备份文件有哪些数据/文件 测试restore -t -f boot.bak0.bz2 24.4.5 应用案例3restore 命令还原模式, 注意细节： 如果你有增量备份，需要把增量备份文件也进行恢复， 有几个增量备份文件，就要恢复几个，按顺序来恢复即可。测试mkdir /opt/boottmpcd /opt/boottmprestore -r -f /opt/boot.bak0.bz2 //恢复到第1 次完全备份状态restore -r -f /opt/boot.bak1.bz2 //恢复到第2 次增量备份状态 24.4.6 应用案例4restore 命令恢复备份的文件，或者整个目录的文件基本语法： restore -r -f 备份好的文件测试[root@hspedu100 opt]# mkdir etctmp[root@hspedu100 opt]# cd etctmp/[root@hspedu100 etctmp]# restore -r -f /opt/etc.bak0.bz2 p135 数据备份与恢复（2）p136 数据备份与恢复小结p137 Linux 可视化管理-webmin 和bt 运维工具25.1 webmin25.1.1 基本介绍Webmin 是功能强大的基于Web 的Unix/linux 系统管理工具。管理员通过浏览器访问Webmin 的各种管理功能并完成相应的管理操作。除了各版本的linux 以外还可用于：AIX、HPUX、Solaris、Unixware、Irix 和FreeBSD 等系统 25.1.2 安装webmin&amp;配置 下载地址: http://download.webmin.com/download/yum/ , 用下载工具下载即可 也可以使用wget http://download.webmin.com/download/yum/webmin-1.700-1.noarch.rpm 安装： rpm -ivh webmin-1.700-1.noarch.rpm 重置密码/usr/libexec/webmin/changepass.pl /etc/webmin root testroot 是webmin 的用户名，不是OS 的, 这里就是把webmin 的root 用户密码改成了test 修改webmin 服务的端口号（默认是10000 出于安全目的）vim /etc/webmin/miniserv.conf # 修改端口 将port=10000 修改为其他端口号，如port=6666 重启webmin/etc/webmin/restart # 重启/etc/webmin/start # 启动/etc/webmin/stop # 停止 防火墙放开6666 端口firewall-cmd –zone=public –add-port=6666/tcp –permanent # 配置防火墙开放6666 端口firewall-cmd –reload # 更新防火墙配置firewall-cmd –zone=public –list-ports # 查看已经开放的端口号 在这个位置我出现了登录不上去的问题，搜了很多方案没有解决😢包括 1.杀进程换端口 2.reboot 3.在gnu上登录 登录webminhttp://ip:6666 可以访问了用root 账号和重置的新密码test p138 webmin演示25.1.3 简单使用演示比如修改语言设置，IP 访问控制，查看进程, 修改密码， 任务调度，mysql 等. p139 bt宝塔介绍和安装25.2 bt(宝塔)25.2.1 基本介绍bt 宝塔Linux 面板是提升运维效率的服务器管理软件，支持一键LAMP/LNMP/集群/监控/网站/FTP/数据库/JAVA 等多项服务器管理功能。 25.2.2 安装和使用 安装: yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh 安装成功后控制台会显示登录地址，账户密码，复制浏览器打开登录， myself command 外网面板地址: http://58.221.242.226:8888/d9b5227e内网面板地址: http://192.168.200.130:8888/d9b5227eusername: tz6prifgpassword: 2276bcb2 p140 介绍25.2.3 使用介绍， 比如可以登录终端, 配置，快捷安装运行环境和系统工具, 添加计划任务脚本http://192.168.200.130:8888/2e673418/ 25.2.4 如果bt 的用户名，密码忘记了，使用bt default 可以查看 p141 小结p142 Linux 面试题-(腾讯,百度,美团,滴滴)26.1 分析日志t.log(访问量)，将各个ip 地址截取，并统计出现次数,并按从大到小排序(腾讯)http://192.168.200.10/index1.htmlhttp://192.168.200.10/index2.htmlhttp://192.168.200.20/index1.htmlhttp://192.168.200.30/index1.htmlhttp://192.168.200.40/index1.htmlhttp://192.168.200.30/order.htmlhttp://192.168.200.10/order.html答案: cat t.txt | cut -d ‘/‘ -f 3 | sort | uniq -c | sort -nr 26.2 统计连接到服务器的各个ip 情况，并按连接数从大到小排序(腾讯)netstat -an | grep ESTABLISHED | awk -F “ “ ‘{print $5}’ | cut -d “:” -f 1 | sort | uniq -c| sort -nr 其他redis 相关指令 123456789101112131415161718# 重新加载系统服务systemctl daemon-reload# 开机启动system enable redis-server.service# 关闭redis-serversystem stop redis-server.service# 启动redis-serversystem start redis-server.service# 重新启动redis-serversystem restart redis-server.service# 查看redis-server运行状态system status redis-server.service","link":"/2021/04/09/linux%E9%9F%A9%E9%A1%BA%E5%B9%B32021/"},{"title":"SpringCloud基础笔记","text":"前言 SpringCloud基础笔记 第1章 微服务介绍1. 课程介绍1.1 课程内容 介绍微服务的由来，以及微服务和 Spring Cloud 之间的关系 介绍 Spring Cloud 核心组件的使用，使小伙伴们通过核心组件可以快速搭建一个微服务架构 介绍 Spring Cloud 中的辅助类组件，例如微服务监控、链路追踪等等 介绍 Spring Cloud Alibaba ，以及相关核心组件的具体用法 1.2 课程收获1.了解微服务的由来以及基本原理 2.学会 Spring Cloud 中各个组件的使用 3.了解 Spring Cloud 中核心组件的运行原理 4.掌握通过 Spring Cloud 搭建微服务架构 5.掌握辅助组件的用法 2 微服务介绍微服务架构越来越流行，这个没有异议。2009 年，Netflix 重新定义了它的应用程序员的开发模型，这个算是微服务的首次探索。20014 年，《Microservices》，这篇文章以一个更加通俗易懂的方式，为大家定义了微服务。为什么要用微服务？互联网应用产品的两大特点： 需求变化快 用户群体庞大在这样的情况下，我们需要构建一个能够灵活扩展，同时能够快速应对外部环境变化的一个应用，使用传统的开发方式，显然无法满足需求。这个时候，微服务就登场了。 2.1 什么是微服务简单来说，微服务就是一种将一个单一应用程序拆分为一组小型服务的方法，拆分完成后，每一个服务都运行在独立的进程中，服务于服务之间采用轻量级的通信机制来进行沟通（Spring Cloud 中采用基于HTTP 的 RESTful API）。每一个服务，都是围绕具体的业务进行构建，例如一个电商系统，订单服务、支付服务、物流服务、会员服务等等，这些拆分后的应用都是独立的应用，都可以独立的部署到生产环境中。就是在采用微服务之后，我们的项目不再拘泥于一种语言，可以 Java、Go、Python、PHP 等等，混合使用，这在传统的应用开发中，是无法想象的。而使用了微服务之后，我们可以根据业务上下文来选择合适的语言和构建工具进行构建。微服务可以理解为是 SOA 的一个传承，一个本质的区别是微服务是一个真正分布式、去中心化的，微服务的拆分比 SOA 更加彻底。 2.2 微服务优势 复杂度可控 独立部署 技术选型灵活 较好的容错性 较强的可扩展性 2.3 使用 Spring Cloud 的优势Spring Cloud 可以理解为微服务这种思想在 Java 领域的一个具体落地。Spring Cloud 在发展之初，就借鉴了微服务的思想，同时结合 Spring Boot，Spring Cloud 提供了组件的一键式启动和部署的能力，极大的简化了微服务架构的落地。Spring Cloud 这种框架，从设计之初，就充分考虑了分布式架构演化所需要的功能，例如服务注册、配置中心、消息总线以及负载均衡等。这些功能都是以可插拔的形式提供出来的，这样，在分布式系统不断演化的过程中，我们的 Spring Cloud 也可以非常方便的进化。 3. Spring Cloud 介绍3.1 什么是 Spring CloudSpring Cloud 是一系列框架的集合，Spring Cloud 内部包含了许多框架，这些框架互相协作，共同来构建分布式系统。利用这些组件，可以非常方便的构建一个分布式系统。 3.2 核心特性 服务注册与发现 负载均衡 服务之间调用 容错、服务降级、断路器 消息总线 分布式配置中心 链路器 3.3 版本名称 不同于其他的框架，Spring Cloud 版本名称是通过 A（Angel）、B（Brixton）、C（Camden）、 D（Dalston）、E（Edgware）、F（Finchley）。。 这样来明明的，这些名字使用了伦敦地铁站的名 字，目前最新版是 H （Hoxton）版。 Spring Cloud 中，除了大的版本之外，还有一些小版本，小版本命名方式如下： M ，M 版是 milestone 的缩写，所以我们会看到一些版本叫 M1、M2 RC，RC 是 Release Candidate，表示该项目处于候选状态，这是正式发版之前的一个状态，所以 我们会看到 RC1、RC2SR，SR 是 Service Release ，表示项目正式发布的稳定版，其实相当于 GA（GenerallyAvailable） 版。所以，我们会看到 SR1、SR2 SNAPSHOT，这个表示快照版 4. Spring Cloud 体系4.1 Spring Cloud 包含的组件Spring Cloud Netflix，这个组件，在 Spring Cloud 成立之初，立下了汗马功劳。但是， 2018 年的断更，也是 Netflix 掉链子了。Spring Cloud Config，分布式配置中心，利用 Git/Svn 来集中管理项目的配置文件Spring Cloud Bus，消息总线，可以构建消息驱动的微服务，也可以用来做一些状态管理等Spring Cloud Spring BootHoxton 2.2.xGreenwich 2.1.xFinchley 2.0.xEdgware 1.5.xDalston 1.5.xSpring Cloud Consul，服务注册发现Spring Cloud Stream，基于 Redis、RabbitMQ、Kafka 实现的消息微服务Spring Cloud OpenFeign，提供 OpenFeign 集成到 Spring Boot 应用中的方式，主要解决微服务之间的调用问题Spring Cloud Gateway，Spring Cloud 官方推出的网关服务Spring Cloud Cloudfoundry，利用 Cloudfoundry 集成我们的应用程序Spring Cloud Security，在 Zuul 代理中，为 OAuth2 客户端认证提供支持Spring Cloud AWS ，快速集成亚马逊云服务Spring Cloud Contract，一个消费者驱动的、面向 Java 的契约框架Spring Cloud Zookeeper，基于 Apache Zookeeper 的服务注册和发现Spring Cloud Data Flow，在一个结构化的平台上，组成数据微服务Spring Cloud Kubernetes，Spring Cloud 提供的针对 Kubernetes 的支持Spring Cloud FunctionSpring Cloud Task，短生命周期的微服务 4.2 Spring Cloud 和 Spring Boot 版本关系Spring Cloud Spring BootHoxton 2.2.xGreenwich 2.1.xFinchley 2.0.xEdgware 1.5.xDalston 1.5.x 第2章 服务注册中心5.Eureka 注册中心Eureka 是 Spring Cloud 中的注册中心，类似于 Dubbo 中的 Zookeeper。那么到底什么是注册中心，我们为什么需要注册中心？我们首先来看一个传统的单体应用： 在单体应用中，所有的业务都集中在一个项目中，当用户从浏览器发起请求时，直接由前端发起请求给后端，后端调用业务逻辑，给前端请求做出响应，完成一次调用。整个调用过程是一条直线，不需要服务之间的中转，所以没有必要引入注册中心。随着公司项目越来越大，我们会将系统进行拆分，例如一个电商项目，可以拆分为订单模块、物流模块、支付模块、CMS 模块等等。这样，当用户发起请求时，就需要各个模块之间进行协作，这样不可避免的要进行模块之间的调用。此时，我们的系统架构就会发生变化： 在这里，大家可以看到，模块之间的调用，变得越来越复杂，而且模块之间还存在强耦合。例如 A 调用B，那么就要在 A 中写上 B 的地址，也意味着 B 的部署位置要固定，同时，如果以后 B 要进行集群化部署，A 也需要修改。为了解决服务之间的耦合，注册中心闪亮登场。 EurekaEureka 是 Netflix 公司提供的一款服务注册中心，Eureka 基于 REST 来实现服务的注册与发现，曾经，Eureka 是 Spring Cloud 中最重要的核心组件之一。Spring Cloud 中封装了 Eureka，在 Eureka 的基础上，优化了一些配置，然后提供了可视化的页面，可以方便的查看服务的注册情况以及服务注册中心集群的运行情况。Eureka 由两部分：服务端和客户端，服务端就是注册中心，用来接收其他服务的注册，客户端则是一个 Java 客户端，用来注册，并可以实现负载均衡等功能。 从图中，我们可以看出，Eureka 中，有三个角色：Eureka Server：注册中心Eureka Provider：服务提供者Eureka Consumer：服务消费者 5.1 Eureka 搭建Eureka 本身是使用 Java 来开发的，Spring Cloud 使用 Spring Boot 技术对 Eureka 进行了封装，所以，在 Spring Cloud 中使用 Eureka 非常方便，只需要引入 spring-cloud-starter-netflix-eurekaserver这个依赖即可，然后就像启动一个普通的 Spring Boot 项目一样启动 Eureka 即可。创建一个普通的 Spring Boot 项目，创建时，添加 Eureka 依赖： 加依赖时出现 问题 始终在报红，后面重写编辑了一下就行了，真是服了 项目创建成功后，在项目启动类上添加注解，标记该项目是一个 Eureka Server： 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication {public static void main(String[] args) {SpringApplication.run(EurekaApplication.class, args);}} @EnableEurekaServer 注解表示开启 Eureka 的功能。接下来，在 application.properties 中添加基本配置信息： 12345678910# 给当前服务取一个名字spring.application.name=eureka# 设置端口号server.port=1111# 默认情况下，Eureka Server 也是一个普通的微服务，所以当它还是一个注册中心的时候，他会有两层身份：1.注册中心；2.普通服务，即当前服务会自己把自己注册到自己上面来# register-with-eureka 设置为 false，表示当前项目不要注册到注册中心上eureka.client.register-with-eureka=false# 表示是否从 Eureka Server 上获取注册信息eureka.client.fetch-registry=false 配置完成后，就可以启动项目了。如果在项目启动时，遇到 java.lang.TypeNotPresentException: Typejavax.xml.bind.JAXBContext not present 异常，这是因为 JDK9 以上，移除了 JAXB，这个时候，只需要我们手动引入 JAXB 即可。 1234567891011121314151617181920&lt;dependency&gt;&lt;groupId&gt;javax.xml.bind&lt;/groupId&gt;&lt;artifactId&gt;jaxb-api&lt;/artifactId&gt;&lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt;&lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt;&lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.glassfish.jaxb&lt;/groupId&gt;&lt;artifactId&gt;jaxb-runtime&lt;/artifactId&gt;&lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;javax.activation&lt;/groupId&gt;&lt;artifactId&gt;activation&lt;/artifactId&gt;&lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; 项目启动成功后，浏览器输入 http://localhost:1111 就可以查看 Eureka 后台管理页面了： 5.2 Eureka 集群使用了注册中心之后，所有的服务都要通过服务注册中心来进行信息交换。服务注册中心的稳定性就非常重要了，一旦服务注册中心掉线，会影响到整个系统的稳定性。所以，在实际开发中，Eureka 一般都是以集群的形式出现的。Eureka 集群，实际上就是启动多个 Eureka 实例，多个 Eureka 实例之间，互相注册，互相同步数据，共同组成一个 Eureka 集群。搭建 Eureka 集群，首先我们需要一点准备工作，修改电脑的 hosts 文件（ C:\\Windows\\System32\\drivers\\etc\\hosts ）： 在 5.1 小节 demo 的基础上，我们在 resources 目录下，再添加两个配置文件，分别为 applicationa.properties 以及 application-b.properties: application-a.properites 内如如下： 12345678910111213# 给当前服务取一个名字spring.application.name=eureka# 设置端口号server.port=1111eureka.instance.hostname=eurekaA# 默认情况下，Eureka Server 也是一个普通的微服务，所以当它还是一个注册中心的时候，他会有两层身份：1.注册中心；2.普通服务，即当前服务会自己把自己注册到自己上面来# register-with-eureka 设置为 false，表示当前项目不要注册到注册中心上eureka.client.register-with-eureka=true# 表示是否从 Eureka Server 上获取注册信息eureka.client.fetch-registry=true# A 服务要注册到 B 上面eureka.client.service-url.defaultZone=http://eurekaB:1112/eureka application-b.properites 内如如下： 123456789101112# 给当前服务取一个名字spring.application.name=eureka# 设置端口号server.port=1112eureka.instance.hostname=eurekaB# 默认情况下，Eureka Server 也是一个普通的微服务，所以当它还是一个注册中心的时候，他会有两层身份：1.注册中心；2.普通服务，即当前服务会自己把自己注册到自己上面来# register-with-eureka 设置为 false，表示当前项目不要注册到注册中心上eureka.client.register-with-eureka=true# 表示是否从 Eureka Server 上获取注册信息eureka.client.fetch-registry=trueeureka.client.service-url.defaultZone=http://eurekaA:1111/eureka 配置完成后，对当前项目打包，打成 jar 包： 打包完成后，在命令行启动两个 Eureka 实例。两个启动命令分别如下： 12java -jar eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=ajava -jar eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=b 启动成功后，就可以看到，两个服务之间互相注册，共同给组成一个集群。 5.3 Eureka 工作细节Eureka 本身可以分为两大部分，Eureka Server 和 Eureka Client 5.3.1 Eureka ServerEureka Server 主要对外提供了三个功能： 服务注册，所有的服务都注册到 Eureka Server 上面来 提供注册表，注册表就是所有注册上来服务的一个列表，Eureka Client 在调用服务时，需要获取 这个注册表，一般来说，这个注册表会缓存下来，如果缓存失效，则直接获取最新的注册表 同步状态，Eureka Client 通过注册、心跳等机制，和 Eureka Server 同步当前客户端的状态 5.3.2 Eureka Client Eureka Client 主要是用来简化每一个服务和 Eureka Server 之间的交互。Eureka Client 会自动拉取、 更新以及缓存 Eureka Server 中的信息，这样，即使 Eureka Server 所有节点都宕机，Eureka Client 依然能够获取到想要调用服务的地址（但是地址可能不准确）。 5.3.2.1 服务注册服务提供者将自己注册到服务注册中心（Eureka Server），需要注意，所谓的服务提供者，只是一个业务上上的划分，本质上他就是一个 Eureka Client。当 Eureka Client 向 Eureka Server 注册时，他需要提供自身的一些元数据信息，例如 IP 地址、端口、名称、运行状态等等。 5.3.2.2 服务续约Eureka Client 注册到 Eureka Server 上之后，事情没有结束，刚刚开始而已。注册成功后，默认情况下，Eureka CLient 每隔 30 秒就要向 Eureka Server 发送一条心跳消息，来告诉 Eureka Server 我还在运行。如果 Eureka Server 连续 90 秒都有没有收到 Eureka Client 的续约消息（连续三次没发送），它会认为 Eureka Client 已经掉线了，会将掉线的 Eureka Client 从当前的服务注册列表中剔除。 服务续约，有两个相关的属性（一般不建议修改）： 12eureka.instance.lease-renewal-interval-in-seconds=30eureka.instance.lease-expiration-duration-in-seconds=90 eureka.instance.lease-renewal-interval-in-seconds 表示服务的续约时间，默认是 30 秒 eureka.instance.lease-expiration-duration-in-seconds 服务失效时间，默认是 90 秒 5.3.2.3 服务下线当 Eureka Client 下线时，它会主动发送一条消息，告诉 Eureka Server ，我下线啦。 5.3.2.4 获取注册表信息Eureka Client 从 Eureka Server 上获取服务的注册信息，并将其缓存在本地。本地客户端，在需要调用远程服务时，会从该信息中查找远程服务所对应的 IP 地址、端口等信息。Eureka Client 上缓存的服务注册信息会定期更新(30 秒)，如果 Eureka Server 返回的注册表信息与本地缓存的注册表信息不同的话，Eureka Client 会自动处理。这里，也涉及到两个属性，一个是是否允许获取注册表信息： 1eureka.client.fetch-registry=true Eureka Client 上缓存的服务注册信息，定期更新的时间间隔，默认 30 秒： 1eureka.client.registry-fetch-interval-seconds=30 5.4 Eureka 集群原理我们来看官方的一张 Eureka 集群架构图： 在这个集群架构中，Eureka Server 之间通过 Replicate 进行数据同步，不同的 Eureka Server 之间不区分主从节点，所有节点都是平等的。节点之间，通过置顶 serviceUrl 来互相注册，形成一个集群，进而提高节点的可用性。 在 Eureka Server 集群中，如果有某一个节点宕机，Eureka Client 会自动切换到新的 Eureka Server上。每一个 Eureka Server 节点，都会互相同步数据。Eureka Server 的连接方式，可以是单线的，就是 A–&gt;b–&gt;C ，此时，A 的数据也会和 C 之间互相同步。但是一般不建议这种写法，在我们配置serviceUrl 时，可以指定多个注册地址，即 A 可以即注册到 B 上，也可以同时注册到 C 上。Eureka 分区： region：地理上的不同区域 zone：具体的机房 第3章 服务注册与消费6.1 服务注册服务注册就是把一个微服务注册到 Eureka Server 上，这样，当其他服务需要调用该服务时，只需要从Eureka Server 上查询该服务的信息即可。这里我们创建一个 provider，作为我们的服务提供者，创建项目时，选择 Eureka Client 依赖，这样，当服务创建成功后，简单配置一下，就可以被注册到 Eureka Server 上了： 项目创建成功后，我们只需要在 application.properties 中配置一下项目的注册地址即可。注册地址的配置，和 Eureka Server 集群的配置很像。配置如下： 123spring.application.name=providerserver.port=1113eureka.client.service-url.defaultZone=http://localhost:1111/eureka 6.2 服务消费首先在 provider 中提供一个接口，然后创建一个新的 consumer 项目，消费这个接口。在 provider 中，提供一个 hello 接口，如下： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String Hello(){ return \"hello lunanboy\"; } } 接下来，创建一个 consumer 项目，consumer 项目中，去消费 provider 提供的接口。consumer 要能够获取到 provider 这个接口的地址，他就需要去 Eureka Server 中查询，如果直接在 consumer 中写死 provider 地址，意味着这两个服务之间的耦合度就太高了，我们要降低耦合度。首先我们来看一个写死的调用。 创建一个 consumer 项目，添加 web 和 eureka client 依赖： 创建完成后，我们首先也在 application.properties 中配置一下注册信息： 配置完成后，假设我们现在想在 consumer 中调用 provider 提供的服务，我们可以直接将调用写死，就是说，整个调用过程不会涉及到 Eureka Server。 12345678910111213141516171819@GetMapping(\"/hello1\")public String hello1() { HttpURLConnection con = null; try { URL url = new URL(\"http://localhost:1113/hello\"); con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String s = br.readLine(); br.close(); return s; } } catch (MalformedURLException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return \"error\";} 这是一段利用了 HttpUrlConnection 来发起的请求，请求中 provider 的地址写死了，意味着 provider和 consumer 高度绑定在一起，这个不符合微服务的思想。 要改造它，我们可以借助 Eureka Client 提供的 DiscoveryClient 工具，利用这个工具，我们可以根据服务名从 Eureka Server 上查询到一个服务的详细信息，改造后的代码如下： 123456789101112131415161718192021222324252627282930313233343536//导springcloud的@AutowiredDiscoveryClient discoveryClient;@GetMapping(\"/hello2\")public String hello2() { // provider在部署时可能是一个集群化的 List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(\"provider\"); // 现在仅有一项 ServiceInstance serviceInstance = list.get(0); String host = serviceInstance.getHost(); int port = serviceInstance.getPort(); HttpURLConnection conn = null; StringBuffer sb = new StringBuffer(); sb.append(\"http://\") .append(host) .append(\":\") .append(port) .append(\"/hello\"); try { URL url = new URL(sb.toString()); conn = (HttpURLConnection) url.openConnection(); if (conn.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(conn.getInputStream())); String s = br.readLine(); br.close(); return s; } } catch (MalformedURLException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return \"error\";} 注意，DiscoveryClient 查询到的服务列表是一个集合，因为服务在部署的过程中，可能是集群化部署，集合中的每一项就是一个实例。这里我们可以稍微展示一下集群化部署。首先，修改 provider 中的 hello 接口： 123456789@RestControllerpublic class HelloController {@Value(\"${server.port}\")Integer port;@GetMapping(\"/hello\")public String hello() {return \"hello javaboy:\" + port;}} 因为我一会会启动多个 provider 实例，多个 provider 实例的端口不同，为了区分调用时到底是哪一个provider 提供的服务，这里在接口返回值中返回端口。修改完成后，对 provider 进行打包。provider 打包成功之后，我们在命令行启动两个 provider 实例： 12java -jar provider-0.0.1-SNAPSHOT.jar --server.port=1113java -jar provider-0.0.1-SNAPSHOT.jar --server.port=1116 启动完成后，检查 Eureka Server 上，这两个 provider 是否成功注册上来。注册成功后，在 consumer 中再去调用 provider，DiscoveryClient 集合中，获取到的就不是一个实例了，而是两个实例。这里我们可以手动实现一个负载均衡： 12345678910111213141516171819202122232425262728293031int count = 0;@GetMapping(\"/hello3\")public String hello3() {List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(\"provider\");ServiceInstance instance = list.get((count++) % list.size());String host = instance.getHost();int port = instance.getPort();StringBuffer sb = new StringBuffer();sb.append(\"http://\").append(host).append(\":\").append(port).append(\"/hello\");HttpURLConnection con = null;try {URL url = new URL(sb.toString());con = (HttpURLConnection) url.openConnection();if (con.getResponseCode() == 200) {BufferedReader br = new BufferedReader(newInputStreamReader(con.getInputStream()));String s = br.readLine();br.close();return s;}} catch (MalformedURLException e) {e.printStackTrace();} catch (IOException e) {e.printStackTrace();}return \"error\";} 在从集合中，获取数据时，通过一个小小举动，就可以实现线性负载均衡。 6.2.2 升级改造从两个方面进行改造： Http 调用 负载均衡 Http 调用，我们使用 Spring 提供的 RestTemplate 来实现。 首先，在当前服务中，提供一个 RestTemplate 的实例： 12345678910@SpringBootApplicationpublic class ConsumerApplication {public static void main(String[] args) {SpringApplication.run(ConsumerApplication.class, args);}@BeanRestTemplate restTemplateOne() {return new RestTemplate();}} 然后，在 Http 调用时，不再使用 HttpUrlConnection，而是直接使用 RestTemplate： 12345678910111213141516171819202122232425//导springcloud的 @Autowired DiscoveryClient discoveryClient; @Autowired RestTemplate restTemplate; @GetMapping(\"/hello2\") public String hello2() { // provider在部署时可能是一个集群化的 List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(\"provider\"); // 现在仅有一项 ServiceInstance serviceInstance = list.get(0); String host = serviceInstance.getHost(); int port = serviceInstance.getPort(); HttpURLConnection conn = null; StringBuffer sb = new StringBuffer(); sb.append(\"http://\") .append(host) .append(\":\") .append(port) .append(\"/hello\"); String s = restTemplate.getForObject(sb.toString(), String.class); return s; } 用 RestTemplate ，一行代码就实现了 Http 调用。接下来，使用 Ribbon 来快速实现负载均衡。首先，我们需要给 RestTemplate 实例添加一个 @LoadBalanced 注解，开启负载均衡：此时 12345@Bean@LoadBalancedRestTemplate restTemplate() {return new RestTemplate();} 此时的 RestTemplate 就自动具备了负载均衡的功能。此时的调用代码如下 123456@GetMapping(\"/hello3\") public String hello3() { // 给定一个模糊的provider 随机出一个 // 既有负载均衡又有调用 return restTemplate.getForObject(\"http://provider/hello\", String.class); } Java 中关于 Http 请求的工具实际上非常多，自带的 HttpUrlConnection，古老的 HttpClient，后起之秀 OkHttp 等，除了这些之外，还有一个好用的工具–RestTemplate，这是 Spring 中就开始提供的Http 请求工具，不过很多小伙伴们可能是因为 Spring Cloud 才听说它。今天我们就来聊一聊这个RestTemplate。 6.3 RestTemplateRestTemplate 是从 Spring3.0 开始支持的一个 Http 请求工具，这个请求工具和 Spring Boot 无关，更和 Spring Cloud 无关。RestTemplate 提供了常见的 REST 请求方法模板，例如 GET、POST、PUT、DELETE 请求以及一些通用的请求执行方法 exchange 和 execute 方法。RestTemplate 本身实现了 RestOperations 接口，而在 RestOperations 接口中，定义了常见的RESTful 操作，这些操作在 RestTemplate 中都得到了很好的实现。 6.3.1 GET首先我们在 provider 中定义一个 hello2 接口： 1234@GetMapping(\"/hello2\") public String Hello2(String name){ return \"hello \" + name; } 接下来，我们在 consumer 去访问这个接口，这个接口是一个 GET 请求，所以，访问方式，就是调用RestTemplate 中的 GET 请求。可以看到，在 RestTemplate 中，关于 GET 请求，一共有如下两大类方法： 这两大类方法实际上是重载的，唯一不同的，就是返回值类型。getForObject 返回的是一个对象，这个对象就是服务端返回的具体值。getForEntity 返回的是一个ResponseEntity，这个ResponseEntity 中除了服务端返回的具体数据外，还保留了 Http 响应头的数据。 1234567891011121314151617181920212223@GetMapping(\"/hello4\") public void hello4(){ String s1 = restTemplate.getForObject(\"http://provider/hello2?name={1}\", String.class,\"javaboy\"); System.out.println(s1); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://provider/hello2\", String.class); //服务端返回的具体数据 String body = responseEntity.getBody(); //状态码 HttpStatus statusCode = responseEntity.getStatusCode(); System.out.println(\"HttpStatus = \" + statusCode); // int statusCodeValue = responseEntity.getStatusCodeValue(); System.out.println(\"statusCodeValue = \" + statusCodeValue); //响应头 HttpHeaders headers = responseEntity.getHeaders(); //header里有很多种 需要遍历以下 Set&lt;String&gt; keySet = headers.keySet(); System.out.println(\"--------------header----------------\"); for (String s : keySet) { System.out.println(s+\":\" + headers.get(s)); } } 这里大家可以看到，getForObject 直接拿到了服务的返回值，getForEntity 不仅仅拿到服务的返回值，还拿到 http 响应的状态码。然后，启动 Eureka Server、provider 以及 consumer ，访问 consumer中的 hello4 接口，既可以看到请求结果。看清楚两者的区别之后，接下来看下两个各自的重载方法，getForObject 和 getForEntity 分别有三个重载方法，两者的三个重载方法基本都是一致的。所以，这里，我们主要看其中一种。三个重载方法，其实代表了三种不同的传参方式。 1234567891011121314@GetMapping(\"/hello5\") public void hello5() throws UnsupportedEncodingException { String s1 = restTemplate.getForObject(\"http://provider/hello2?name={1}\", String.class,\"javaboy\"); System.out.println(s1); HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"zhangsan\"); s1 = restTemplate.getForObject(\"http://provider/hello2?name={name}\", String.class, map); System.out.println(s1); String url = \"http://provider/hello2?name=\" + URLEncoder.encode(\"张三\", \"UTF-8\"); URI uri = URI.create(url); s1 = restTemplate.getForObject(uri, String.class); System.out.println(s1); } 这就是我们说的三种不同的传参方式。 6.3.2 POST首先在 provider 中提供两个 POST 接口，同时，因为 POST 请求可能需要传递 JSON，所以，这里我们创建一个普通的 Maven 项目作为 commons 模块，然后这个 commons 模块被 provider 和consumer 共同引用，这样我们就可以方便的传递 JSON 了。commons 模块创建成功后，首先在 commons 模块中添加 User 对象，然后该模块分别被 provider 和consumer 引用。然后，我们在 provider 中，提供和两个 POST 接口： 12345678@PostMapping(\"/user1\")public User addUser1(User user) {return user;}@PostMapping(\"/user2\")public User addUser2(@RequestBody User user) {return user;} 这里定义了两个 User 添加的方法，两个方法代表了两种不同的传参方式。第一种方法是以 key/value形式来传参，第二种方法是以 JSON 形式来传参。定义完成后，接下来，我们在 consumer 中调用这两个 POST 接口。 可以看到，这里的 post 和前面的 get 非常像，只是多出来了三个方法，就是 postForLocation，另外两个 postForObject 和 postForEntiy 和前面 get 基本一致，所以这里我们主要来看 postForObject，看完之后，我们再来看这个额外的 postForLocation。 1234567891011121314@GetMapping(\"/hello6\")public void hello6() {MultiValueMap&lt;String, Object&gt; map = new LinkedMultiValueMap&lt;&gt;();map.add(\"username\", \"javaboy\");map.add(\"password\", \"123\");map.add(\"id\", 99);User user = restTemplate.postForObject(\"http://provider/user1\", map,User.class);System.out.println(user);user.setId(98);user = restTemplate.postForObject(\"http://provider/user2\", user,User.class);System.out.println(user);} post 参数到底是 key/value 形式还是 json 形式，主要看第二个参数，如果第二个参数是MultiValueMap ，则参数是以 key/value 形式来传递的，如果是一个普通对象，则参数是以 json 形式来传递的。最后再看看一下 postForLocation 。有的时候，当我执行完一个 post 请求之后，立马要进行重定向，一个非常常见的场景就是注册，注册是一个 post 请求，注册完成之后，立马重定向到登录页面去登录。对于这种场景，我们就可以使用 postForLocation。首先我们在 provider 上提供一个用户注册接口： 12345678910111213@Controllerpublic class RegisterController {@PostMapping(\"/register\")public String register(User user) {return \"redirect:http://provider/loginPage?username=\" +user.getUsername();}@GetMapping(\"/loginPage\")@ResponseBodypublic String loginPage(String username) {return \"loginPage:\" + username;}} 注意，这里的 post 接口，响应一定是 302，否则 postForLocation 无效。注意，重定向的地址，一定要写成绝对路径，不要写相对路径，否则在 consumer 中调用时会出问题 12345678910@GetMapping(\"/hello7\")public void hello7() {MultiValueMap&lt;String, Object&gt; map = new LinkedMultiValueMap&lt;&gt;();map.add(\"username\", \"javaboy\");map.add(\"password\", \"123\");map.add(\"id\", 99);URI uri = restTemplate.postForLocation(\"http://provider/register\", map);String s = restTemplate.getForObject(uri, String.class);System.out.println(s);} 这就是 postForLocation ，调用该方法返回的是一个 Uri，这个 Uri 就是重定向的地址（里边也包含了重定向的参数），拿到 Uri 之后，就可以直接发送新的请求了。 6.3.3 PUTPUT 请求比较简单，重载的方法也比较少。我们首先在 provider 中提供一个 PUT 接口： 12345678@PutMapping(\"/user1\")public void updateUser1(User user) {System.out.println(user);}@PutMapping(\"/user2\")public void updateUser2(@RequestBody User user) {System.out.println(user);} 注意，PUT 接口传参其实和 POST 很像，也接受两种类型的参数，key/value 形式以及 JSON 形式。在 consumer 中，我们来调用该接口： 12345678910111213@GetMapping(\"/hello8\")public void hello8() {MultiValueMap&lt;String, Object&gt; map = new LinkedMultiValueMap&lt;&gt;();map.add(\"username\", \"javaboy\");map.add(\"password\", \"123\");map.add(\"id\", 99);restTemplate.put(\"http://provider/user1\", map);User user = new User();user.setId(98);user.setUsername(\"zhangsan\");user.setPassword(\"456\");restTemplate.put(\"http://provider/user1\", user);} consumer 中的写法基本和 post 类似，也是两种方式，可以传递两种不同类型的参数。 6.3.4 DELETEDELETE 也比较容易，我们有两种方式来传递参数，key/value 形式或者 PathVariable（参数放在路径中），首先我们在 provider 中定义两个 DELETE 方法： 12345678@DeleteMapping(\"/user1\")public void deleteUser1(Integer id) {System.out.println(id);}@DeleteMapping(\"/user2/{id}\")public void deleteUser2(@PathVariable Integer id) {System.out.println(id);} 然后在 consumer 中调用这两个删除的接口： 12345@GetMapping(\"/hello9\")public void hello9() {restTemplate.delete(\"http://provider/user1?id={1}\", 99);restTemplate.delete(\"http://provider/user2/{1}\", 99);} delete 中参数的传递，也支持 map，这块实际上和 get 是一样的。 6.4 客户端负载均衡客户端负载均衡就是相对服务端负载均衡而言的。服务端负载均衡，就是传统的 Nginx 的方式，用 Nginx 做负载均衡，我们称之为服务端负载均衡： 这种负载均衡，我们称之为服务端负载均衡，它的一个特点是，就是调用的客户端并不知道具体是哪一个 Server 提供的服务，它也不关心，反正请求发送给 Nginx，Nginx 再将请求转发给 Tomcat，客户端只需要记着 Nginx 的地址即可。客户端负载均衡则是另外一种情形： 客户端负载均衡，就是调用的客户端本身是知道所有 Server 的详细信息的，当需要调用 Server 上的接口的时候，客户端从自身所维护的 Server 列表中，根据提前配置好的负载均衡策略，自己挑选一个Server 来调用，此时，客户端知道它所调用的是哪一个 Server。在 RestTemplate 中，要想使用负载均衡功能，只需要给 RestTemplate 实例上添加一个@LoadBalanced 注解即可，此时，RestTemplate 就会自动具备负载均衡功能，这个负载均衡就是客户端负载均衡。 6.5 负载均衡原理在 Spring Cloud 中，实现负载均衡非常容易，只需要添加 @LoadBalanced 注解即可。只要添加了该注解，一个原本普普通通做 Rest 请求的工具 RestTemplate 就会自动具备负载均衡功能，这个是怎么实现的呢？整体上来说，这个功能的实现就是三个核心点： 从 Eureka Client 本地缓存的服务注册信息中，选择一个可以调用的服务 根据 1 中所选择的服务，重构请求 URL 地址 将 1、2 步的功能嵌入到 RestTemplate 中 第4章 Consul在 Spring Cloud 中，大部分组件都有备选方案，例如注册中心，除了常见 Eureka 之外，像zookeeper 我们也可以直接使用在 Spring Cloud 中，还有另外一个比较重要的方案，就是 Consul。Consul 是 HashiCorp 公司推出来的开源产品。主要提供了：服务发现、服务隔离、服务配置等功能。相比于 Eureka 和 zookeeper，Consul 配置更加一站式，因为它内置了很多微服务常见的需求：服务发现与注册、分布式一致性协议实现、健康检查、键值对存储、多数据中心等，我们不再需要借助第三方组件来实现这些功能。 7.1 安装不同于 Eureka ，Consul 使用 Go 语言开发，所以，使用 Consul ，我们需要先安装软件。 在 Linux 中，首先执行如下命令下载 Consul：1wget https://releases.hashicorp.com/consul/1.6.2/consul_1.6.2_linux_amd64.zip 然后解压下载文件： 1unzip consul_1.6.2_linux_amd64.zip 解压完成后，我们在当前目录下就可以看到 consul 文件，然后执行如下命令，启动 Consul： 1./consul agent -dev -ui -node=consul-dev -client=192.168.91.128 启动成功后，在物理机中，我们可以直接访问 Consul 的后台管理页面（注意，这个访问要确保 8500端口可用，或者直接关闭防火墙）： 在windows下安装下载 https://www.consul.io/downloads 在安装的位置解压得到 consul.exe 文件（我的解压位置是：E:\\consul） 环境变量 增加一条E:\\consul 启动 cmd 命令窗口执行：consul agent -dev consul 自带 UI 界面，打开网址：http://localhost:8500 ，可以看到当前注册的服务界面 cmd 命令窗口执行:consul.exe agent -server ui -bootstrap -client 0.0.0.0 -data-dir=”E:\\consul” -bind X.X.X.X 其中X.X.X.X为服务器ip,即可使用http://X.X.X.X:8500 访问ui而不是只能使用localhost连接 注意：添加环境变量之后有可能无法启动，需要到consul.exe文件夹下面去执行consul agent -dev 7.2 Consul 使用简单看一个注册消费的案例。首先我们来创建一个服务提供者。就是一个普通的 Spring Boot 项目，添加如下依赖： 项目创建成功后，添加如下配置： 123456spring.application.name=consul-providerserver.port=2000# Consul 相关配置spring.cloud.consul.host=192.168.91.128spring.cloud.consul.port=8500spring.cloud.consul.discovery.service-name=consul-provider 在项目启动类上开启服务发现的功能： 1234567@SpringBootApplication@EnableDiscoveryClientpublic class ConsulProviderApplication {public static void main(String[] args) {SpringApplication.run(ConsulProviderApplication.class, args);}} 最后添加一个测试接口： 1234567@RestControllerpublic class HelloController {@GetMapping(\"/hello\")public String hello() {return \"hello\";}} 接下来就是启动项目，项目启动成功后，访问 consul 后台管理页面，看到如下信息，表示 consul 已经注册成功了。 7.3 Consul 集群注册为了区分集群中的哪一个 provider 提供的服务，我们修改一下 consul 中的接口： 123456789@RestControllerpublic class HelloController {@Value(\"${server.port}\")Integer port;@GetMapping(\"/hello\")public String hello() {return \"hello&gt;&gt;\" + port;}} 修改完成后，对项目进行打包。打包成功后，命令行执行如下两行命令，启动两个 provider 实例： 12java -jar consul-provider-0.0.1-SNAPSHOT.jar --server.port=2000java -jar consul-provider-0.0.1-SNAPSHOT.jar --server.port=2001 启动成功后，再去 consul 后台管理页面，就可以看到有两个实例了： 7.4 消费首先创建一个消费实例，创建方式和 provider 一致。创建成功后，添加如下配置： 12345spring.application.name=consul-consumerserver.port=2002spring.cloud.consul.host=192.168.91.128spring.cloud.consul.port=8500spring.cloud.consul.discovery.service-name=consul-consumer 开启服务发现，并添加 RestTemplate： 1234567891011@SpringBootApplication@EnableDiscoveryClientpublic class ConsulConsumerApplication {public static void main(String[] args) {SpringApplication.run(ConsulConsumerApplication.class, args);}@BeanRestTemplate restTemplate() {return new RestTemplate();}} 最后，提供一个服务调用的方法： 12345678910111213141516@RestControllerpublic class HelloController {@AutowiredLoadBalancerClient loadBalancerClient;@AutowiredRestTemplate restTemplate;@GetMapping(\"/hello\")public void hello() {ServiceInstance choose = loadBalancerClient.choose(\"consul-provider\");System.out.println(\"服务地址：\" + choose.getUri());System.out.println(\"服务名称:\" + choose.getServiceId());String s = restTemplate.getForObject(choose.getUri() + \"/hello\",String.class);System.out.println(s);}} 这里，我们通过 loadBalancerClient 实例，可以获取要调用的 ServiceInstance。获取到调用地址之后，再用 RestTemplate 去调用。然后，启动项目，浏览器输入 http://localhost:2002/hello ，查看请求结果，这个请求自带负载均衡功能。 第5章 Hystrix 基本介绍 简单使用/容错/服务降级 请求命令 异常处理 请求缓存 请求合并 8.1 基本介绍Hystrix 叫做断路器/熔断器。微服务系统中，整个系统出错的概率非常高，因为在微服务系统中，涉及到的模块太多了，每一个模块出错，都有可能导致整个服务出，当所有模块都稳定运行时，整个服务才算是稳定运行。我们希望当整个系统中，某一个模块无法正常工作时，能够通过我们提前配置的一些东西，来使得整个系统正常运行，即单个模块出问题，不影响整个系统。 8.2 基本用法首先创建一个新的 SpringBoot 模块，然后添加依赖： 项目创建成功后，添加如下配置，将 Hystrix 注册到 Eureka 上： 123spring.application.name=hystrixserver.port=3000eureka.client.service-url.defaultZone=http://localhost:1111/eureka 然后，在项目启动类上添加如下注解，开启断路器，同时提供一个 RestTemplate 实例： 123456789101112@SpringBootApplication@EnableCircuitBreakerpublic class HystrixApplication {public static void main(String[] args) {SpringApplication.run(HystrixApplication.class, args);}@Bean@LoadBalancedRestTemplate restTemplate() {return new RestTemplate();}} 启动类上的注解，也可以使用 @SpringCloudApplication 代替： 1234567891011@SpringCloudApplicationpublic class HystrixApplication {public static void main(String[] args) {SpringApplication.run(HystrixApplication.class, args);}@Bean@LoadBalancedRestTemplate restTemplate() {return new RestTemplate();}} 这样，Hystrix 的配置就算完成了。接下来提供 Hystrix 的接口。 1234567891011121314151617181920212223242526272829303132333435@Servicepublic class HelloService {@AutowiredRestTemplate restTemplate;/*** 在这个方法中，我们将发起一个远程调用，去调用 provider 中提供的 /hello 接口** 但是，这个调用可能会失败。** 我们在这个方法上添加 @HystrixCommand 注解，配置 fallbackMethod 属性，这个属性表示该方法调用失败时的临时替代方法* @return*/@HystrixCommand(fallbackMethod = \"error\")public String hello() {return restTemplate.getForObject(\"http://provider/hello\", String.class);}/*** 注意，这个方法名字要和 fallbackMethod 一致* 方法返回值也要和对应的方法一致* @return*/public String error() {return \"error\";}}@RestControllerpublic class HelloController {@AutowiredHelloService helloService;@GetMapping(\"/hello\")public String hello() {return helloService.hello();}} 8.3 请求命令请求命令就是以继承类的方式来替代前面的注解方式。我们来自定义一个 HelloCommand： 1234567891011public class HelloCommand extends HystrixCommand&lt;String&gt; {RestTemplate restTemplate;public HelloCommand(Setter setter, RestTemplate restTemplate) {super(setter);this.restTemplate = restTemplate;}@Overrideprotected String run() throws Exception {return restTemplate.getForObject(\"http://provider/hello\", String.class);}} 调用方法： 1234567891011121314151617181920@GetMapping(\"/hello2\")public void hello2() {HelloCommand helloCommand = newHelloCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"javaboy\")), restTemplate);String execute = helloCommand.execute();//直接执行System.out.println(execute);HelloCommand helloCommand2 = newHelloCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"javaboy\")), restTemplate);try {Future&lt;String&gt; queue = helloCommand2.queue();String s = queue.get();System.out.println(s);//先入队，后执行} catch (InterruptedException e) {e.printStackTrace();} catch (ExecutionException e) {e.printStackTrace();}} 注意： 一个实例只能执行一次 可以直接执行，也可以先入队，后执行号外：通过注解实现请求异步调用首先，定义如下方法，返回 Future ： 12345678910@HystrixCommand(fallbackMethod = \"error\")public Future&lt;String&gt; hello2() {return new AsyncResult&lt;String&gt;() {@Overridepublic String invoke() {return restTemplate.getForObject(\"http://provider/hello\",String.class);}};} 然后，调用该方法： 123456789101112@GetMapping(\"/hello3\")public void hello3() {Future&lt;String&gt; hello2 = helloService.hello2();try {String s = hello2.get();System.out.println(s);} catch (InterruptedException e) {e.printStackTrace();} catch (ExecutionException e) {e.printStackTrace();}} 通过继承的方式使用 Hystrix，如何实现服务容错/降级？重写继承类的 getFallback 方法即可： 1234567891011121314151617181920public class HelloCommand extends HystrixCommand&lt;String&gt; {RestTemplate restTemplate;public HelloCommand(Setter setter, RestTemplate restTemplate) {super(setter);this.restTemplate = restTemplate;}@Overrideprotected String run() throws Exception {return restTemplate.getForObject(\"http://provider/hello\", String.class);}/*** 这个方法就是请求失败的回调** @return*/@Overrideprotected String getFallback() {return \"error-extends\";}} 8.4 异常处理就是当发起服务调用时，如果不是 provider 的原因导致请求调用失败，而是 consumer 中本身代码有问题导致的请求失败，即 consumer 中抛出了异常，这个时候，也会自动进行服务降级，只不过这个时候降级，我们还需要知道到底是哪里出异常了。如下示例代码，如果 hello 方法中，执行时抛出异常，那么一样也会进行服务降级，进入到 error 方法中，在 error 方法中，我们可以获取到异常的详细信息： 1234567891011121314151617181920212223242526272829@Servicepublic class HelloService {@AutowiredRestTemplate restTemplate;/*** 在这个方法中，我们将发起一个远程调用，去调用 provider 中提供的 /hello 接口* &lt;p&gt;* 但是，这个调用可能会失败。* &lt;p&gt;* 我们在这个方法上添加 @HystrixCommand 注解，配置 fallbackMethod 属性，这个属性表示该方法调用失败时的临时替代方法** @return*/@HystrixCommand(fallbackMethod = \"error\")public String hello() {int i = 1 / 0;return restTemplate.getForObject(\"http://provider/hello\", String.class);}/*** 注意，这个方法名字要和 fallbackMethod 一致* 方法返回值也要和对应的方法一致** @return*/public String error(Throwable t) {return \"error:\" + t.getMessage();}} 这是注解的方式。也可以通过继承的方式： 123456789101112131415161718192021public class HelloCommand extends HystrixCommand&lt;String&gt; {RestTemplate restTemplate;public HelloCommand(Setter setter, RestTemplate restTemplate) {super(setter);this.restTemplate = restTemplate;}@Overrideprotected String run() throws Exception {int i = 1 / 0;return restTemplate.getForObject(\"http://provider/hello\", String.class);}/*** 这个方法就是请求失败的回调** @return*/@Overrideprotected String getFallback() {return \"error-extends:\"+getExecutionException().getMessage();}} 如果是通过继承的方式来做 Hystrix，在 getFallback 方法中，我们可以通过 getExecutionException 方法来获取执行的异常信息。另一种可能性（作为了解）。如果抛异常了，我们希望异常直接抛出，不要服务降级，那么只需要配置忽略某一个异常即可： 123456@HystrixCommand(fallbackMethod = \"error\",ignoreExceptions =ArithmeticException.class)public String hello() {int i = 1 / 0;return restTemplate.getForObject(\"http://provider/hello\", String.class);} 这个配置表示当 hello 方法抛出 ArithmeticException 异常时，不要进行服务降级，直接将错误抛出。 8.5 请求缓存请求缓存就是在 consumer 中调用同一个接口，如果参数相同，则可以使用之前缓存下来的数据。首先修改 provider 中的 hello2 接口，一会用来检测缓存配置是否生效： 12345@GetMapping(\"/hello2\")public String hello2(String name) {System.out.println(new Date() + \"&gt;&gt;&gt;\" + name);return \"hello \" + name;} 然后，在 hystrix 的请求方法中，添加如下注解： 12345@HystrixCommand(fallbackMethod = \"error2\") @CacheResult//这个注解表示该方法的请求结果会被缓存起来，默认情况下，缓存的 key 就是方法的参 public String hello3(String name) { return restTemplate.getForObject(\"http://provider/hello2?name={1}\", String.class, name); } 这个配置完成后，缓存并不会生效，一般来说，我们使用缓存，都有一个缓存生命周期这样一个概念。这里也一样，我们需要初始化 HystrixRequestContext，初始化完成后，缓存开始生效，HystrixRequestContext close 之后，缓存失效。 1234567@GetMapping(\"/hello4\")public void hello4() {HystrixRequestContext ctx = HystrixRequestContext.initializeContext();String javaboy = helloService.hello3(\"javaboy\");javaboy = helloService.hello3(\"javaboy\");ctx.close();} 想在这个地方执行 不能忘记编写error2 method 123public String error2(String name){ return \"error:lunanboy\"; } 在 ctx close 之前，缓存是有效的，close 之后，缓存就失效了。也就是说，访问一次 hello4 接口，provider 只会被调用一次（第二次使用的缓存），如果再次调用 hello4 接口，之前缓存的数据是失效的。默认情况下，缓存的 key 就是所调用方法的参数，如果参数有多个，就是多个参数组合起来作为缓存的key。例如如下方法： 1234567@HystrixCommand(fallbackMethod = \"error2\")@CacheResult//这个注解表示该方法的请求结果会被缓存起来，默认情况下，缓存的 key 就是方法的参数，缓存的 value 就是方法的返回值。public String hello3(String name,Integer age) {return restTemplate.getForObject(\"http://provider/hello2?name={1}\",String.class, name);} 此时缓存的 key 就是 name+age，但是，如果有多个参数，但是又只想使用其中一个作为缓存的 key，那么我们可以通过 @CacheKey 注解来解决。 1234567@HystrixCommand(fallbackMethod = \"error2\")@CacheResult//这个注解表示该方法的请求结果会被缓存起来，默认情况下，缓存的 key 就是方法的参数，缓存的 value 就是方法的返回值。public String hello3(@CacheKey String name, Integer age) {return restTemplate.getForObject(\"http://provider/hello2?name={1}\",String.class, name);} 上面这个配置，虽然有两个参数，但是缓存时以 name 为准。也就是说，两次请求中，只要 name 一样，即使 age 不一样，第二次请求也可以使用第一次请求缓存的结果。另外还有一个注解叫做 @CacheRemove()。在做数据缓存时，如果有一个数据删除的方法，我们一般除了删除数据库中的数据，还希望能够顺带删除缓存中的数据，这个时候 @CacheRemove() 就派上用场了。@CacheRemove() 在使用时，必须指定 commandKey 属性，commandKey 其实就是缓存方法的名字，指定了 commandKey，@CacheRemove 才能找到数据缓存在哪里了，进而才能成功删除掉数据。例如如下方法定义缓存与删除缓存： 1234567891011@HystrixCommand(fallbackMethod = \"error2\")@CacheResult//这个注解表示该方法的请求结果会被缓存起来，默认情况下，缓存的 key 就是方法的参数，缓存的 value 就是方法的返回值。public String hello3(String name) {return restTemplate.getForObject(\"http://provider/hello2?name={1}\",String.class, name);}@HystrixCommand@CacheRemove(commandKey = \"hello3\")public String deleteUserByName(String name) {return null;} 再去调用： 1234567891011@GetMapping(\"/hello4\")public void hello4() {HystrixRequestContext ctx = HystrixRequestContext.initializeContext();//第一请求完，数据已经缓存下来了String javaboy = helloService.hello3(\"javaboy\");//删除数据，同时缓存中的数据也会被删除helloService.deleteUserByName(\"javaboy\");//第二次请求时，虽然参数还是 javaboy，但是缓存数据已经没了，所以这一次，provider 还是会收到请求javaboy = helloService.hello3(\"javaboy\");ctx.close();} 如果是继承的方式使用 Hystrix ，只需要重写 getCacheKey 方法即可： 123456789101112131415161718192021222324252627public class HelloCommand extends HystrixCommand&lt;String&gt; {RestTemplate restTemplate;String name;public HelloCommand(Setter setter, RestTemplate restTemplate,String name) {super(setter);this.name = name;this.restTemplate = restTemplate;}@Overrideprotected String run() throws Exception {return restTemplate.getForObject(\"http://provider/hello2?name={1}\",String.class, name);}@Overrideprotected String getCacheKey() {return name;}/*** 这个方法就是请求失败的回调** @return*/@Overrideprotected String getFallback() {return \"error-extends:\"+getExecutionException().getMessage();}} 调用时候，一定记得初始化 HystrixRequestContext: 12345678910111213141516171819202122@GetMapping(\"/hello2\")public void hello2() {HystrixRequestContext ctx = HystrixRequestContext.initializeContext();HelloCommand helloCommand = newHelloCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"javaboy\")), restTemplate,\"javaboy\");String execute = helloCommand.execute();//直接执行System.out.println(execute);HelloCommand helloCommand2 = newHelloCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"javaboy\")), restTemplate,\"javaboy\");try {Future&lt;String&gt; queue = helloCommand2.queue();String s = queue.get();System.out.println(s);//先入队，后执行} catch (InterruptedException e) {e.printStackTrace();} catch (ExecutionException e) {e.printStackTrace();}ctx.close();} 8.6 请求合并如果 consumer 中，频繁的调用 provider 中的同一个接口，在调用时，只是参数不一样，那么这样情况下，我们就可以将多个请求合并成一个，这样可以有效提高请求发送的效率。首先我们在 provider 中提供一个请求合并的接口： 1234567891011121314@RestControllerpublic class UserController {@GetMapping(\"/user/{ids}\")//假设 consumer 传过来的多个 id 的格式是 1,2,3,4....public List&lt;User&gt; getUserByIds(@PathVariable String ids) {String[] split = ids.split(\",\");List&lt;User&gt; users = new ArrayList&lt;&gt;();for (String s : split) {User u = new User();u.setId(Integer.parseInt(s));users.add(u);}return users;}} 这个接口既可以处理合并之后的请求，也可以处理单个请求（单个请求的话，List 集合中就只有一项数据。）然后，在 Hystrix 中，定义 UserService： 12345678910@Servicepublic class UserService {@AutowiredRestTemplate restTemplate;public List&lt;User&gt; getUsersByIds(List&lt;Integer&gt; ids) {User[] users = restTemplate.getForObject(\"http://provider/user/{1}\",User[].class, StringUtils.join(ids, \",\"));return Arrays.asList(users);}} 接下来定义 UserBatchCommand ，相当于我们之前的 HelloCommand： 1234567891011121314public class UserBatchCommand extends HystrixCommand&lt;List&lt;User&gt;&gt; {private List&lt;Integer&gt; ids;private UserService userService;public UserBatchCommand(List&lt;Integer&gt; ids, UserService userService) {super(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"batchCmd\")).andCommandKey(HystrixCommandKey.Factory.asKey(\"batchKey\")));this.ids = ids;this.userService = userService;}@Overrideprotected List&lt;User&gt; run() throws Exception {return userService.getUsersByIds(ids);}} 最后，定义最最关键的请求合并方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UserCollapseCommand extends HystrixCollapser&lt;List&lt;User&gt;, User, Integer&gt; { private UserService userService; private Integer id; public UserCollapseCommand(UserService userService, Integer id) { super(HystrixCollapser.Setter.withCollapserKey(HystrixCollapserKey.Factory.asKey(\"UserCollapseCommand\")).andCollapserPropertiesDefaults(HystrixCollapserProperties.Setter().withTimerDelayInMilliseconds(200))); this.userService = userService; this.id = id; } /** * 请求参数 * * @return */ @Override public Integer getRequestArgument() { return id; } /** * 请求合并的方法 * * @param collection * @return */ @Override protected HystrixCommand&lt;List&lt;User&gt;&gt; createCommand(Collection&lt;CollapsedRequest&lt;User, Integer&gt;&gt; collection) { List&lt;Integer&gt; ids = new ArrayList&lt;&gt;(collection.size()); for (CollapsedRequest&lt;User, Integer&gt; userIntegerCollapsedRequest : collection) { ids.add(userIntegerCollapsedRequest.getArgument()); } return new UserBatchCommand(ids, userService); } /** * 请求结果转发 * * @param users * @param collection */ @Override protected void mapResponseToRequests(List&lt;User&gt; users, Collection&lt;CollapsedRequest&lt;User, Integer&gt;&gt; collection) { int count = 0; for (CollapsedRequest&lt;User, Integer&gt; userIntegerCollapsedRequest : collection) { userIntegerCollapsedRequest.setResponse(users.get(count++)); } }} 最后就是测试调用： 12345678910111213141516171819202122@GetMapping(\"/hello5\") public void hello5() throws ExecutionException, InterruptedException { HystrixRequestContext ctx = HystrixRequestContext.initializeContext(); UserCollapseCommand cmd1 = new UserCollapseCommand(userService, 99); UserCollapseCommand cmd2 = new UserCollapseCommand(userService, 98); UserCollapseCommand cmd3 = new UserCollapseCommand(userService, 97); Future&lt;User&gt; q1 = cmd1.queue(); Future&lt;User&gt; q2 = cmd2.queue(); Future&lt;User&gt; q3 = cmd3.queue(); User u1 = q1.get(); User u2 = q2.get(); User u3 = q3.get(); System.out.println(u1); System.out.println(u2); System.out.println(u3); Thread.sleep(1000); UserCollapseCommand cmd4 = new UserCollapseCommand(userService, 96); Future&lt;User&gt; q4 = cmd4.queue(); User u4 = q4.get(); System.out.println(u4); ctx.close(); } 通过注解实现请求合并 123456789101112131415161718@Servicepublic class UserService { @Autowired RestTemplate restTemplate; @HystrixCollapser(batchMethod = \"getUsersByIds\", collapserProperties = {@HystrixProperty(name = \"timerDelayInMilliseconds\", value = \"200\")}) public Future&lt;User&gt; getUserById(Integer id) { return null; } @HystrixCommand public List&lt;User&gt; getUsersByIds(List&lt;Integer&gt; ids) { User[] users = restTemplate.getForObject(\"http://provider/user/{1}\", User[].class, StringUtils.join(ids, \",\")); return Arrays.asList(users); }} 这里的核心是 @HystrixCollapser 注解。在这个注解中，指定批处理的方法即可。测试代码如下： 123456789101112131415161718@GetMapping(\"/hello6\")public void hello6() throws ExecutionException, InterruptedException { HystrixRequestContext ctx = HystrixRequestContext.initializeContext(); Future&lt;User&gt; q1 = userService.getUserById(99); Future&lt;User&gt; q2 = userService.getUserById(98); Future&lt;User&gt; q3 = userService.getUserById(97); User u1 = q1.get(); User u2 = q2.get(); User u3 = q3.get(); System.out.println(u1); System.out.println(u2); System.out.println(u3); Thread.sleep(2000); Future&lt;User&gt; q4 = userService.getUserById(96); User u4 = q4.get(); System.out.println(u4); ctx.close();} 第6章 OpenFeign9.1 OpenFeign前面无论是基本调用，还是 Hystrix，我们实际上都是通过手动调用 RestTemplate 来实现远程调用的。使用 RestTemplate 存在一个问题：繁琐，每一个请求，参数不同，请求地址不同，返回数据类型不同，其他都是一样的，所以我们希望能够对请求进行简化。我们希望对请求进行简化，简化方案就是 OpenFeign。一开始这个组件不叫这个名字，一开始就叫 Feign，Netflix Feign，但是 Netflix 中的组件现在已经停止开源工作，OpenFeign 是 Spring Cloud 团队在 Netflix Feign 的基础上开发出来的声明式服务调用组件。关于 OpenFeign 组件的 Issue：https://github.com/OpenFeign/feign/issues/373 9.1.1 HelloWorld继续使用之前的 Provider。新建一个 Spring Boot 模块，创建时，选择 OpenFeign 依赖，如下： 项目创建成功后，在 application.properties 中进行配置，使项目注册到 Eureka 上： 123spring.application.name=openfeignserver.port=4000eureka.client.service-url.defaultZone=http://localhost:1111/eureka 接下来在启动类上添加注解，开启 Feign 的支持： 1234567@SpringBootApplication@EnableFeignClientspublic class OpenfeignApplication {public static void main(String[] args) {SpringApplication.run(OpenfeignApplication.class, args);}} 接下来，定义 HelloService 接口，去使用 OpenFeign： 123456@FeignClient(\"provider\")public interface HelloService { //这里的方法名无所谓，随意取 @GetMapping(\"/hello\") String hello();} 最后调用 HelloController 中，调用 HelloService 进行测试： 123456789@RestControllerpublic class HelloController {@AutowiredHelloService helloService;@GetMapping(\"/hello\")public String hello() {return helloService.hello();}} 接下来，启动 OpenFeign 项目，进行测试。 9.2 参数传递和普通参数传递的区别： 参数一定要绑定参数名。 如果通过 header 来传递参数，一定记得中文要转码。测试的服务端接口，继续使用 provider 提供的接口。这里，我们主要在 openfeign 中添加调用接口即可： 12345678910111213@FeignClient(\"provider\")public interface HelloService {@GetMapping(\"/hello\")String hello();//这里的方法名无所谓，随意取@GetMapping(\"/hello2\")String hello2(@RequestParam(\"name\") String name);@PostMapping(\"/user2\")User addUser(@RequestBody User user);@DeleteMapping(\"/user2/{id}\")void deleteUserById(@PathVariable(\"id\") Integer id);@GetMapping(\"/user3\")void getUserByName(@RequestHeader(\"name\") String name);} 注意，凡是 key/value 形式的参数，一定要标记参数的名称。 HelloController 中调用 HelloService： 1234567891011121314@GetMapping(\"/hello\")public String hello() throws UnsupportedEncodingException {String s = helloService.hello2(\"江南一点雨\");System.out.println(s);User user = new User();user.setId(1);user.setUsername(\"javaboy\");user.setPassword(\"123\");User u = helloService.addUser(user);System.out.println(u);helloService.deleteUserById(1);helloService.getUserByName(URLEncoder.encode(\"江南一点雨\", \"UTF-8\"));return helloService.hello();} 注意：放在 header 中的中文参数，一定要编码之后传递。 9.3 继承特性将 provider 和 openfeign 中公共的部分提取出来，一起使用。我们新建一个 Module，叫做 hello-api，注意，由于这个模块要被其他模块所依赖，所以这个模块是一个 Maven 项目，但是由于这个模块要用到 SpringMVC 的东西，因此在创建成功后，给这个模块添加一个 web 依赖，导入 SpringMVC 需要的一套东西。项目创建成功后，首先添加依赖： 12345678910&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;version&gt;2.2.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.javaboy&lt;/groupId&gt;&lt;artifactId&gt;commons&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 然后定义公共接口，就是provider 和 openfeign 中公共的部分： 123456789101112public interface IUserService {@GetMapping(\"/hello\")String hello();//这里的方法名无所谓，随意取@GetMapping(\"/hello2\")String hello2(@RequestParam(\"name\") String name);@PostMapping(\"/user2\")User addUser(@RequestBody User user);@DeleteMapping(\"/user2/{id}\")void deleteUserById(@PathVariable(\"id\") Integer id);@GetMapping(\"/user3\")void getUserByName(@RequestHeader(\"name\") String name);} 定义完成后，接下来，在 provider 和 openfeign 中，分别引用该模块： 12345&lt;dependency&gt;&lt;groupId&gt;org.javaboy&lt;/groupId&gt;&lt;artifactId&gt;hello-api&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 添加成功之后，在 provider 中实现该接口： 12345678910111213141516171819202122232425262728293031323334353637383940414243@RestControllerpublic class HelloController implements IUserService {@Value(\"${server.port}\")Integer port;@Overridepublic String hello() {return \"hello javaboy:\" + port;}@Overridepublic String hello2(String name) {System.out.println(new Date() + \"&gt;&gt;&gt;\" + name);return \"hello \" + name;}@PostMapping(\"/user1\")public User addUser1(User user) {return user;}@Overridepublic User addUser2(@RequestBody User user) {return user;}@PutMapping(\"/user1\")public void updateUser1(User user) {System.out.println(user);}@PutMapping(\"/user2\")public void updateUser2(@RequestBody User user) {System.out.println(user);}@DeleteMapping(\"/user1\")public void deleteUser1(Integer id) {System.out.println(id);}@Overridepublic void deleteUser2(@PathVariable Integer id) {System.out.println(id);}@Overridepublic void getUserByName(@RequestHeader String name) throwsUnsupportedEncodingException {System.out.println(URLDecoder.decode(name, \"UTF-8\"));}} 在 openfeign 中，定义接口继承自公共接口： 123@FeignClient(\"provider\")public interface HelloService extends IUserService {} 接下来，测试代码不变。关于继承特性： 使用继承特性，代码简洁明了不易出错。服务端和消费端的代码统一，一改俱改，不易出错。这是优点也是缺点，这样会提高服务端和消费端的耦合度。 9.2 中所讲的参数传递，在使用了继承之后，依然不变，参数该怎么传还是怎么传。 9.4 日志OpenFeign 中，我们可以通过配置日志，来查看整个请求的调用过程。日志级别一共分为四种： NONE：不开启日志，默认就是这个 BASIC：记录请求方法、URL、响应状态码、执行时间 HEADERS：在 BASIC 的基础上，加载请求/响应头 FULL：在 HEADERS 基础上，再增加 body 以及请求元数据。四种级别，可以通过 Bean 来配置： 1234567891011@SpringBootApplication@EnableFeignClientspublic class OpenfeignApplication {public static void main(String[] args) {SpringApplication.run(OpenfeignApplication.class, args);}@BeanLogger.Level loggerLevel() {return Logger.Level.FULL;}} 最后，在 application.properties 中开启日志级别： 1logging.level.org.javaboy.openfeign.HelloService=debug 重启 OpenFeign，进行测试。 9.5 数据压缩12345678# 开启请求的数据压缩feign.compression.request.enabled=true# 开启响应的数据压缩feign.compression.response.enabled=true# 压缩的数据类型feign.compression.request.mime-types=text/html,application/json# 压缩的数据下限，2048 表示当要传输的数据大于 2048 时，才会进行数据压缩feign.compression.request.min-request-size=2048 9.6 +HystrixHystrix 中的容错、服务降级等功能，在 OpenFeign 中一样要使用。首先定义服务降级的方法： 12345678910111213141516171819202122@Component@RequestMapping(\"/javaboy\")//防止请求地址重复public class HelloServiceFallback implements HelloService {@Overridepublic String hello() {return \"error\";}@Overridepublic String hello2(String name) {return \"error2\";}@Overridepublic User addUser2(User user) {return null;}@Overridepublic void deleteUser2(Integer id) {}@Overridepublic void getUserByName(String name) throws UnsupportedEncodingException {}} 然后，在 HelloService 中配置这个服务降级类： 123@FeignClient(value = \"provider\",fallback = HelloServiceFallback.class)public interface HelloService extends IUserService {} 最后，在 application.properties 中开启 Hystrix。 1feign.hystrix.enabled=true 也可以通过自定义 FallbackFactory 来实现请求降级： 12345678910111213141516171819202122232425262728@Componentpublic class HelloServiceFallbackFactory implementsFallbackFactory&lt;HelloService&gt; {@Overridepublic HelloService create(Throwable throwable) {return new HelloService() {@Overridepublic String hello() {return \"error---\";}@Overridepublic String hello2(String name) {return \"error2---\";}@Overridepublic User addUser2(User user) {return null;}@Overridepublic void deleteUser2(Integer id) {}@Overridepublic void getUserByName(String name) throwsUnsupportedEncodingException {}};}} HelloService 中进行配置： 1234@FeignClient(value = \"provider\",fallbackFactory =HelloServiceFallbackFactory.class)public interface HelloService extends IUserService {} 第7章 Resilience4j10.1 Resilience4j 简介Resilience4j 是 Spring Cloud Greenwich 版推荐的容错解决方案，相比 Hystrix，Resilience4j 专为Java8 以及函数式编程而设计。Resilience4j 主要提供了如下功能： 断路器 限流 基于信号量的隔离 缓存 限时 请求重试 10.2 基本用法首先搭建一个简单的测试环境。 12345&lt;dependency&gt;&lt;groupId&gt;junit&lt;/groupId&gt;&lt;artifactId&gt;junit&lt;/artifactId&gt;&lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; 10.2.1 断路器Resilience4j 提供了很多功能，不同的功能对应不同的依赖，可以按需添加。使用断路器，则首先添加断路器的依赖： 12345&lt;dependency&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-circuitbreaker&lt;/artifactId&gt;&lt;version&gt;0.13.2&lt;/version&gt;&lt;/dependency&gt; 一个正常执行的例子： 123456789101112131415161718192021222324@Testpublic void test1() {//获取一个CircuitBreakerRegistry实例，可以调用ofDefaults获取一个CircuitBreakerRegistry实例，也可以自定义属性。CircuitBreakerRegistry registry = CircuitBreakerRegistry.ofDefaults();CircuitBreakerConfig config = CircuitBreakerConfig.custom()//故障率阈值百分比，超过这个阈值，断路器就会打开.failureRateThreshold(50)//断路器保持打开的时间，在到达设置的时间之后，断路器会进入到 half open 状态.waitDurationInOpenState(Duration.ofMillis(1000))//当断路器处于half open 状态时，环形缓冲区的大小.ringBufferSizeInHalfOpenState(2).ringBufferSizeInClosedState(2).build();CircuitBreakerRegistry r1 = CircuitBreakerRegistry.of(config);CircuitBreaker cb1 = r1.circuitBreaker(\"javaboy\");CircuitBreaker cb2 = r1.circuitBreaker(\"javaboy2\", config);CheckedFunction0&lt;String&gt; supplier =CircuitBreaker.decorateCheckedSupplier(cb1, () -&gt; \"hello resilience4\";Try&lt;String&gt; result = Try.of(supplier).map(v -&gt; v + \" hello world\");System.out.println(result.isSuccess());System.out.println(result.get());} 一个出异常的断路器： 123456789101112131415161718192021222324@Testpublic void test2() {CircuitBreakerConfig config = CircuitBreakerConfig.custom()//故障率阈值百分比，超过这个阈值，断路器就会打开.failureRateThreshold(50)//断路器保持打开的时间，在到达设置的时间之后，断路器会进入到 half open 状态.waitDurationInOpenState(Duration.ofMillis(1000))//当断路器处于half open 状态时，环形缓冲区的大小.ringBufferSizeInClosedState(2).build();CircuitBreakerRegistry r1 = CircuitBreakerRegistry.of(config);CircuitBreaker cb1 = r1.circuitBreaker(\"javaboy\");System.out.println(cb1.getState());//获取断路器的一个状态cb1.onError(0, new RuntimeException());System.out.println(cb1.getState());//获取断路器的一个状态cb1.onError(0, new RuntimeException());System.out.println(cb1.getState());//获取断路器的一个状态CheckedFunction0&lt;String&gt; supplier =CircuitBreaker.decorateCheckedSupplier(cb1, () -&gt; \"hello resilience4j\");Try&lt;String&gt; result = Try.of(supplier).map(v -&gt; v + \" hello world\");System.out.println(result.isSuccess());System.out.println(result.get());} 注意，由于 ringBufferSizeInClosedState 的值为 2，表示当有两条数据时才会去统计故障率，所以，下面的手动故障测试，至少调用两次 onError ，断路器才会打开。 10.2.2 限流RateLimiter 本身和前面的断路器很像。首先添加依赖： 12345&lt;dependency&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-ratelimiter&lt;/artifactId&gt;&lt;version&gt;0.13.2&lt;/version&gt;&lt;/dependency&gt; 限流测试： 123456789101112131415161718@Testpublic void test3() {RateLimiterConfig config = RateLimiterConfig.custom().limitRefreshPeriod(Duration.ofMillis(1000)).limitForPeriod(4).timeoutDuration(Duration.ofMillis(1000)).build();RateLimiter rateLimiter = RateLimiter.of(\"javaboy\", config);CheckedRunnable checkedRunnable =RateLimiter.decorateCheckedRunnable(rateLimiter, () -&gt; {System.out.println(new Date());});Try.run(checkedRunnable).andThenTry(checkedRunnable).andThenTry(checkedRunnable).andThenTry(checkedRunnable).onFailure(t -&gt; System.out.println(t.getMessage()));} 10.2.3 请求重试首先第一步还是加依赖： 12345&lt;dependency&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-retry&lt;/artifactId&gt;&lt;version&gt;0.13.2&lt;/version&gt;&lt;/dependency&gt; 案例： 12345678910111213141516171819202122@Testpublic void test4() {RetryConfig config = RetryConfig.custom()//重试次数.maxAttempts(2)//重试间隔.waitDuration(Duration.ofMillis(500))//重试异常.retryExceptions(RuntimeException.class).build();Retry retry = Retry.of(\"javaboy\", config);Retry.decorateRunnable(retry, new Runnable() {int count = 0;//开启了重试功能之后，run 方法执行时，如果抛出异常，会自动触发重试功能@Overridepublic void run() {if (count++ &lt; 3) {throw new RuntimeException();}}}).run();} 10.3 结合微服务Retry、CircuitBreaker、RateLimiter 10.3.1 Retry首先创建一个 Spring Boot 项目，创建时，添加 eureka-client 依赖，使之能够注册到 eureka 上。 项目创建成功后，手动添加 Resilience4j 依赖： 1234567891011121314151617181920212223&lt;dependency&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-spring-boot2&lt;/artifactId&gt;&lt;version&gt;1.3.1&lt;/version&gt;&lt;exclusions&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-circuitbreaker&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-ratelimiter&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-bulkhead&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-timelimiter&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;/exclusions&gt;&lt;/dependency&gt; resilience4j-spring-boot2 中包含了 Resilience4j 的所有功能，但是没有配置的功能无法使用，需要将之从依赖中剔除掉。接下来，在 application.yml 中配置 retry： 1234567891011121314151617181920212223resilience4j: retry: # 表示Retry的优先级 retry-aspect-order: 399 backends: retryA: # 重试次数 maxRetryAttempts: 5 # 重试等待时间 waitDuration: 500 # 间隔乘数 exponentialBackoffMultiplier: 1.1 retryExceptions: - java.lang.RuntimeExceptionspring: application: name: resilience4jserver: port: 5000eureka: client: service-url: defaultZone: http://localhost:1111/eureka 最后，创建测试 RestTemplate 和 HelloService： 1234567891011121314151617181920212223242526272829@SpringBootApplicationpublic class Resilience4j2Application {public static void main(String[] args) {SpringApplication.run(Resilience4j2Application.class, args);}@BeanRestTemplate restTemplate() {return new RestTemplate();}}@Service@Retry(name = \"retryA\")//表示要使用的重试策略public class HelloService {@AutowiredRestTemplate restTemplate;public String hello() {return restTemplate.getForObject(\"http://localhost:1113/hello\",String.class);}}@RestControllerpublic class HelloController {@AutowiredHelloService helloService;@GetMapping(\"/hello\")public String hello() {return helloService.hello();}} 10.3.2 CircuitBreaker首先从依赖中删除排除 CircuitBreaker。然后在 application.yml 中进行配置： 12345678910111213141516171819resilience4j: retry: retry-aspect-order: 399 # 表示Retry的优先级 backends:retryA:maxRetryAttempts: 5 # 重试次数waitDuration: 500 # 重试等待时间exponentialBackoffMultiplier: 1.1 # 间隔乘数retryExceptions:- java.lang.RuntimeExceptioncircuitbreaker:instances:cbA:ringBufferSizeInClosedState: 5ringBufferSizeInHalfOpenState: 3waitInterval: 5000recordExceptions:- org.springframework.web.client.HttpServerErrorExceptioncircuit-breaker-aspect-order: 398 配置完成后，用 @CircuitBreakder 注解标记相关方法： 12345678910111213@Service@CircuitBreaker(name = \"cbA\", fallbackMethod = \"error\")public class HelloService {@AutowiredRestTemplate restTemplate;public String hello() {return restTemplate.getForObject(\"http://localhost:1113/hello\",String.class);}public String error(Throwable t) {return \"error\";}} @CircuitBreaker 注解中的 name 属性用来指定 circuitbreaker 配置，fallbackMethod 属性用来指定服务降级的方法，需要注意的是，服务降级方法中，要添加异常参数。 10.3.3 RateLimiterRateLimiter 作为限流工具，主要在服务端使用，用来保护服务端的接口。首先在 provider 中添加 RateLimiter 依赖： 12345678910111213141516171819&lt;dependency&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-spring-boot2&lt;/artifactId&gt;&lt;version&gt;1.2.0&lt;/version&gt;&lt;exclusions&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-circuitbreaker&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-bulkhead&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;exclusion&gt;&lt;groupId&gt;io.github.resilience4j&lt;/groupId&gt;&lt;artifactId&gt;resilience4j-timelimiter&lt;/artifactId&gt;&lt;/exclusion&gt;&lt;/exclusions&gt;&lt;/dependency&gt; 接下来，在 provider 的 application.properties 配置文件中，去配置 RateLimiter： 1234# 这里配置每秒钟处理一个请求resilience4j.ratelimiter.limiters.rlA.limit-for-period=1resilience4j.ratelimiter.limiters.rlA.limit-refresh-period=1sresilience4j.ratelimiter.limiters.rlA.timeout-duration=1s 为了查看请求效果，在 provider 的 HelloController 中打印每一个请求的时间： 1234567@Override@RateLimiter(name = \"rlA\")public String hello() {String s = \"hello javaboy:\" + port;System.out.println(new Date());return s;} 这里通过 @RateLimiter 注解来标记该接口限流。配置完成后，重启 provider。然后，在客户端模拟多个请求，查看限流效果： 123456public String hello() {for (int i = 0; i &lt; 5; i++) {restTemplate.getForObject(\"http://localhost:1113/hello\", String.class);}return \"success\";} 10.4 服务监控微服务由于服务数量众多，所以出故障的概率很大，这种时候不能单纯的依靠人肉运维。早期的 Spring Cloud 中，服务监控主要使用 Hystrix Dashboard，集群数据库监控使用 Turbine。在 Greenwich 版本中，官方建议监控工具使用 Micrometer。Micrometer： 提供了度量指标，例如 timers、counters 一揽子开箱即用的解决方案，例如缓存、类加载器、垃圾收集等等新建一个 Spring Boot 项目，添加 Actuator 依赖。项目创建成功后，添加如下配置，开启所有端点： 1management.endpoints.web.exposure.include=* 然后就可以在浏览器查看项目的各项运行数据，但是这些数据都是 JSON 格式。 我们需要一个可视化工具来展示这些 JSON 数据。这里主要和大家介绍 Prometheus。 10.4.1 Prometheus安装 1234wgethttps://github.com/prometheus/prometheus/releases/download/v2.16.0/prometheus-2.16.0.linux-amd64.tar.gztar -zxvf prometheus-2.16.0.linux-amd64.tar.gz 解压完成后，配置一下数据路径和要监控的服务地址： 12cd prometheus-2.16.0.linux-amd64/vi prometheus.yml 修改 prometheus.yml 配置文件，主要改两个地方，一个是数据接口，另一个是服务地址： 接下来，将 Prometheus 整合到 Spring Boot 项目中。首先加依赖： 1234&lt;dependency&gt;&lt;groupId&gt;io.micrometer&lt;/groupId&gt;&lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt; 然后在 application.properties 配置中，添加 Prometheus 配置： 1234management.endpoints.web.exposure.include=*management.endpoint.prometheus.enabled=truemanagement.metrics.export.prometheus.enabled=truemanagement.endpoint.metrics.enabled=true 接下来启动 Prometheus。启动命令： 1./prometheus --config.file=prometheus.yml 启动成功后，浏览器输入 http://192.168.91.128:9090 查看 Prometheus 数据信息。Grafana：https://grafana.com/grafana/download?platform=linux 第8章 Zuul11.1 服务网关Zuul 和 Gateway由于每一个微服务的地址都有可能发生变化，无法直接对外公布这些服务地址，基于安全以及高内聚低耦合等设计，我们有必要将内部系统和外部系统做一个切割。一个专门用来处理外部请求的组件，就是服务网关。 权限问题统一处理 数据剪裁和聚合 简化客户端的调用 可以针对不同的客户端提供不同的网关支持 Spring Cloud 中，网关主要有两种实现方案： Zuul Spring Cloud Gateway 11.2 ZuulZuul 是 Netflix 公司提供的网关服务。Zuul 的功能： 权限控制，可以做认证和授权 监控 动态路由 负载均衡 静态资源处理 Zuul 中的功能基本上都是基于过滤器来实现，它的过滤器有几种不同的类型： PRE ROUTING POST ERROR 11.2.1 HelloWorld首先创建项目，添加 Zuul 依赖。 项目创建成功后，将 zuul 注册到 eureka 上： 123spring.application.name=zuulserver.port=2020eureka.client.service-url.defaultZone=http://localhost:1111/eureka 然后在启动类上开启网关代理： 1234567@SpringBootApplication@EnableZuulProxy//开启网关代理public class ZuulApplication {public static void main(String[] args) {SpringApplication.run(ZuulApplication.class, args);}} 配置完成后，重启 Zuul，接下来，在浏览器中，通过 Zuul 的代理就可以访问到 provider 了。http://localhost:2020/provider/hello在这个访问地址中，provider 就是要访问的服务名称，/hello 则是要访问的服务接口。这是一个简单例子，Zuul 中的路由规则也可以自己配置。 12zuul.routes.javaboy-a.path=/javaboy-a/**zuul.routes.javaboy-a.service-id=provider 上面这个配置，表示 /javaboy-a/** ，满足这个匹配规则的请求，将被转发到 provider 实例上。上面两行配置，也可以进行简化： 1zuul.routes.provider=/javaboy-a/** 11.2.2 请求过滤对于来自客户端的请求，可以在 Zuul 中进行预处理，例如权限判断等。定义一个简单的权限过滤器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Componentpublic class PermissFilter extends ZuulFilter { /** * 过滤器类型，权限判断一般是pre * * @return */ @Override public String filterType() { return \"pre\"; } /** * 过滤器优先级 * * @return */ @Override public int filterOrder() { return 0; } /** * 是否过滤 * * @return */ @Override public boolean shouldFilter() { return false; } /** * 核心的过滤逻辑写在这里 * * @return 这个方法虽然有返回值，但是这个返回值目前无所谓 * @throws ZuulException */ @Override public Object run() throws ZuulException { RequestContext ctx = RequestContext.getCurrentContext(); //获取当前请求 HttpServletRequest request = ctx.getRequest(); String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); if (!\"lunanboy\".equals(username) || !\"123\".equals(password)) { //如果请求不满足,直接在这里给出响应 ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); ctx.addZuulResponseHeader(\"content-type\", \"text/html;charset=utf-8\"); ctx.setResponseBody(\"非法访问\"); } return null; }} 重启 Zuul，接下来，发送请求必须带上 username 和 password 参数，否则请求不通过。 http://localhost:2020/javaboy-a/hello?username=javaboy&amp;password=123 11.2.3 Zuul 中的其他配置匹配规则 例如有两个服务，一个叫 consumer，另一个叫 consumer-hello，在做路由规则设置时，假如出现了如下配置： 12zuul.routes.consumer=/consumer/**zuul.routes.consumer-hello=/consumer/hello/** 此时，如果访问一个地址：http://localhost:2020/consumer/hello/123，会出现冲突。实际上，这个地址是希望和 consumer-hello 这个服务匹配的，这个时候，只需要把配置文件改为 yml 格式就可以了。 忽略路径 默认情况下，zuul 注册到 eureka 上之后，eureka 上的所有注册服务都会被自动代理。如果不想给某一个服务做代理，可以忽略该服务，配置如下： 1zuul.ignored-services=provider 上面这个配置表示忽略 provider 服务，此时就不会自动代理 provider 服务了。也可以忽略某一类地址： 1zuul.ignored-patterns=/**/hello/** 这个表示请求路径中如果包含 hello，则不做代理。 前缀也可以给路由加前缀。 1zuul.prefix=/javaboy 这样，以后所有的请求地址自动多了前缀，/javaboy 第9章 Spring Cloud Gateway12.1 简介特点： 限流路径重写动态路由集成 Spring Cloud DiscoveryClient集成 Hystrix 断路器 和 Zuul 对比： Zuul 是 Netflix 公司的开源产品，Spring Cloud Gateway 是 Spring 家族中的产品，可以和Spring 家族中的其他组件更好的融合。 Zuul1 不支持长连接，例如 websocket。 Spring Cloud Gateway 支持限流。 Spring Cloud Gateway 基于 Netty 来开发，实现了异步和非阻塞，占用资源更小，性能强于Zuul。 12.2 基本用法Spring Cloud Gateway 支持两种不同的用法： 编码式yml 配置 两种都来看下。 编码式首先创建 Spring Boot 项目，添加 Spring Cloud Gateway 模块： 项目创建成功后，直接配置一个 RouteLocator 这样一个 Bean，就可以实现请求转发。 1234567@BeanRouteLocator routeLocator(RouteLocatorBuilder builder) {return builder.routes().route(\"javaboy_route\", r -&gt;r.path(\"/get\").uri(\"http://httpbin.org\")).build();} 这里只需要提供 RouteLocator 这个 Bean，就可以实现请求转发。配置完成后，重启项目，访问： http://localhost:8080/get properties 配置 123spring.cloud.gateway.routes[0].id=javaboy_routespring.cloud.gateway.routes[0].uri=http://httpbin.orgspring.cloud.gateway.routes[0].predicates[0]=Path=/get YML 配置 12345678spring: cloud: gateway: routes: - id: lunanboy_route url: http://httpbin.org predicate: - Path=/get 12.2.1 服务化首先给 Gateway 添加依赖，将之注册到 Eureka 上。加依赖： 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 加配置： 1234567891011121314spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- Path=/getapplication:name: gatewayeureka:client:service-url:defaultZone: http://localhost:1111/eureka 配置路由转发： 1234567891011121314151617181920spring: cloud: gateway:# routes: # - id: lunanboy_route# url: http://httpbin.org# predicate: # - Path=/get discovery: locator: enabled: true # 开启自动代理 application: name: gatewayeureka: client: service-url: defaultZone: http://localhost:1111/eurekalogging: level: org.springframework.cloud.gateway: debug 为什么provider要大写 且需要注释掉routes里面的配置内容 接下来，就可以通过 Gateway 访问到其他注册在 Eureka 上的服务了，访问方式和 Zuul 一样。 12.3 Predicate通过时间匹配： 12345678spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- After=2021-01-01T01:01:01+08:00[Asia/Shanghai] 表示，请求时间在 2021-01-01T01:01:01+08:00[Asia/Shanghai] 时间之后，才会被路由。除了 After 之外，还有两个关键字： Before，表示在某个时间点之前进行请求转发 Between，表示在两个时间点之间，两个时间点用 , 隔开 也可以通过请求方式匹配，就是请求方法： 12345678spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- Method=GET 这个配置表示只给 GET 请求进行路由。通过请求路径匹配： 12345678spring:cloud:gateway:routes:- id: javaboy_routeuri: http://www.javaboy.orgpredicates:- Path=/2019/0612/{segment} 表示路径满足 /2019/0612/ 这个规则，都会被进行转发，例如： 通过参数进行匹配： 12345678spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- Query=name 表示请求中一定要有 name 参数才会进行转发，否则不会进行转发。也可以指定参数和参数的值。例如参数的 key 为 name，value 必须要以 java 开始： 12345678spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- Query=name,java.* 多种匹配方式也可以组合使用。 12345678910spring:cloud:gateway:routes:- id: javaboy_routeuri: http://httpbin.orgpredicates:- Query=name,java.*- Method=GET- After=2021-01-01T01:01:01+08:00[Asia/Shanghai] 12.4 FilterSpring Cloud Gateway 中的过滤器分为两大类： GatewayFilter GlobalFilter AddRequestParameter 过滤器使用： 1234567891011spring:cloud:gateway:routes:- id: javaboy_route uri: lb://provider filters:- AddRequestParameter=name,javaboy predicates:- Method=GET 这个过滤器就是在请求转发路由的时候，自动额外添加参数。 第10章 Spring Cloud Config13.1 基本用法分布式配置中心解决方案：国内：360：QConf淘宝：diamond百度：disconf国外：Apache Commons Configurationownercfg4j 13.1.1 简介Spring Cloud Config 是一个分布式系统配置管理的解决方案，它包含了 Client 和 Server 。配置文件放在 Server 端，通过 接口的形式提供给 Client。Spring Cloud Config 主要功能：集中管理各个环境、各个微服务的配置文件提供服务端和客户端支持配置文件修改后，可以快速生效配置文件通过 Git/SVn 进行管理，天然支持版本回退功能。支持高并发查询、也支持多种开发语言。 13.1.2 准备工作准备工作主要是给 GitHub 上提交数据。本地准备好相应的配置文件，提交到 GitHub：https://github.com/wongsung/configRepo 13.1.2 ConfigServer首先创建一个 ConfigServer 工程，创建时添加 ConfigServer 依赖： 项目创建成功后，项目启动类上添加注解，开启 config server 功能： 1234567@SpringBootApplication@EnableConfigServerpublic class ConfigServerApplication {public static void main(String[] args) {SpringApplication.run(ConfigServerApplication.class, args);}} 然后在配置文件中配置仓库的基本信息： 123456789spring.application.name=config-serverserver.port=8081# 配置文件仓库地址spring.cloud.config.server.git.uri=https://github.com/wongsung/configRepo.git# 仓库中，配置文件的目录spring.cloud.config.server.git.search-paths=client1# 仓库的用户名密码spring.cloud.config.server.git.username=1510161612@qq.comspring.cloud.config.server.git.password= 启动项目后，就可以访问配置文件了。访问地址：http://localhost:8081/client1/prod/master实际上，访问地址有如下规则： /{application}/{profile}/[{label}]/{application}-{profile}.yml/{application}-{profile}.properties/{label}/{application}-{profile}.yml/{label}/{application}-{profile}.properties applicaiton 表示配置文件名profile 表示配置文件 profile，例如 test、dev、prodlabel 表示git 分支，参数可选，默认就是master接下来，可以修改配置文件，并且重新提交到 GitHub，此时，刷新 ConfigServer 接口，就可以及时看到最新的配置内容。 13.1.3 ConfigClient创建一个 Spring Boot 项目，添加 ConfigClient 依赖： 项目创建成功后，resources 目录下，添加 bootstrap.properties 配置，内容如下： 123456# 下面三行配置，分别对应 config-server 中的 {application}、{profile}以及{label}占位符spring.application.name=client1spring.cloud.config.profile=devspring.cloud.config.label=masterspring.cloud.config.uri=http://localhost:8080server.port=8082 接下来创建一个 HelloController 进行测试： 123456789@RestControllerpublic class HelloController {@Value(\"${javaboy}\")String javaboy;@GetMapping(\"/hello\")public String hello() {return javaboy;}} 13.1.4 配置使用占位符灵活控制查询目录。修改 config-server 配置文件： 1spring.cloud.config.server.git.search-paths={application} 这里的 {application} 占位符，表示链接上来的 client1 的 spring.application.name 属性的值。在 confi-server 中，也可以用 {profile} 表示 client 的 spring.cloud.config.profile，也可以用 {label} 表示 client 的 spring.cloud.config.label虽然在实际开发中，配置文件一般都是放在 Git 仓库中，但是，config-server 也支持将配置文件放在classpath 下。在 config-server 中添加如下配置： 12# 表示让 config-server 从 classpath 下查找配置，而不是去 Git 仓库中查找spring.profiles.active=native 也可以在 config-server 中，添加如下配置，表示指定配置文件的位置： 1spring.cloud.config.server.native.search-locations=file:/E:/properties/ 13.2 配置文件加解密(这个得看视频了)😭13.2.1 常见加密方案不可逆加密可逆加密不可逆加密，就是理论上无法根据加密后的密文推算出明文。一般用在密码加密上，常见的算法如MD5 消息摘要算法、SHA 安全散列算法。可逆加密，看名字就知道可以根据加密后的密文推断出明文的加密方式，可逆加密一般又分为两种：对称加密非对称加密 对称加密指加密的密钥和解密的密钥是一样的。常见算法des、3des、aes非对称加密就是加密的密钥和解密的密钥不一样，加密的叫做公钥，可以告诉任何人，解密的叫做私钥，只有自己知道。常见算法 RSA。 13.2.2 对称加密首先下载不限长度的 JCE：http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip将下载的文件解压，解压出来的 jar 拷贝到 Java 安装目录中：C:\\Program Files\\Java\\jdk-13.0.1\\lib\\security然后，在 config-server 的 bootstrap.properties 配置文件中，添加如下内容配置密钥： 12# 密钥encrypt.key=javaboy 然后，启动 config-server ，访问如下地址，查看密钥配置是否OK：http://localhost:8081/encrypt/status然后，访问：http://localhost:8081/encrypt ，注意这是一个 POST 请求，访问该地址，可以对一段明文进行加密。把加密后的明文存储到 Git 仓库中，存储时，要注意加一个 {cipher} 前缀。 13.2.3 非对称加密非对称加密需要我们首先生成一个密钥对。在命令行执行如下命令，生成 keystore： 12keytool -genkeypair -alias config-server -keyalg RSA -keystoreD:\\springcloud\\config-server.keystore 命令执行完成后，拷贝生成的 keystore 文件到 config-server 的 resources 目录下。然后在 config-server 的 bootstrap.properties 目录中，添加如下配置： 1234encrypt.key-store.location=config-server.keystoreencrypt.key-store.alias=config-serverencrypt.key-store.password=111111encrypt.key-store.secret=111111 重启 config-server ，测试方法与对称加密一致。注意，在 pom.xml 的 build 节点中，添加如下配置，防止 keystore 文件被过滤掉。 123456789&lt;resources&gt;&lt;resource&gt;&lt;directory&gt;src/main/resources&lt;/directory&gt;&lt;includes&gt;&lt;include&gt;**/*.properties&lt;/include&gt;&lt;include&gt;**/*.keystore&lt;/include&gt;&lt;/includes&gt;&lt;/resource&gt;&lt;/resources&gt; 13.3 安全管理防止用户直接通过访问 config-server 看到配置文件内容，我们可以用 spring security 来保护 configserver接口。 首先在 config-server 中添加 spring security 依赖： 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 添加完依赖后，config-server 中的接口就自动被保护起来了。默认自动生成的密码不好记，所以我们可以在 config-server 中，自己配置用户名密码。在 config-server 的配置文件中，添加如下配置，固定用户名密码： 12spring.security.user.name=javaboyspring.security.user.password=123 然后，在 config-client 的 bootstrap.properties 配置文件中，添加如下配置： 12spring.cloud.config.username=javaboyspring.cloud.config.password=123 13.4 服务化前面的配置都是直接在 config-client 中写死 config-server 的地址。首先启动 Eureka 。然后，为了让 config-server 和 config-client 都能注册到 Eureka ，给它俩添加如下依赖： 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 然后，在 application.properties 配置文件中配置注册信息。 1eureka.client.service-url.defaultZone=http://localhost:1111/eureka 然后，修改 config-client 的配置文件，不再直接写死 config-server 的地址了。 12345678910111213# 下面三行配置，分别对应 config-server 中的 {application}、{profile}以及{label}占位符spring.application.name=client1spring.cloud.config.profile=devspring.cloud.config.label=master#spring.cloud.config.uri=http://localhost:8081# 开启通过 eureka 获取 config-server 的功能spring.cloud.config.discovery.enabled=true# 配置 config-server 服务名称spring.cloud.config.discovery.service-id=config-serverserver.port=8082spring.cloud.config.username=javaboyspring.cloud.config.password=123eureka.client.service-url.defaultZone=http://localhost:1111/eureka 注意，加入 eureka client 之后，启动 config-server 可能会报错，此时，我们重新生成一个 jks 格式的密钥。 12keytool -genkeypair -alias mytestkey -keyalg RSA -keypass 111111 -keystoreD:\\springcloud\\config-service.jks -storepass 111111 生成之后，拷贝到 configserver 的 resources 目录下，同时修改 bootstrap.properties 配置。 12345678# 密钥#encrypt.key=javaboyencrypt.key-store.location=classpath:config-service.jksencrypt.key-store.alias=mytestkeyencrypt.key-store.password=111111encrypt.key-store.secret=111111spring.security.user.name=javaboyspring.security.user.password=123 同时也修改一个 pom.xml 中的过滤条件： 123456789&lt;resources&gt;&lt;resource&gt;&lt;directory&gt;src/main/resources&lt;/directory&gt;&lt;includes&gt;&lt;include&gt;**/*.properties&lt;/include&gt;&lt;include&gt;**/*.jks&lt;/include&gt;&lt;/includes&gt;&lt;/resource&gt;&lt;/resources&gt; 13.5 动态刷新当配置文件发生变化之后，config-server 可以及时感知到变化，但是 config-client 不会及时感知到变化，默认情况下，config-client 只有重启才能加载到最新的配置文件。首先给 config-client 添加如下依赖： 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 然后，添加配置，使 refresh 端点暴露出来： 1management.endpoints.web.exposure.include=refresh 最后，再给 config-client 使用了配置文件的地方加上 @RefreshScope 注解，这样，当配置改变后，只需要调用 refresh 端点，config-client 中的配置就可以自动刷新。 12345678910@RestController@RefreshScopepublic class HelloController {@Value(\"${javaboy}\")String javaboy;@GetMapping(\"/hello\")public String hello() {return javaboy;}} 重启 config-client，以后，只要配置文件发生变化，发送 POST 请求，调用 http://localhost:8082/actuator/refresh 接口即可，配置文件就会自动刷新。 13.6 请求失败重试config-client 在调用 config-server 时，一样也可能发生请求失败的问题，这个时候，我们可以配置一个请求重试的功能。要给 config-client 添加重试功能，只需要添加如下依赖即可： 12345678&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.retry&lt;/groupId&gt;&lt;artifactId&gt;spring-retry&lt;/artifactId&gt;&lt;/dependency&gt; 然后，修改配置文件，开启失败快速响应。 12# 开启失败快速响应spring.cloud.config.fail-fast=true 然后，注释掉配置文件的用户名密码，重启 config-client，此时加载配置文件失败，就会自动重试。也可以通过如下配置保证服务的可用性： 12345678910# 开启失败快速响应spring.cloud.config.fail-fast=true# 请求重试的初识间隔时间spring.cloud.config.retry.initial-interval=1000# 最大重试次数spring.cloud.config.retry.max-attempts=6# 重试时间间隔乘数spring.cloud.config.retry.multiplier=1.1# 最大间隔时间spring.cloud.config.retry.max-interval=2000 第11章.Spring Cloud BusSpring Cloud Bus 通过轻量级的消息代理连接各个微服务，可以用来广播配置文件的更改，或者管理服务监控。安装 RabbitMQ。Docker 中 RabbitMQ 安装命令： 15672是管理界面的端口，5672是服务的端口。 1docker run -dit --name Myrabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin -p 15672:15672 -p 5672:5672 rabbitmq 1docker run -d --hostname my-rabbit --name some-rabbit -p 15672:15672 -p 5672:5672 rabbitmq 首先给client和server加上springcloudbus依赖 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 然后，给两个分别配置，使之都连接到 RabbitMQ 上： 1234spring.rabbitmq.host=192.168.91.128spring.rabbitmq.port=5672spring.rabbitmq.username=adminspring.rabbitmq.password=adm 同时，由于 configserver 将提供刷新接口，所以给 configserver 加上 actuator 依赖： 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 然后记得在 config-server 中，添加开启 bus-refresh 端点： 1management.endpoints.web.exposure.include=bus-refresh 由于给 config-server 中的所有接口都添加了保护，所以刷新接口将无法直接访问，此时，可以通过修改 Security 配置，对端点的权限做出修改： 123456789101112@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter {@Overrideprotected void configure(HttpSecurity http) throws Exception {http.authorizeRequests().anyRequest().authenticated().and().httpBasic().and().csrf().disable();}} 在这段配置中，开启了 HttpBasic 登录，这样，在发送刷新请求时，就可以直接通过 HttpBasic 配置认证信息了。最后分别启动 config-server 和 config-client，然后修改配置信息提交到 GitHub，刷新 config-client 接口，查看是否有变化。然后，发送如下 POST 请求：http://localhost:8081/actuator/bus-refresh这个 post 是针对 config-server 的，config-server 会把这个刷新的指令传到 rabbitmq ，然后rabbitmq 再把指令传给 各个 client。 逐个刷新如果更新配置文件之后，不希望每一个微服务都去刷新配置文件，那么可以通过如下配置解决问题。首先，给每一个 config-client 添加一个 instance-id： 1eureka.instance.instance-id=${spring.application.name}:${server.port} 然后，对 config-client 进行打包。打包完成后，通过如下命令启动两个 config-client 实例： 12java -jar config-client-0.0.1-SNAPSHOT.jar --server.port=8082java -jar config-client-0.0.1-SNAPSHOT.jar --server.port=8083 修改配置文件，并且提交到 GitHub 之后，可以通过如下方式只刷新某一个微服务，例如只刷新 8082的服务。 1http://localhost:8081/actuator/bus-refresh/client1:8082 client1:8082 表示服务的 instance-id。","link":"/2020/10/18/SpringCloud-H/"},{"title":"ES","text":"前言 最近的elasticsearch笔记 ​ 2021-04-20 18:05:26 ES个人使用环境为 es7.12 1.ElasticSearch 核心概念介绍4.1 ElasticSearch 十大核心概念4.1.1 集群（Cluster）一个或者多个安装了 es 节点的服务器组织在一起，就是集群，这些节点共同持有数据，共同提供搜索服务。 一个集群有一个名字，这个名字是集群的唯一标识，该名字成为 cluster name，默认的集群名称是 elasticsearch，具有相同名称的节点才会组成一个集群。 可以在 config/elasticsearch.yml 文件中配置集群名称： 1cluster.name: javaboy-es 在集群中，节点的状态有三种：绿色、黄色、红色： 绿色：节点运行状态为健康状态。所有的主分片、副本分片都可以正常工作。 黄色：表示节点的运行状态为警告状态，所有的主分片目前都可以直接运行，但是至少有一个副本分片是不能正常工作的。 红色：表示集群无法正常工作。 4.1.2 节点（Node）集群中的一个服务器就是一个节点，节点中会存储数据，同时参与集群的索引以及搜索功能。一个节点想要加入一个集群，只需要配置一下集群名称即可。默认情况下，如果我们启动了多个节点，多个节点还能够互相发现彼此，那么它们会自动组成一个集群，这是 es 默认提供的，但是这种方式并不可靠，有可能会发生脑裂现象。所以在实际使用中，建议一定手动配置一下集群信息。 4.1.3 索引（Index）索引可以从两方面来理解： 名词 具有相似特征文档的集合。 动词 索引数据以及对数据进行索引操作。 4.1.4 类型（Type）类型是索引上的逻辑分类或者分区。在 es6 之前，一个索引中可以有多个类型，从 es7 开始，一个索引中，只能有一个类型。在 es6.x 中，依然保持了兼容，依然支持单 index 多个 type 结构，但是已经不建议这么使用。 4.1.5 文档（Document）一个可以被索引的数据单元。例如一个用户的文档、一个产品的文档等等。文档都是 JSON 格式的。 4.1.6 分片（Shards）索引都是存储在节点上的，但是受限于节点的空间大小以及数据处理能力，单个节点的处理效果可能不理想，此时我们可以对索引进行分片。当我们创建一个索引的时候，就需要指定分片的数量。每个分片本身也是一个功能完善并且独立的索引。 默认情况下，一个索引会自动创建 1 个分片，并且为每一个分片创建一个副本。 4.1.7 副本（Replicas）副本也就是备份，是对主分片的一个备份。 4.1.8 Settings集群中对索引的定义信息，例如索引的分片数、副本数等等。 4.1.9 MappingMapping 保存了定义索引字段的存储类型、分词方式、是否存储等信息。 4.1.10 Analyzer字段分词方式的定义。 4.2 ElasticSearch Vs 关系型数据库 关系型数据库 ElasticSearch 数据库 索引 表 类型 行 文档 列 字段 表结构 映射（Mapping） SQL DSL(Domain Specific Language) Select * from xxx GET http:// update xxx set xx=xxx PUT http:// Delete xxx DELETE http:// 索引 全文索引 2.ElasticSearch 分词器1.1 内置分词器ElasticSearch 核心功能就是数据检索，首先通过索引将文档写入 es。查询分析则主要分为两个步骤： 词条化：分词器将输入的文本转为一个一个的词条流。 过滤：比如停用词过滤器会从词条中去除不相干的词条（的，嗯，啊，呢）停用词；另外还有同义词过滤器、小写过滤器等。 ElasticSearch 中内置了多种分词器可以供使用。 内置分词器： 分词器 作用 Standard Analyzer 标准分词器，适用于英语等。 Simple Analyzer 简单分词器，基于非字母字符进行分词，单词会被转为小写字母。 Whitespace Analyzer 空格分词器。按照空格进行切分。 Stop Analyzer 类似于简单分词器，但是增加了停用词的功能。 Keyword Analyzer 关键词分词器，输入文本等于输出文本。 Pattern Analyzer 利用正则表达式对文本进行切分，支持停用词。 Language Analyzer 针对特定语言的分词器。 Fingerprint Analyzer 指纹分析仪分词器，通过创建标记进行重复检测。 1.2 中文分词器在 Es 中，使用较多的中文分词器是 elasticsearch-analysis-ik，这个是 es 的一个第三方插件，代码托管在 GitHub 上： https://github.com/medcl/elasticsearch-analysis-ik 1.2.1 安装两种使用方式： 第一种： 首先打开分词器官网：https://github.com/medcl/elasticsearch-analysis-ik。 在 https://github.com/medcl/elasticsearch-analysis-ik/releases 页面找到最新的正式版，下载下来。我们这里的下载链接是 https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip。 将下载文件解压。 在 es/plugins 目录下，新建 ik 目录，并将解压后的所有文件拷贝到 ik 目录下。 重启 es 服务。 第二种： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip 1.2.2 测试es 重启成功后，首先创建一个名为 test 的索引： 接下来，在该索引中进行分词测试： 1.2.3 自定义扩展词库1.2.3.1 本地自定义在 es/plugins/ik/config 目录下，新建 ext.dic 文件（文件名任意），在该文件中可以配置自定义的词库。 如果有多个词，换行写入新词即可。 然后在 es/plugins/ik/config/IKAnalyzer.cfg.xml 中配置扩展词典的位置： 1.2.3.2 远程词库也可以配置远程词库，远程词库支持热更新（不用重启 es 就可以生效）。 热更新只需要提供一个接口，接口返回扩展词即可。 具体使用方式如下，新建一个 Spring Boot 项目，引入 Web 依赖即可。然后在 resources/stastic 目录下新建 ext.dic 文件，写入扩展词： 接下来，在 es/plugins/ik/config/IKAnalyzer.cfg.xml 文件中配置远程扩展词接口： 配置完成后，重启 es ，即可生效。 热更新，主要是响应头的 Last-Modified 或者 ETag 字段发生变化，ik 就会自动重新加载远程扩展辞典。 3. ElasticSearch 索引管理启动一个 master 节点和两个 slave 节点进行测试。 2.1 新建索引2.1.1 通过 head 插件新建索引在 head 插件中，选择 索引选项卡，然后点击新建索引。新建索引时，需要填入索引名称、分片数以及副本数。 索引创建成功后，如下图： 0、1、2、3、4 分别表示索引的分片，粗框表示主分片，细框表示副本（点一下框，通过 primary 属性可以查看是主分片还是副本）。.kibana 索引只有一个分片和一个副本，所以只有 0。 2.1.2 通过请求创建可以通过 postman 发送请求，也可以通过 kibana 发送请求，由于 kibana 有提示，所以这里采用 kibana。 创建索引请求： 1PUT book 创建成功后，可以查看索引信息： 需要注意两点： 索引名称不能有大写字母 索引名是唯一的，不能重复，重复创建会出错 2.2 更新索引索引创建好之后，可以修改其属性。 例如修改索引的副本数： 1234PUT book/_settings{ \"number_of_replicas\": 2} 修改成功后，如下： 更新分片数也是一样。 2.3 修改索引的读写权限索引创建成功后，可以向索引中写入文档： 1234PUT book/_doc/1{ \"title\":\"三国演义\"} 写入成功后，可以在 head 插件中查看： 默认情况下，索引是具备读写权限的，当然这个读写权限可以关闭。 例如，关闭索引的写权限： 1234PUT book/_settings{ \"blocks.write\": true} 关闭之后，就无法添加文档了。关闭了写权限之后，如果想要再次打开，方式如下： 1234PUT book/_settings{ \"blocks.write\": false} 其他类似的权限有： blocks.write blocks.read blocks.read_only 2.4 查看索引head 插件查看方式如下： 请求查看方式如下： 1GET book/_settings 也可以同时查看多个索引信息： 1GET book,test/_settings 也可以查看所有索引信息： 1GET book,test/_settings 2.5 删除索引head 插件可以删除索引： 请求删除如下： 1DELETE test 删除一个不存在的索引会报错。 5.6 索引打开/关闭关闭索引： 1POST book/_close 打开索引： 1POST book/_open 当然，可以同时关闭/打开多个索引，多个索引用 , 隔开，或者直接使用 _all 代表所有索引。 2.7 复制索引索引复制，只会复制数据，不会复制索引配置。 12345POST _reindex{ \"source\": {\"index\":\"book\"}, \"dest\": {\"index\":\"book_new\"}} 复制的时候，可以添加查询条件。 2.8 索引别名可以为索引创建别名，如果这个别名是唯一的，该别名可以代替索引名称。 1234567891011POST /_aliases{ \"actions\": [ { \"add\": { \"index\": \"book\", \"alias\": \"book_alias\" } } ]} 添加结果如下： 将 add 改为 remove 就表示移除别名： 1234567891011POST /_aliases{ \"actions\": [ { \"remove\": { \"index\": \"book\", \"alias\": \"book_alias\" } } ]} 查看某一个索引的别名： 1GET /book/_alias 查看某一个别名对应的索引（book_alias 表示一个别名）： 1GET /book_alias/_alias 可以查看集群上所有可用别名： 1GET /_alias 6.ElasticSearch 文档基本操作6.1创建文档首先新建一个索引 blog 然后向索引添加一个文档 123456PUT blog/_doc/1{ \"title\":\"ElasticSearch 文档基本操作\", \"data\":\"2021-04-09\", \"content\":\"### 6.1创建文档首先新建一个索引 blog\"} 1 表示新建文档的id 添加成功后响应的json如下： 123456789101112131415{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1} _index 表示文档的索引。 _type 表示文档的类型。 _id 表示文档的id。 _version 表示文档的版本(更新文档，版本会自动+1，针对一个文档的更新)。 result 表示执行结果。 _shards 表示分片信息。 _seq_no _primary_term 这两个也是版本控制用的(针对当前index) 添加成功后可以添加查看的文档： 添加文档是也可以不指定id，此时系统会默认给出一个id，如果不指定id，则需要使用POST请求，而不能使用PUT请求 1234{ \"error\" : \"Incorrect HTTP method for uri [/blog/_doc?pretty=true] and method [PUT], allowed: [POST]\", \"status\" : 405} 123456789101112{\"_index\": \"blog\",\"_type\": \"_doc\",\"_id\": \"5zULtngB3KliN6uQB99S\",\"_version\": 1,\"_score\": 1,\"_source\": {\"title\": \"666\",\"data\": \"2021-04-09\",\"content\": \"### 6.1创建文档首先新建一个索引 blog\"}} 6.2获取文档Es 中提供了GET API来查看存储在es中的文档。使用方式如下 1GET blog/_doc/5zULtngB3KliN6uQB99S 上面的命令表示获取id为 5zULtngB3KliN6uQB99S 的文档。 如果获取不存在的文档，会返回如下信息 123456{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"found\" : false} 如果仅仅只是想探测某个文档是否存在，可以使用head请求： 不存在的响应 存在的响应 也可以批量获取文档 1234GET blog/_mget{ \"ids\":[\"1\",\"5zULtngB3KliN6uQB99S\"]} GET 请求携带了请求体？ 某些特定请求，例如JS的HTTP请求库是不允许存在GET请求有请求体的，实际上在RFC7231文档中，并没有规定GET请求的请求体改如何处理，这造成了一定程度的混乱，有的HTTP服务器支持GET请求携带请求体，有的HTTP服务器则不支持。虽然ES工程师倾向于使用GET做查询，但是为了保证兼容性，ES同时也支持使用POST，例如上面的的批量查询案例也可以使用POST请求。 6.3文档更新文档更新一次，version就会自增1。 可以直接更新整个文档 1234PUT blog/_doc/5zULtngB3KliN6uQB99S{ \"title\":\"666\"} 这总方式，更新的文档会覆盖掉原有的文档 只想更新文档字段，可以通过脚本来实现 12345678910POST blog/_update/1{ \"script\": { \"lang\":\"painless\", \"source\":\"ctx._source.title=params.title\", \"params\":{ \"title\":\"666666\" } }} 更新的请求格式：POST{index}/_update/{id} 在脚本中lang表示脚本语言,painless是es内置的一种脚本语言，source表示具体执行的脚本，ctx是一个上下文对象，通过ctx可以访问到_source、_title等。 也可以通过同样的方式向文档中添加字段 1234567POST blog/_update/1{ \"script\": { \"lang\": \"painless\", \"source\": \"ctx._source.tags=[\\\"java\\\",\\\"php\\\"]\" }} 成功后的文档如下 12345678910111213141516171819{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 3, \"_seq_no\" : 2, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"title\" : \"666666\", \"data\" : \"2021-04-09\", \"content\" : \"### 6.1创建文档首先新建一个索引 blog\", \"tags\" : [ \"java\", \"php\" ] }} 7.ElasticSearch 文档路由es 是一个分布式系统，当我们存储一个文档到 es 上之后，这个文档实际上是被存储到 master 节点中的某一个主分片上。 例如新建一个索引，该索引有两个分片，0个副本，如下： 接下来，向该索引中保存一个文档： 1234PUT blog/_doc/a{ \"title\":\"a\"} 文档保存成功后，可以查看该文档被保存到哪个分片中去了： 1GET _cat/shards/blog?v 查看结果如下： 123index shard prirep state docs store ip nodeblog 1 p STARTED 0 208b 127.0.0.1 masterblog 0 p STARTED 0 208b 127.0.0.1 slave02 从这个结果中，可以看出，文档被保存到分片 0 中。 那么 es 中到底是按照什么样的规则去分配分片的？ es 中的路由机制是通过哈希算法，将具有相同哈希值的文档放到一个主分片中，分片位置的计算方式如下： shard=hash(routing) % number_of_primary_shards routing 可以是一个任意字符串，es 默认是将文档的 id 作为 routing 值，通过哈希函数根据 routing 生成一个数字，然后将该数字和分片数取余，取余的结果就是分片的位置。 默认的这种路由模式，最大的优势在于负载均衡，这种方式可以保证数据平均分配在不同的分片上。但是他有一个很大的劣势，就是查询时候无法确定文档的位置，此时它会将请求广播到所有的分片上去执行。另一方面，使用默认的路由模式，后期修改分片数量不方便。 当然开发者也可以自定义 routing 的值，方式如下： 1234PUT blog/_doc/d?routing=javaboy{ \"title\":\"d\"} 如果文档在添加时指定了 routing，则查询、删除、更新时也需要指定 routing。 1GET blog/_doc/d?routing=javaboy 自定义 routing 有可能会导致负载不均衡，这个还是要结合实际情况选择。 典型场景： 对于用户数据，我们可以将 userid 作为 routing，这样就能保证同一个用户的数据保存在同一个分片中，检索时，同样使用 userid 作为 routing，这样就可以精准的从某一个分片中获取数据。 8.ElasticSearch 并发的处理方式：锁和版本控制当我们使用 es 的 API 去进行文档更新时，它首先读取原文档出来，然后对原文档进行更新，更新完成后再重新索引整个文档。不论你执行多少次更新，最终保存在 es 中的是最后一次更新的文档。但是如果有两个线程同时去更新，就有可能出问题。 要解决问题，就是锁。 8.1 锁悲观锁 很悲观，每一次去读取数据的时候，都认为别人可能会修改数据，所以屏蔽一切可能破坏数据完整性的操作。关系型数据库中，悲观锁使用较多，例如行锁、表锁等等。 乐观锁 很乐观，每次读取数据时，都认为别人不会修改数据，因此也不锁定数据，只有在提交数据时，才会检查数据完整性。这种方式可以省去锁的开销，进而提高吞吐量。 在 es 中，实际上使用的就是乐观锁。 8.2 版本控制es6.7之前 在 es6.7 之前，使用 version+version_type 来进行乐观并发控制。根据前面的介绍，文档每被修改一个，version 就会自增一次，es 通过 version 字段来确保所有的操作都有序进行。 version 分为内部版本控制和外部版本控制。 8.2.1 内部版本es 自己维护的就是内部版本，当创建一个文档时，es 会给文档的版本赋值为 1。 每当用户修改一次文档，版本号就回自增 1。 如果使用内部版本，es 要求 version 参数的值必须和 es 文档中 version 的值相当，才能操作成功。 8.2.2 外部版本也可以维护外部版本。 在添加文档时，就指定版本号： 1234PUT blog/_doc/1?version=200&amp;version_type=external{ \"title\":\"2222\"} 以后更新的时候，版本要大于已有的版本号。 version_type=external 或者 version_type=external_gt 表示以后更新的时候，版本要大于已有的版本号。 version_type=external_gte 表示以后更新的时候，版本要大于等于已有的版本号。 8.2.3 最新方案（Es6.7 之后）现在使用 if_seq_no 和 if_primary_term 两个参数来做并发控制。 seq_no 不属于某一个文档，它是属于整个索引的（version 则是属于某一个文档的，每个文档的 version 互不影响）。现在更新文档时，使用 seq_no 来做并发。由于 seq_no 是属于整个 index 的，所以任何文档的修改或者新增，seq_no 都会自增。 现在就可以通过 seq_no 和 primary_term 来做乐观并发控制。 1234PUT blog/_doc/2?if_seq_no=5&amp;if_primary_term=1{ \"title\":\"6666\"} es倒排索引倒排索引是 es 中非常重要的索引结构，是从文档词项到文档 ID 的一个映射过程。 8.1 “正排索引”我们在关系型数据库中见到的索引，就是“正排索引”。 关系型数据库中的索引如下，假设我有一个博客表： 我们可以针对这个表建立索引（正排索引）： 当我们通过 id 或者标题去搜索文章时，就可以快速搜到。 但是如果我们按照文章内容的关键字去搜索，就只能去内容中做字符匹配了。为了提高查询效率，就要考虑使用倒排索引。 8.2 倒排索引倒排索引就是以内容的关键字建立索引，通过索引找到文档 id，再进而找到整个文档。 一般来说，倒排索引分为两个部分： 单词词典（记录所有的文档词项，以及词项到倒排列表的关联关系） 倒排列表（记录单词与对应的关系，由一系列倒排索引项组成，倒排索引项指：文档 id、词频（TF）（词项在文档中出现的次数，评分时使用）、位置（Position，词项在文档中分词的位置）、偏移（记录词项开始和结束的位置）） 当我们去索引一个文档时，就回建立倒排索引，搜索时，直接根据倒排索引搜索。 9.ElasticSearch 动态映射与静态映射映射就是 Mapping，它用来定义一个文档以及文档所包含的字段该如何被存储和索引。所以，它其实有点类似于关系型数据库中表的定义。 9.1 映射分类动态映射 顾名思义，就是自动创建出来的映射。es 根据存入的文档，自动分析出来文档中字段的类型以及存储方式，这种就是动态映射。 举一个简单例子，新建一个索引，然后查看索引信息： 在创建好的索引信息中，可以看到，mappings 为空，这个 mappings 中保存的就是映射信息。 现在我们向索引中添加一个文档，如下： 12345PUT blog/_doc/1{ \"title\":\"1111\", \"date\":\"2020-11-11\"} 文档添加成功后，就会自动生成 Mappings： 可以看到，date 字段的类型为 date，title 的类型有两个，text 和 keyword。 默认情况下，文档中如果新增了字段，mappings 中也会自动新增进来。 有的时候，如果希望新增字段时，能够抛出异常来提醒开发者，这个可以通过 mappings 中 dynamic 属性来配置。 dynamic 属性有三种取值： true，默认即此。自动添加新字段。 false，忽略新字段。 strict，严格模式，发现新字段会抛出异常。 具体配置方式如下，创建索引时指定 mappings（这其实就是静态映射）： 1234567891011121314PUT blog{ \"mappings\": { \"dynamic\":\"strict\", \"properties\": { \"title\":{ \"type\": \"text\" }, \"age\":{ \"type\":\"long\" } } }} 然后向 blog 中索引中添加数据： 123456PUT blog/_doc/2{ \"title\":\"1111\", \"date\":\"2020-11-11\", \"age\":99} 在添加的文档中，多出了一个 date 字段，而该字段没有预定义，所以这个添加操作就回报错： 12345678910111213{ \"error\" : { \"root_cause\" : [ { \"type\" : \"strict_dynamic_mapping_exception\", \"reason\" : \"mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed\" } ], \"type\" : \"strict_dynamic_mapping_exception\", \"reason\" : \"mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed\" }, \"status\" : 400} 动态映射还有一个日期检测的问题。 例如新建一个索引，然后添加一个含有日期的文档，如下： 1234PUT blog/_doc/1{ \"remark\":\"2020-11-11\"} 添加成功后，remark 字段会被推断是一个日期类型。 此时，remark 字段就无法存储其他类型了。 1234PUT blog/_doc/1{ \"remark\":\"javaboy\"} 此时报错如下： 123456789101112131415161718192021{ \"error\" : { \"root_cause\" : [ { \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'\" } ], \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'\", \"caused_by\" : { \"type\" : \"illegal_argument_exception\", \"reason\" : \"failed to parse date field [javaboy] with format [strict_date_optional_time||epoch_millis]\", \"caused_by\" : { \"type\" : \"date_time_parse_exception\", \"reason\" : \"Failed to parse with all enclosed parsers\" } } }, \"status\" : 400} 要解决这个问题，可以使用静态映射，即在索引定义时，将 remark 指定为 text 类型。也可以关闭日期检测。 123456PUT blog{ \"mappings\": { \"date_detection\": false }} 此时日期类型就回当成文本来处理。 静态映射 略。 9.2 类型推断es 中动态映射类型推断方式如下： 10.ElasticSearch 四种字段类型详解10.1 核心类型10.1.1 字符串类型 string：这是一个已经过期的字符串类型。在 es5 之前，用这个来描述字符串，现在的话，它已经被 text 和 keyword 替代了。 text：如果一个字段是要被全文检索的，比如说博客内容、新闻内容、产品描述，那么可以使用 text。用了 text 之后，字段内容会被分析，在生成倒排索引之前，字符串会被分词器分成一个个词项。text 类型的字段不用于排序，很少用于聚合。这种字符串也被称为 analyzed 字段。 keyword：这种类型适用于结构化的字段，例如标签、email 地址、手机号码等等，这种类型的字段可以用作过滤、排序、聚合等。这种字符串也称之为 not-analyzed 字段。 10.1.2 数字类型 在满足需求的情况下，优先使用范围小的字段。字段长度越短，索引和搜索的效率越高。 浮点数，优先考虑使用 scaled_float。通过缩放因子（底层将一个浮点数变为整数和一个缩放倍数，用来节省空间）将浮点数缩放，更好的空间利用 在使用scaled_float时，需要制定缩放因子scaled_float scaled_float 举例： 1234567891011121314PUT product{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\" }, \"price\":{ \"type\": \"scaled_float\", \"scaling_factor\": 100 } } }} 10.1.3 日期类型由于 JSON 中没有日期类型，所以 es 中的日期类型形式就比较多样： 2020-11-11 或者 2020-11-11 11:11:11 一个从 1970.1.1 零点到现在的一个秒数或者毫秒数。 es 内部将时间转为 UTC，然后将时间按照 millseconds-since-the-epoch 的长整型来存储。 自定义日期类型： 12345678910PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" } } }} 这个能够解析出来的时间格式比较多。 123456789101112131415PUT product/_doc/1{ \"date\":\"2020-11-11\"}PUT product/_doc/2{ \"date\":\"2020-11-11T11:11:11Z\"}PUT product/_doc/3{ \"date\":\"1604672099958\"} 上面三个文档中的日期都可以被解析，内部存储的是毫秒计时的长整型数。 10.1.4 布尔类型（boolean）JSON 中的 “true”、“false”、true、false 都可以。 10.1.5 二进制类型（binary）二进制接受的是 base64 编码的字符串，默认不存储，也不可搜索。 10.1.6 范围类型 integer_range float_range long_range double_range date_range ip_range 定义的时候，指定范围类型即可： 12345678910111213PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" }, \"price\":{ \"type\":\"float_range\" } } }} 插入文档的时候，需要指定范围的界限： 12345678910111213PUT product{ \"mappings\": { \"properties\": { \"date\":{ \"type\": \"date\" }, \"price\":{ \"type\":\"float_range\" } } }} 指定范围的时，可以使用 gt、gte、lt、lte。 10.2 复合类型10.2.1 数组类型es 中没有专门的数组类型。默认情况下，任何字段都可以有一个或者多个值。需要注意的是，数组中的元素必须是同一种类型。 添加数组是，数组中的第一个元素决定了整个数组的类型。 10.2.2 对象类型（object）由于 JSON 本身具有层级关系，所以文档包含内部对象。内部对象中，还可以再包含内部对象。 1234567PUT product/_doc/2{ \"date\":\"2020-11-11T11:11:11Z\", \"ext_info\":{ \"address\":\"China\" }} 10.2.3 嵌套类型（nested）nested 是 object 中的一个特例。 如果使用 object 类型，假如有如下一个文档： 123456789101112{ \"user\":[ { \"first\":\"Zhang\", \"last\":\"san\" }, { \"first\":\"Li\", \"last\":\"si\" } ]} 由于 Lucene 没有内部对象的概念，所以 es 会将对象层次扁平化，将一个对象转为字段名和值构成的简单列表。即上面的文档，最终存储形式如下： 1234{\"user.first\":[\"Zhang\",\"Li\"],\"user.last\":[\"san\",\"si\"]} 扁平化之后，用户名之间的关系没了。这样会导致如果搜索 Zhang si 这个人，会搜索到。 此时可以 nested 类型来解决问题，nested 对象类型可以保持数组中每个对象的独立性。nested 类型将数组中的每一饿对象作为独立隐藏文档来索引，这样每一个嵌套对象都可以独立被索引。 {{“user.first”:”Zhang”,“user.last”:”san”},{“user.first”:”Li”,“user.last”:”si”}} 优点 文档存储在一起，读取性能高。 缺点 更新父或者子文档时需要更新更个文档。 10.3 地理类型使用场景： 查找某一个范围内的地理位置 通过地理位置或者相对中心点的距离来聚合文档 把距离整个到文档的评分中 通过距离对文档进行排序 10.3.1 geo_pointgeo_point 就是一个坐标点，定义方式如下： 12345678910PUT people{ \"mappings\": { \"properties\": { \"location\":{ \"type\": \"geo_point\" } } }} 创建时指定字段类型，存储的时候，有四种方式： 1234567891011121314151617181920212223//objectPUT people/_doc/1{ \"location\":{ \"lat\": 34.27, \"lon\": 108.94 }}//字符串PUT people/_doc/2{ \"location\":\"34.27,108.94\"}//经纬度的hash值PUT people/_doc/3{ \"location\":\"uzbrgzfxuzup\"}//数组 经度在前，纬度在后PUT people/_doc/4{ \"location\":[108.94,34.27]} 注意，使用数组描述，先经度后纬度。 地址位置转 geo_hash：http://www.csxgame.top/#/ 10.3.2 geo_shape 指定 geo_shape 类型： 12345678910PUT people{ \"mappings\": { \"properties\": { \"location\":{ \"type\": \"geo_shape\" } } }} 添加文档时需要指定具体的类型： 1234567PUT people/_doc/1{ \"location\":{ \"type\":\"point\", \"coordinates\": [108.94,34.27] }} 如果是 linestring，如下： 1234567PUT people/_doc/2{ \"location\":{ \"type\":\"linestring\", \"coordinates\": [[108.94,34.27],[100,33]] }} 10.4 特殊类型10.4.1 IP存储 IP 地址，类型是 ip： 12345678910PUT blog{ \"mappings\": { \"properties\": { \"address\":{ \"type\": \"ip\" } } }} 添加文档： 1234PUT blog/_doc/1{ \"address\":\"192.168.91.1\"} 搜索文档： 12345678GET blog/_search{ \"query\": { \"term\": { \"address\": \"192.168.0.0/16\" } }} 10.4.2 token_count用于统计字符串分词后的词项个数。 12345678910111213141516PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"fields\": { \"length\":{ \"type\":\"token_count\", \"analyzer\":\"standard\" } } } } }} 相当于新增了 title.length 字段用来统计分词后词项的个数。 添加文档： 1234PUT blog/_doc/1{ \"title\":\"zhang san\"} 可以通过 token_count 去查询： 12345678GET blog/_search{ \"query\": { \"term\": { \"title.length\": 2 } }} 11.ElasticSearch 23 种映射参数详解11.1 analyzer定义文本字段的分词器。默认对索引和查询都是有效的。 假设不用分词器，我们先来看一下索引的结果，创建一个索引并添加一个文档： 123456PUT blogPUT blog/_doc/1{ \"title\":\"定义文本字段的分词器。默认对索引和查询都是有效的。\"} 查看词条向量（term vectors） 1234GET blog/_termvectors/1{ \"fields\": [\"title\"]} 查看结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"took\" : 0, \"term_vectors\" : { \"title\" : { \"field_statistics\" : { \"sum_doc_freq\" : 22, \"doc_count\" : 1, \"sum_ttf\" : 23 }, \"terms\" : { \"义\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 1, \"start_offset\" : 1, \"end_offset\" : 2 } ] }, \"分\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 7, \"start_offset\" : 7, \"end_offset\" : 8 } ] }, \"和\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 15, \"start_offset\" : 16, \"end_offset\" : 17 } ] }, \"器\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 9, \"start_offset\" : 9, \"end_offset\" : 10 } ] }, \"字\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 4, \"start_offset\" : 4, \"end_offset\" : 5 } ] }, \"定\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 0, \"start_offset\" : 0, \"end_offset\" : 1 } ] }, \"对\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 12, \"start_offset\" : 13, \"end_offset\" : 14 } ] }, \"引\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 14, \"start_offset\" : 15, \"end_offset\" : 16 } ] }, \"效\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 21, \"start_offset\" : 22, \"end_offset\" : 23 } ] }, \"文\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 2, \"start_offset\" : 2, \"end_offset\" : 3 } ] }, \"是\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 19, \"start_offset\" : 20, \"end_offset\" : 21 } ] }, \"有\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 20, \"start_offset\" : 21, \"end_offset\" : 22 } ] }, \"本\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 3, \"start_offset\" : 3, \"end_offset\" : 4 } ] }, \"查\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 16, \"start_offset\" : 17, \"end_offset\" : 18 } ] }, \"段\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 5, \"start_offset\" : 5, \"end_offset\" : 6 } ] }, \"的\" : { \"term_freq\" : 2, \"tokens\" : [ { \"position\" : 6, \"start_offset\" : 6, \"end_offset\" : 7 }, { \"position\" : 22, \"start_offset\" : 23, \"end_offset\" : 24 } ] }, \"索\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 13, \"start_offset\" : 14, \"end_offset\" : 15 } ] }, \"认\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 11, \"start_offset\" : 12, \"end_offset\" : 13 } ] }, \"词\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 8, \"start_offset\" : 8, \"end_offset\" : 9 } ] }, \"询\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 17, \"start_offset\" : 18, \"end_offset\" : 19 } ] }, \"都\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 18, \"start_offset\" : 19, \"end_offset\" : 20 } ] }, \"默\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 10, \"start_offset\" : 11, \"end_offset\" : 12 } ] } } } }} 可以看到，默认情况下，中文就是一个字一个字的分，这种分词方式没有任何意义。如果这样分词，查询就只能按照一个字一个字来查，像下面这样： 12345678GET blog/_search{ \"query\": { \"term\": { \"title\": \"定\" } }} 无意义！！！ 所以，我们要根据实际情况，配置合适的分词器。 给字段设定分词器： 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\":\"text\", \"analyzer\": \"ik_smart\" } } }} 存储文档： 1234PUT blog/_doc/1{ \"title\":\"定义文本字段的分词器。默认对索引和查询都是有效的。\"} 查看词条向量： 1234GET blog/_termvectors/1{ \"fields\": [\"title\"]} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144{ \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"took\" : 1, \"term_vectors\" : { \"title\" : { \"field_statistics\" : { \"sum_doc_freq\" : 12, \"doc_count\" : 1, \"sum_ttf\" : 13 }, \"terms\" : { \"分词器\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 4, \"start_offset\" : 7, \"end_offset\" : 10 } ] }, \"和\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 8, \"start_offset\" : 16, \"end_offset\" : 17 } ] }, \"字段\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 2, \"start_offset\" : 4, \"end_offset\" : 6 } ] }, \"定义\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 0, \"start_offset\" : 0, \"end_offset\" : 2 } ] }, \"对\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 6, \"start_offset\" : 13, \"end_offset\" : 14 } ] }, \"文本\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 1, \"start_offset\" : 2, \"end_offset\" : 4 } ] }, \"有效\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 11, \"start_offset\" : 21, \"end_offset\" : 23 } ] }, \"查询\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 9, \"start_offset\" : 17, \"end_offset\" : 19 } ] }, \"的\" : { \"term_freq\" : 2, \"tokens\" : [ { \"position\" : 3, \"start_offset\" : 6, \"end_offset\" : 7 }, { \"position\" : 12, \"start_offset\" : 23, \"end_offset\" : 24 } ] }, \"索引\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 7, \"start_offset\" : 14, \"end_offset\" : 16 } ] }, \"都是\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 10, \"start_offset\" : 19, \"end_offset\" : 21 } ] }, \"默认\" : { \"term_freq\" : 1, \"tokens\" : [ { \"position\" : 5, \"start_offset\" : 11, \"end_offset\" : 13 } ] } } } }} 然后就可以通过词去搜索了： 12345678GET blog/_search{ \"query\": { \"term\": { \"title\": \"索引\" } }} 11.2 search_analyzer查询时候的分词器。默认情况下，如果没有配置 search_analyzer，则查询时，首先查看有没有 search_analyzer，有的话，就用 search_analyzer 来进行分词，如果没有，则看有没有 analyzer，如果有，则用 analyzer 来进行分词，否则使用 es 默认的分词器。 11.3 normalizernormalizer 参数用于解析前（索引或者查询）的标准化配置。 比如，在 es 中，对于一些我们不想切分的字符串，我们通常会将其设置为 keyword，搜索时候也是使用整个词进行搜索。如果在索引前没有做好数据清洗，导致大小写不一致，例如 javaboy 和 JAVABOY，此时，我们就可以使用 normalizer 在索引之前以及查询之前进行文档的标准化。 先来一个反例，创建一个名为 blog 的索引，设置 author 字段类型为 keyword： 12345678910PUT blog{ \"mappings\": { \"properties\": { \"author\":{ \"type\": \"keyword\" } } }} 添加两个文档： 123456789PUT blog/_doc/1{ \"author\":\"javaboy\"}PUT blog/_doc/2{ \"author\":\"JAVABOY\"} 然后进行搜索： 12345678GET blog/_search{ \"query\": { \"term\": { \"author\": \"JAVABOY\" } }} 大写关键字可以搜到大写的文档，小写关键字可以搜到小写的文档。 如果使用了 normalizer，可以在索引和查询时，分别对文档进行预处理。 normalizer 定义方式如下： 123456789101112131415161718192021PUT blog{ \"settings\": { \"analysis\": { \"normalizer\":{ \"my_normalizer\":{ \"type\":\"custom\", \"filter\":[\"lowercase\"] } } } }, \"mappings\": { \"properties\": { \"author\":{ \"type\": \"keyword\", \"normalizer\":\"my_normalizer\" } } }} 在 settings 中定义 normalizer，然后在 mappings 中引用。 测试方式和前面一致。此时查询的时候，大写关键字也可以查询到小写文档，因为无论是索引还是查询，都会将大写转为小写。 11.4 boostboost 参数可以设置字段的权重。 默认权重为1 boost 有两种使用思路，一种就是在定义 mappings 的时候使用，在指定字段类型时使用；另一种就是在查询时使用。 实际开发中建议使用后者，前者有问题：如果不重新索引文档，权重无法修改。 只支持 term 查询 (不支持prefix，range，fuzzy查询) mapping 中使用 boost（不推荐）：7.12出现提示将在8.0移除 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"content\":{ \"type\": \"text\", \"boost\": 2 } } }} 另一种方式就是在查询的时候，指定 boost 1234567891011PUT blog{ \"mappings\": { \"properties\": { \"content\":{ \"type\": \"text\" , \"analyzer\": \"ik_smart\" } } }} 1234POST blog/_doc{ \"content\":\"你好测试的人,你好测试的人,你好测试的人\"} 1234567891011GET blog/_search{ \"query\": { \"match\": { \"content\": { \"query\": \"你好\", \"boost\": 2 } } }} 为何在建索引时加权重是一个不好的行为？ 1.如果不重新索引文档，权重无法修改。 2.在查询时可以调整权重而不需要重新索引 3.在索引添加权重会占用一个字节的空间，This reduces the resolution of the field length normalization factor which can lead to lower quality relevance calculations.(这一段没看懂) 11.5 coercecoerce 用来清除脏数据，默认为 true。 比如一个数字 5 ，一般是integer，但是可能是String “5”，也可能是是浮点数 float 5.0, 甚至 “5.0”String 123456789101112131415161718192021222324PUT blog{ \"mappings\": { \"properties\": { \"number_one\":{ \"type\": \"integer\" }, \"number_two\":{ \"type\": \"integer\", \"coerce\": false } } }}PUT blog/_doc/1{ \"number_one\":\"10\"}PUT blog/_doc/2{ \"number_two\":\"10\"} 可以通过update mapping api 让coerce 更新已有的字段 同样可以在建索引时全局禁用coercion并启用部分 123456789101112131415161718192021222324252627PUT my-index-000001{ \"settings\": { \"index.mapping.coerce\": false }, \"mappings\": { \"properties\": { \"number_one\": { \"type\": \"integer\", \"coerce\": true }, \"number_two\": { \"type\": \"integer\" } } }}PUT my-index-000001/_doc/1{ \"number_one\": \"10\" } PUT my-index-000001/_doc/2{ \"number_two\": \"10\" } 11.6 copy_to将多个字段的值复制到一个字段中 123456789101112131415161718192021222324252627282930313233343536PUT my-index-000001{ \"mappings\": { \"properties\": { \"first_name\": { \"type\": \"text\", \"copy_to\": \"full_name\" }, \"last_name\": { \"type\": \"text\", \"copy_to\": \"full_name\" }, \"full_name\": { \"type\": \"text\" } } }}PUT my-index-000001/_doc/1{ \"first_name\": \"John\", \"last_name\": \"Smith\"}GET my-index-000001/_search{ \"query\": { \"match\": { \"full_name\": { \"query\": \"John Smith\", \"operator\": \"and\" } } }} 11.7 doc_values 和 fielddataes 中的搜索主要是用到倒排索引，doc_values 参数是为了加快排序、聚合操作而生的。当建立倒排索引的时候，会额外增加列式存储映射。 doc_values 默认是开启的，如果确定某个字段不需要排序或者不需要聚合，那么可以关闭 doc_values。 大部分的字段在索引时都会生成 doc_values，除了 text。text 字段在查询时会生成一个 fielddata 的数据结构，fieldata 在字段首次被聚合、排序的时候生成。 doc_value fielddata 索引时创建 使用时动态创建 磁盘 内存 不占用内存 不占用磁盘 索引速度稍微低一点 文档很多时，动态创建慢，占内存 doc_values 默认开启，fielddata 默认关闭。 doc_values 演示： 1234567891011121314151617181920212223242526272829303132333435PUT usersPUT users/_doc/1{ \"age\":100}PUT users/_doc/2{ \"age\":99}PUT users/_doc/3{ \"age\":98}PUT users/_doc/4{ \"age\":101}GET users/_search{ \"query\": { \"match_all\": {} }, \"sort\":[ { \"age\":{ \"order\": \"desc\" } } ]} 由于 doc_values 默认时开启的，所以可以直接使用该字段排序，如果想关闭 doc_values ，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT users{ \"mappings\": { \"properties\": { \"age\":{ \"type\": \"integer\", \"doc_values\": false } } }}PUT users/_doc/1{ \"age\":100}PUT users/_doc/2{ \"age\":99}PUT users/_doc/3{ \"age\":98}PUT users/_doc/4{ \"age\":101}GET users/_search{ \"query\": { \"match_all\": {} }, \"sort\":[ { \"age\":{ \"order\": \"desc\" } } ]} 11.8 dynamic当对包含新字段的文档建立索引时，ElasticSearch会将字段动态添加到文档或文档中的内部对象。 内部对象从其父对象或映射类型继承动态设置。在类型级别禁用了动态映射，因此不会动态的添加新的顶级字段。 dynamic 可选参数 param description true 默认 runtime 新字段作为运行时字段添加到映射中。未建立索引，而是在查询时从_source加载 false 新字段会被忽略，不能被索引和搜索，但仍会出现在返回的匹配的_source字段中，不会添加到映射中，必须显式添加新字段 strict 如果检测到新字段，会引发异常并拒绝，必须显式添加新字段 11.9 enabledes 默认会索引所有的字段，但是有的字段可能只需要存储，不需要索引。此时可以通过 enabled 字段来控制： 只能应用于顶级映射定义和对象字段。仍然可以从_source字段中检索JSON，但无法搜索或以其他任何方式存储； 已经存在的字段和顶级映射无法更新enabled 可以将非对象字段添加到禁用字段 12345678910111213141516PUT my-index-000001{ \"mappings\": { \"properties\": { \"session_data\": { \"type\": \"object\", \"enabled\": false } } }}PUT my-index-000001/_doc/session_1{ \"session_data\": \"foo bar\" } 返回的结果为 1234567891011121314{ \"_index\" : \"my-index-000001\", \"_type\" : \"_doc\", \"_id\" : \"session_1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1} 11.10 format日期格式。format 可以规范日期格式，而且一次可以定义多个 format。 123456789101112131415161718192021PUT users{ \"mappings\": { \"properties\": { \"birthday\":{ \"type\": \"date\", \"format\": \"yyyy-MM-dd||yyyy-MM-dd HH:mm:ss\" } } }}PUT users/_doc/1{ \"birthday\":\"2020-11-11\"}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11\"} 多个日期格式之间，使用 || 符号连接，注意没有空格。 如果用户没有指定日期的 format，默认的日期格式是 strict_date_optional_time||epoch_mills 另外，所有的日期格式，可以在 https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 网址查看。 11.11 ignore_aboveigbore_above 用于指定分词和索引的字符串最大长度，超过最大长度的话，该字段将不会被索引，这个字段只适用于 keyword 类型。 123456789101112131415161718192021222324252627282930PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"keyword\", \"ignore_above\": 10 } } }}PUT blog/_doc/1{ \"title\":\"javaboy\"}PUT blog/_doc/2{ \"title\":\"javaboyjavaboyjavaboy\"}GET blog/_search{ \"query\": { \"term\": { \"title\": \"javaboyjavaboyjavaboy\" } }} 10.12 ignore_malformedignore_malformed 可以忽略不规则的数据，该参数默认为 false。 12345678910111213141516171819202122232425262728293031323334PUT users{ \"mappings\": { \"properties\": { \"birthday\":{ \"type\": \"date\", \"format\": \"yyyy-MM-dd||yyyy-MM-dd HH:mm:ss\" }, \"age\":{ \"type\": \"integer\", \"ignore_malformed\": true } } }}PUT users/_doc/1{ \"birthday\":\"2020-11-11\", \"age\":99}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11\", \"age\":\"abc\"}PUT users/_doc/2{ \"birthday\":\"2020-11-11 11:11:11aaa\", \"age\":\"abc\"} 10.13 include_in_all这个是针对 _all 字段的，但是在 es7 中，该字段已经被废弃了。 10.14 indexindex 属性指定一个字段是否被索引，该属性为 true 表示字段被索引，false 表示字段不被索引。 12345678910111213141516171819202122232425PUT users{ \"mappings\": { \"properties\": { \"age\":{ \"type\": \"integer\", \"index\": false } } }}PUT users/_doc/1{ \"age\":99}GET users/_search{ \"query\": { \"term\": { \"age\": 99 } }} 如果 index 为 false，则不能通过对应的字段搜索。 10.15 index_optionsindex_options 控制索引时哪些信息被存储到倒排索引中（用在 text 字段中），有四种取值： index_options 备注 docs 只存储文档编号，默认 freqs 在doc基础上，存储词项频率 positions 在freqs基础上，存储词项偏移位置 offsets 在positions基础上，存储词项开始和结束的字符位置 123456789101112131415161718192021222324252627282930313233//这段没怎么懂PUT my-index-000001{ \"mappings\": { \"properties\": { \"text\": { \"type\": \"text\", \"index_options\": \"offsets\" } } }}PUT my-index-000001/_doc/1{ \"text\": \"Quick brown fox\"}GET my-index-000001/_search{ \"query\": { \"match\": { \"text\": \"brown fox\" } }, \"highlight\": { \"fields\": { \"text\": {} } }} 10.16 normsnorms 对字段评分有用，text 默认开启 norms，如果不是特别需要，不要开启 norms。 10.17 null_value在 es 中，值为 null 的字段不索引也不可以被搜索，null_value 可以让值为 null 的字段显式的可索引、可搜索： 1234567891011121314151617181920212223242526PUT users{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\", \"null_value\": \"javaboy_null\" /*这个里面的value有什么意义呢*/ } } }}PUT users/_doc/1{ \"name\":null, \"age\":99}GET users/_search{ \"query\": { \"term\": { \"name\": \"javaboy_null\" } }} 10.18 position_increment_gap被解析的 text 字段会将 term 的位置考虑进去，目的是为了支持近似查询和短语查询，当我们去索引一个含有多个值的 text 字段时，会在各个值之间添加一个假想的空间，将值隔开，这样就可以有效避免一些无意义的短语匹配，间隙大小通过 position_increment_gap 来控制，默认是 100。 123456789101112131415161718192021222324252627PUT my-index-000001/_doc/1{ \"names\": [ \"John Abraham\", \"Lincoln Smith\"]}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": { \"query\": \"Abraham Lincoln\" } } }}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": { \"query\": \"Abraham Lincoln\", \"slop\": 101 } } }} This phrase query matches our document, even though Abraham and Lincoln are in separate strings, because slop &gt; position_increment_gap. 两个get请求两种返回 123456789101112131415161718{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }} 12345678910111213141516171819202122232425262728293031{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.010358453, \"hits\" : [ { \"_index\" : \"my-index-000001\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.010358453, \"_source\" : { \"names\" : [ \"John Abraham\", \"Lincoln Smith\" ] } } ] }} 可以在映射中指定position_increment_gap 12345678910111213141516171819202122232425PUT my-index-000001{ \"mappings\": { \"properties\": { \"names\": { \"type\": \"text\", \"position_increment_gap\": 0 } } }}PUT my-index-000001/_doc/1{ \"names\": [ \"John Abraham\", \"Lincoln Smith\"]}GET my-index-000001/_search{ \"query\": { \"match_phrase\": { \"names\": \"Abraham Lincoln\" } }} 10.19 properties可以在查询，聚合等中使用点表示法来引用内部字段： 1234567891011121314151617181920212223GET my-index-000001/_search{ \"query\": { \"match\": { \"manager.name\": \"Alice White\" } }, \"aggs\": { \"Employees\": { \"nested\": { \"path\": \"employees\" }, \"aggs\": { \"Employee Ages\": { \"histogram\": { \"field\": \"employees.age\", \"interval\": 5 } } } } }} 10.20 similaritysimilarity 指定文档的评分模型，默认有三种： similarity 备注 BM25 es和lucene默认的评分模型 classic TF/IDF评分 boolean boolean模型评分 10.21 store默认情况下，字段会被索引，也可以搜索，但是不会存储，虽然不会被存储的，但是 _source 中有一个字段的备份。如果想将字段存储下来，可以通过配置 store 来实现。 123456789101112131415161718192021222324252627282930PUT my-index-000001{ \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"store\": true }, \"date\": { \"type\": \"date\", \"store\": true }, \"content\": { \"type\": \"text\" } } }}PUT my-index-000001/_doc/1{ \"title\": \"Some short title\", \"date\": \"2015-01-01\", \"content\": \"A very long content field...\"}GET my-index-000001/_search{ \"stored_fields\": [ \"title\", \"date\" ] } 10.22 term_vectorsterm_vectors 是通过分词器产生的信息，包括： 一组 terms 每个 term 的位置 term 的首字符/尾字符与原始字符串原点的偏移量 term_vectors 取值： 取值 备注 no 不存储信息，默认 yes term被存储 with_positions 在yes的基础上增加位置信息 with_offset 在yes的基础上增加偏移信息 with_positions_offsets term，positons，offsets都存储 with_positions_payloads with_positions_offsets_payloads 设置 with_postions_offsets将使字段索引大小翻倍 11.23 fieldsfields 参数可以让同一字段有多种不同的索引方式。例如： 1234567891011121314151617181920212223242526272829PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"fields\": { \"raw\":{ \"type\":\"keyword\" } } } } }}PUT blog/_doc/1{ \"title\":\"javaboy\"}GET blog/_search{ \"query\": { \"term\": { \"title.raw\": \"javaboy\" } }} 12.映射模板13.ElasticSearch 搜索数据导入 下载脚本bookdata.json。链接: https://pan.baidu.com/s/12Gj5aovYKI5g2X8pPJZtLw 提取码: a5h4 创建索引： 1234567891011121314151617181920212223242526272829PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 执行如下脚本导入命令： 1curl -XPOST \"http://localhost:9200/books/_bulk?pretty\" -H \"content-type:application/json\" --data-binary @bookdata.json 14.ElasticSearch 搜索入门搜索分为两个过程： 当向索引中保存文档时，默认情况下，es 会保存两份内容，一份是 _source 中的数据，另一份则是通过分词、排序等一系列过程生成的倒排索引文件，倒排索引中保存了词项和文档之间的对应关系。 搜索时，当 es 接收到用户的搜索请求之后，就会去倒排索引中查询，通过的倒排索引中维护的倒排记录表找到关键词对应的文档集合，然后对文档进行评分、排序、高亮等处理，处理完成后返回文档。 14.1 简单搜索查询文档： 123456GET books/_search{ \"query\": { \"match_all\": {} }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159{ \"took\" : 19, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 38, \"relation\" : \"eq\" }, \"max_score\" : 4.217799, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"279\", \"_score\" : 4.217799, \"_source\" : { \"name\" : \"计算机应用基础\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机理论、基础知识\", \"author\" : \"张宇，赖麟\", \"info\" : \"《计算机应用基础》是国家精品课程“计算机文件基础”的配套教材。全书按照工学结合人才培养模式的要求，以培养能力为目标，基于工作过程组织课程；以典型的工作任务为载体，采用任务驱动的方式来构造知识和技能平台，强调理论和训练一体化，做到“教、学、做”相结合，让学生对知识有整体认识，即按照“先行后知、先学后教”的思想编写。全书内容包括：计算机基础知识、WnwsXP操作系统、McsfOffc2003办公自动化软件、计算机网络基础。《计算机应用基础》的显著特点是以学生为主体，通过实际工作过程中的典型工作任务来训练学生，培养学生解决和处理实际问题的能力，将被动学习转变为主动学习，突出学生能力的培养，更加符合职业技术教育的特点和规律。《计算机应用基础》适合作为普通高等院校和高职高专院校“计算机应用基础”课程的教材，也可作为计算机初学者的入门参考书。\", \"price\" : 29 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"298\", \"_score\" : 3.8557193, \"_source\" : { \"name\" : \"高等学校大学计算机基础课程系列教材：大学计算机基础教程\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"\", \"info\" : \"本书作为高等学校非计算机专业计算机基础课程群（1+X）第一门课程的主教材，主要介绍计算机基础知识和应用技能。本书共包含7章：计算机基础知识、操作系统用户界面及使用、办公自动化软件及使用、多媒体技术基础及应用、网络技术基础及应用、网页设计与制作、数据库技术基础及应用。每章都结合通用的软件版本进行讲解，同时为了帮助学生加深对所学知识的理解，还配备了大量习题。作为国家精品课程主讲教材，本书配有丰富的教学资源，包括多媒体教学课件、课程实验系统、上机练习和考试评价系统、教学素材等计算机辅助教学软件，还有功能完善的教学专用网站。本书可作为高等学校学生学习第一门计算机课程的教材，也可作为计算机爱好者的自学读本。\", \"price\" : 22 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"789\", \"_score\" : 3.58548, \"_source\" : { \"name\" : \"高等学校文科计算机课程系列教材：计算机平面设计技术（CorelDRAW与Photoshop）（附光盘）\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"\", \"info\" : \"《计算机平面设计技术――CDRAW与Ps》是根据教育部高等学校文科类专业计算机课程教学基本要求（美术类）而编写的。《计算机平面设计技术――CDRAW与Ps》系统全面地介绍了平面设计艺术专业最常用、最基本、最普及、最成熟的两个应用软件――CDRAW与Ps，结合实际的设计案例，讲授平面图形的造型方法与技巧、图像的处理方法与技巧、跨程序编辑的方法和意义等知识与技术。教材浓缩了其重要的工具和功能的使用方法，可作为平面设计专业的计算机应用基础课程的教材，也可作为学习计算机平面设计技术的参考。\", \"price\" : 28 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"282\", \"_score\" : 3.4969957, \"_source\" : { \"name\" : \"计算机文化基础（Windows XP＋Office2003）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机理论、基础知识\", \"author\" : \"赵秀英\", \"info\" : \"《计算机文化基础(WnwsXP+Offc2003\", \"price\" : 0 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"783\", \"_score\" : 3.3814218, \"_source\" : { \"name\" : \"计算机科学与技术丛书：现代语音编码技术\", \"publish\" : \"科学出版社\", \"type\" : \"电子与通信\", \"author\" : \"\", \"info\" : \"本书全面、系统地阐述了现代语音编码的原理、技术及应用。 本书分为12章，第一章介绍现代语音编码的基础知识和基本概念，第二章阐述标量量化和矢量量化原理，这两章是语音编码的入门知识；第三章至第七章讨论广泛应用的几种重要的现代语音编码技术原理、系统组成、实用技术及编码标准，是本书的重点；第八章至第十章介绍目前语音编码的最新的一些研究课题及其进展；第十一章、第十二章讨论语音编码的今后发展趋势和方向。主要内容有：语音编码导论、量化原理、时域波形编码技术、频域波形编码技术、变换域波形编码技术、参数编码技术、混合编码技术、极低速率语音编码、宽频带高音质声频编码、第三代移动通信的语音编码、信源-信道联合编码、软件无线电技术在语音编码中的应用。 本书内容丰富、取材新颖、阐述清晰、结构合理、实用性强，包含有最近二十几年来现代语音编码技术的许多新成果和新进展，是一本很好的关于语音编码原理、技术及应用的教科书和参考书。\", \"price\" : 31 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"465\", \"_score\" : 3.3506408, \"_source\" : { \"name\" : \"国家精品课程主讲教材・高等学校大学计算机基础课程系列教材：大学计算机基础实践教程\", \"publish\" : \"高等教育出版社\", \"type\" : \"大学教材\", \"author\" : \"詹国华\", \"info\" : \"《大学计算机基础实践教程》作为高等学校非计算机专业计算机基础课程群（1+X）第一门课程主教材《大学计算机基础教程》的配套实验教材，精心设计了一组综合性、应用性实验，注重实践技能的培养和理论知识的渗透。《大学计算机基础实践教程》共分7章：硬件连接和汉字输入实验、Wnws操作实验、办公软件操作实验、多媒体基础实验、因特网操作实验、网页设计与制作实验、Accss数据库操作实验。作为国家精品课程主讲教材的配套用书，《大学计算机基础实践教程》提供了丰富的教学资源，包括多媒体教学课件、课程实验系统、上机练习和考试评价系统、教学素材等计算机辅助教学软件，还有功能完善的课程教学网站。《大学计算机基础实践教程》可作为高等学校学生学习第一门计算机基础课程的配套教材，也可作为计算机爱好者的自学读本。\", \"price\" : 17 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"429\", \"_score\" : 3.1717708, \"_source\" : { \"name\" : \"微型计算机原理与接口技术（第2版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机组织与体系结构\", \"author\" : \"尹建华\", \"info\" : \"《微型计算机原理与接口技术》以In系列微处理器为背景，以16位微处理器8086为核心，追踪In主流系列高性能微型计算机萼术的发展方向，全面讲述微型计算机系统的基本组成、工作原理、硬件接口技术和典型应用，在此基础上介绍80386，80486和Pn等高档微处理器的发展和特点。使学生系统掌握汇编语言程序设计的基本方法和硬件接口技术，建立微型计算机系统的整体概念，并且使之具有微型计算机软件及硬件初步开发、设计的能力。为使于教师授课和学生学习，《微型计算机原理与接口技术》配备了多媒体CAI课件。全书共11章。主要内容包括：微型计算机基础知识、80x86CPU、微型计算机指令系统、汇编语言程序设计、存储器及其与CPU的接口、输入／输出接口及中断技术、总线和总线标准、常用可编程并行数字接口芯片及其应用、串行通信接口及总线标准、模拟接口技术、常用外设和人机交互接口。《微型计算机原理与接口技术》可作为高等学校工科非计算机专业微型计算机原理及应用课程的教材，也可供从事微型计算机硬件和软件设计的工程技术人员参考。\", \"price\" : 54 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"70\", \"_score\" : 2.9018917, \"_source\" : { \"name\" : \"计算机基础课程系列教材：数据库技术及应用 SQL Server\", \"publish\" : \"高等教育出版社\", \"type\" : \"数据库\", \"author\" : \"李雁翎\", \"info\" : \"SQLSv是一种典型的数据库管理系统，是目前深受广大用户欢迎的数据库应用开发平台。它适应网络技术环境，支持客户／服务器模式，能够满足创建各种类型数据库的需求，因此是目前高等学校讲授大型数据库管理系统的首选软件平台。《数据库技术及应用――SQLSv》以培养学生利用数据库技术对数据和信息进行管理、加工和利用的意识与能力为目标，以数据库原理和技术为知识讲授核心，建构教材的体例。《数据库技术及应用――SQLSv》分为上、下两篇。上篇为基础篇，主要介绍与数据库相关的基本概念，数据库设计方法，SQLSv-体系结构，数据库对象管理，T―SQL语言及应用，存储过程和触发器的使用技术。下篇为应用篇，主要介绍安全管理技术，数据备份、恢复及转换技术，ADO数据对象，VB／SQLSv的应用程序开发方法及实例。《数据库技术及应用――SQLSv》体系完整，结构清晰，实例丰富，图文并茂，精编精讲，易读易懂。全书体例创新，由一组系统化的、围绕一个数据库应用系统的相关例子贯穿始终，特色鲜明，具有普遍适用性。《数据库技术及应用――SQLSv》可作为高等学校本、专科学生的教科书，也可作为学习数据库应用技术读者的自学用书。为了方便教师教学和学生自主学习，《数据库技术及应用――SQLSv》配有《数据库技术及应用――习题与实验指导（SQLSv）》和电子教案、例题、实验软件的电子文档以及相关的教学网站，网址为：／／c.cncs.c／c／nx。\", \"price\" : 22 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"116\", \"_score\" : 2.9018917, \"_source\" : { \"name\" : \"全国计算机等级考试一级B教程（2010年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机考试\", \"author\" : \"\", \"info\" : \"《全国计算机等级考试一级B教程（2010年版）》由教育部考试中心组织，在全国计算机等级考试委员会指导下由有关专家按照《全国计算机等级考试一级B考试大纲（2007年版）》的要求而编写，内容包括计算机基础知识、WnwsXP操作系统、W2003的使用、Exc2003的使用、因特网的基础知识和简单应用等。由教育部考试中心组织和实施的计算机等级考试，是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国范围的等级考试。它面向社会，服务于社会。《全国计算机等级考试一级B教程（2010年版）》除了可以作为计算机等级考试的教材外，还可作为学习计算机基础知识的参考书。\", \"price\" : 33 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"138\", \"_score\" : 2.821856, \"_source\" : { \"name\" : \"全国计算机等级考试三级教程：PC技术（2010年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"考试认证\", \"author\" : \"\", \"info\" : \"《全国计算机等级考试三级教程：PC技术(2010年版\", \"price\" : 0 } } ] }} 123456GET books/_search{ \"query\": { \"match_all\": {} }} hits 中就是查询结果，total 是符合查询条件的文档数。 简单搜索可以简写为： 1GET books/_search 简单搜索默认查询 10 条记录。 14.2 词项查询即 term 查询，就是根据词去查询，查询指定字段中包含给定单词的文档，term 查询不被解析，只有搜索的词和文档中的词精确匹配，才会返回文档。应用场景如：人名、地名等等。 查询 name 字段中包含 十一五 的文档。 12345678GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }} 14.3 分页默认返回前 10 条数据，es 中也可以像关系型数据库一样，给一个分页参数： 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"size\": 10, \"from\": 10} 14.4 过滤返回字段如果返回的字段比较多，又不需要这么多字段，此时可以指定返回的字段： 1234567891011GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"size\": 10, \"from\": 10, \"_source\": [\"name\",\"author\"]} 此时，返回的字段就只有 name 和 author 了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119{ \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 142, \"relation\" : \"eq\" }, \"max_score\" : 1.7939949, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"323\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"徐春祥\", \"name\" : \"普通高等教育十一五国家级规划教材・医学化学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"476\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"陈后金，胡健，薛健\", \"name\" : \"普通高等教育“十一五”国家级规划教材：信号与系统\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"480\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"余家荣\", \"name\" : \"普通高等教育“十一五”国家级规划教材：复变函数\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"494\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"徐美银\", \"name\" : \"全国高职高专教育“十一五”规划教材：经济学原理\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"498\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：组合学讲义\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"502\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"卞毓宁\", \"name\" : \"全国高职高专教育十一五规划教材：统计学概论\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"614\", \"_score\" : 1.7416272, \"_source\" : { \"author\" : \"陈洪亮，张峰，田社平\", \"name\" : \"普通高等教育“十一五”国家级规划教材：电路基础\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"13\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"胡立勇，丁艳锋\", \"name\" : \"普通高等教育“十一五”国家级规划教材：作物栽培学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"65\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：有机化学\" } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"87\", \"_score\" : 1.69223, \"_source\" : { \"author\" : \"\", \"name\" : \"普通高等教育“十一五”国家级规划教材：美术设计基础\" } } ] }} 14.5 最小评分有的文档得分特别低，说明这个文档和我们查询的关键字相关度很低。我们可以设置一个最低分，只有得分超过最低分的文档才会被返回。 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"min_score\":1.75, \"_source\": [\"name\",\"author\"]} 得分低于 1.75 的文档将直接被舍弃。 14.6 高亮查询关键字高亮： 123456789101112131415GET books/_search{ \"query\": { \"term\": { \"name\": \"十一五\" } }, \"min_score\":1.75, \"_source\": [\"name\",\"author\"], \"highlight\": { \"fields\": { \"name\": {} } }} 15.ElasticSearch 全文查询15.1 match querymatch query 会对查询语句进行分词，分词后，如果查询语句中的任何一个词项被匹配，则文档就会被索引到。 12345678GET books/_search{ \"query\": { \"match\": { \"name\": \"美术计算机\" } }} 这个查询首先会对 美术计算机 进行分词，分词之后，再去查询，只要文档中包含一个分词结果，就回返回文档。换句话说，默认词项之间是 OR 的关系，如果想要修改，也可以改为 AND。 1234567891011GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"美术计算机\", \"operator\": \"and\" } } }} 此时就会要求文档中必须同时包含 美术 和 计算机 两个词。 15.2 match_phrase querymatch_phrase query 也会对查询的关键字进行分词，但是它分词后有两个特点： 分词后的词项顺序必须和文档中词项的顺序一致 所有的词都必须出现在文档中 示例如下： 1234567891011GET books/_search{ \"query\": { \"match_phrase\": { \"name\": { \"query\": \"十一五计算机\", \"slop\": 7 } } }} query 是查询的关键字，会被分词器进行分解，分解之后去倒排索引中进行匹配。 slop 是指关键字之间的最小距离，但是注意不是关键字之间间隔的字数。文档中的字段被分词器解析之后，解析出来的词项都包含一个 position 字段表示词项的位置，查询短语分词之后 的 position 之间的间隔要满足 slop 的要求。 15.3 match_phrase_prefix query这个类似于 match_phrase query，只不过这里多了一个通配符，match_phrase_prefix 支持最后一个词项的前缀匹配，但是由于这种匹配方式效率较低，因此大家作为了解即可。 12345678GET books/_search{ \"query\": { \"match_phrase_prefix\": { \"name\": \"计\" } }} 这个查询过程，会自动进行单词匹配，会自动查找以计开始的单词，默认是 50 个，可以自己控制： 1234567891011GET books/_search{ \"query\": { \"match_phrase_prefix\": { \"name\": { \"query\": \"计\", \"max_expansions\": 3 } } }} match_phrase_prefix 是针对分片级别的查询，假设 max_expansions 为 1，可能返回多个文档，但是只有一个词，这是我们预期的结果。有的时候实际返回结果和我们预期结果并不一致，原因在于这个查询是分片级别的，不同的分片确实只返回了一个词，但是结果可能来自不同的分片，所以最终会看到多个词。 15.4 multi_match querymatch 查询的升级版，可以指定多个查询域： 123456789GET books/_search{ \"query\": { \"multi_match\": { \"query\": \"java\", \"fields\": [\"name\",\"info\"] } }} 这种查询方式还可以指定字段的权重： 123456789GET books/_search{ \"query\": { \"multi_match\": { \"query\": \"阳光\", \"fields\": [\"name^4\",\"info\"] } }} 这个表示关键字出现在 name 中的权重是出现在 info 中权重的 4 倍。 15.5 query_string queryquery_string 是一种紧密结合 Lucene 的查询方式，在一个查询语句中可以用到 Lucene 的一些查询语法： 123456789GET books/_search{ \"query\": { \"query_string\": { \"default_field\": \"name\", \"query\": \"(十一五) AND (计算机)\" } }} 15.6 simple_query_string这个是 query_string 的升级，可以直接使用 +、|、- 代替 AND、OR、NOT 等。 123456789GET books/_search{ \"query\": { \"simple_query_string\": { \"fields\": [\"name\"], \"query\": \"(十一五) + (计算机)\" } }} 查询结果和 query_string。 16.ElasticSearch词项查询16.1 term query词项查询。词项查询不会分析查询字符，直接拿查询字符去倒排索引中比对。 12345678GET books/_search{ \"query\": { \"term\": { \"name\": \"程序设计\" } }} 16.2 terms query词项查询，但是可以给多个关键词。 12345678GET books/_search{ \"query\": { \"terms\": { \"name\": [\"程序\",\"设计\",\"java\"] } }} 16.3 range query范围查询，可以按照日期范围、数字范围等查询。 range query 中的参数主要有四个： gt lt gte lte 案例： 123456789101112131415161718GET books/_search{ \"query\": { \"range\": { \"price\": { \"gte\": 10, \"lt\": 20 } } }, \"sort\": [ { \"price\": { \"order\": \"desc\" } } ]} 16.4 exists queryexists query 会返回指定字段中至少有一个非空值的文档： 12345678GET books/_search{ \"query\": { \"exists\": { \"field\": \"javaboy\" } }} 注意，空字符串也是有值。null 是空值。 16.5 prefix query前缀查询，效率略低，除非必要，一般不太建议使用。 给定关键词的前缀去查询： 12345678910GET books/_search{ \"query\": { \"prefix\": { \"name\": { \"value\": \"大学\" } } }} 16.6 wildcard querywildcard query 即通配符查询。支持单字符和多字符通配符： ？表示一个任意字符。 * 表示零个或者多个字符。 查询所有姓张的作者的书： 12345678910GET books/_search{ \"query\": { \"wildcard\": { \"author\": { \"value\": \"张*\" } } }} 查询所有姓张并且名字只有两个字的作者的书： 12345678910GET books/_search{ \"query\": { \"wildcard\": { \"author\": { \"value\": \"张?\" } } }} 16.7 regexp query支持正则表达式查询。 查询所有姓张并且名字只有两个字的作者的书： 12345678GET books/_search{ \"query\": { \"regexp\": { \"author\": \"张.\" } }} 16.8 fuzzy query在实际搜索中，有时我们可能会打错字，从而导致搜索不到，在 match query 中，可以通过 fuzziness 属性实现模糊查询。 fuzzy query 返回与搜索关键字相似的文档。怎么样就算相似？以LevenShtein 编辑距离为准。编辑距离是指将一个字符变为另一个字符所需要更改字符的次数，更改主要包括四种： 更改字符（javb–〉java） 删除字符（javva–〉java） 插入字符（jaa–〉java） 转置字符（ajva–〉java） 为了找到相似的词，模糊查询会在指定的编辑距离中创建搜索关键词的所有可能变化或者扩展的集合，然后进行搜索匹配。 12345678GET books/_search{ \"query\": { \"fuzzy\": { \"name\": \"javba\" } }} 16.9 ids query根据指定的 id 查询。 12345678GET books/_search{ \"query\": { \"ids\":{ \"values\": [1,2,3] } }} 17.ElasticSearch 复合查询17.1 constant_score query当我们不关心检索词项的频率（TF）对搜索结果排序的影响时，可以使用 constant_score 将查询语句或者过滤语句包裹起来。一般来说词项出现次数多会靠前？ 12345678910111213GET books/_search{ \"query\": { \"constant_score\": { \"filter\": { \"term\": { \"name\": \"java\" } }, \"boost\": 1.5 } }} 17.2 bool querybool query 可以将任意多个简单查询组装在一起，有四个关键字可供选择，四个关键字所描述的条件可以有一个或者多个。 must：文档必须匹配 must 选项下的查询条件。 should：文档可以匹配 should 下的查询条件，也可以不匹配。 must_not：文档必须不满足 must_not 选项下的查询条件。 filter：类似于 must，但是 filter 不评分，只是过滤数据。 例如查询 name 属性中必须包含 java，同时书价不在 [0,35] 区间内，info 属性可以包含 程序设计 也可以不包含程序 设计： 123456789101112131415161718192021222324252627282930313233GET books/_search{ \"query\": { \"bool\": { \"must\": [ { \"term\": { \"name\": { \"value\": \"java\" } } } ], \"must_not\": [ { \"range\": { \"price\": { \"gte\": 0, \"lte\": 35 } } } ], \"should\": [ { \"match\": { \"info\": \"程序设计\" } } ] } }} 这里还涉及到一个关键字，minmum_should_match 参数。 minmum_should_match 参数在 es 官网上称作最小匹配度。在之前学习的 multi_match 或者这里的 should 查询中，都可以设置 minmum_should_match 参数。 假设我们要做一次查询，查询 name 中包含 语言程序设计 关键字的文档： 12345678GET books/_search{ \"query\": { \"match\": { \"name\": \"语言程序设计\" } }} 在这个查询过程中，首先会进行分词，分词方式如下： 12345GET books/_analyze{ \"text\": [\"语言程序设计\"], \"analyzer\": \"ik_max_word\"} 分词后的 term 会构造成一个 should 的 bool query，每一个 term 都会变成一个 term query 的子句。换句话说，上面的查询和下面的查询等价： 12345678910111213141516171819202122232425262728293031323334353637GET books/_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"name\": { \"value\": \"语言\" } } }, { \"term\": { \"name\": { \"value\": \"程序设计\" } } }, { \"term\": { \"name\": { \"value\": \"程序\" } } }, { \"term\": { \"name\": { \"value\": \"设计\" } } } ] } }} 在这两个查询语句中，都是文档只需要包含词项中的任意一项即可，文档就回被返回，在 match 查询中，可以通过 operator 参数设置文档必须匹配所有词项。 如果想匹配一部分词项，就涉及到一个参数，就是 minmum_should_match，即最小匹配度。即至少匹配多少个词。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"operator\": \"and\" } } }}GET books/_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"name\": { \"value\": \"语言\" } } }, { \"term\": { \"name\": { \"value\": \"程序设计\" } } }, { \"term\": { \"name\": { \"value\": \"程序\" } } }, { \"term\": { \"name\": { \"value\": \"设计\" } } } ], \"minimum_should_match\": \"50%\" } }, \"from\": 0, \"size\": 70} 50% 表示词项个数的 50%。 如下两个查询等价（参数 4 是因为查询关键字分词后有 4 项）： 12345678910111213141516171819202122GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"minimum_should_match\": 4 } } }}GET books/_search{ \"query\": { \"match\": { \"name\": { \"query\": \"语言程序设计\", \"operator\": \"and\" } } }} 17.3 dis_max query假设现在有两本书： 123456789101112131415161718192021222324252627PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"content\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } }}POST blog/_doc{ \"title\":\"如何通过Java代码调用ElasticSearch\", \"content\":\"松哥力荐，这是一篇很好的解决方案\"}POST blog/_doc{ \"title\":\"初识 MongoDB\", \"content\":\"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\"} 现在假设搜索 Java解决方案 关键字，但是不确定关键字是在 title 还是在 content，所以两者都搜索： 12345678910111213141516171819GET blog/_search{ \"query\": { \"bool\": { \"should\": [ { \"match\": { \"title\": \"java解决方案\" } }, { \"match\": { \"content\": \"java解决方案\" } } ] } }} 搜索结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 882, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.1972204, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HhK653gBeADYd85qxnL3\", \"_score\" : 1.1972204, \"_source\" : { \"title\" : \"如何通过Java代码调用ElasticSearch\", \"content\" : \"松哥力荐，这是一篇很好的解决方案\" } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HxK753gBeADYd85qCnLy\", \"_score\" : 1.1069256, \"_source\" : { \"title\" : \"初识 MongoDB\", \"content\" : \"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\" } } ] }} 要理解这个原因，我们需要来看下 should query 中的评分策略： 首先会执行 should 中的两个查询 对两个查询结果的评分求和 对求和结果乘以匹配语句总数 在对第三步的结果除以所有语句总数 反映到具体的查询中： 前者 title 中 包含 java，假设评分是 1.1 content 中包含解决方案，假设评分是 1.2 有得分的 query 数量，这里是 2 总的 query 数量也是 2 最终结果：（1.1+1.2）*2/2=2.3 后者 title 中 不包含查询关键字，没有得分 content 中包含解决方案和 java，假设评分是 2 有得分的 query 数量，这里是 1 总的 query 数量也是 2 最终结果：2*1/2=1 在这种查询中，title 和 content 相当于是相互竞争的关系，所以我们需要找到一个最佳匹配字段。 为了解决这一问题，就需要用到 dis_max query（disjunction max query，分离最大化查询）：匹配的文档依然返回，但是只将最佳匹配的评分作为查询的评分。 12345678910111213141516171819GET blog/_search{ \"query\": { \"dis_max\": { \"queries\": [ { \"match\": { \"title\": \"java解决方案\" } }, { \"match\": { \"content\": \"java解决方案\" } } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 14, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 1.1069256, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HxK753gBeADYd85qCnLy\", \"_score\" : 1.1069256, \"_source\" : { \"title\" : \"初识 MongoDB\", \"content\" : \"简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案\" } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"HhK653gBeADYd85qxnL3\", \"_score\" : 0.62177753, \"_source\" : { \"title\" : \"如何通过Java代码调用ElasticSearch\", \"content\" : \"松哥力荐，这是一篇很好的解决方案\" } } ] }} 在 dis_max query 中，还有一个参数 tie_breaker（取值在0～1），在 dis_max query 中，是完全不考虑其他 query 的分数，只是将最佳匹配的字段的评分返回。但是，有的时候，我们又不得不考虑一下其他 query 的分数，此时，可以通过 tie_breaker 来优化 dis_max query。tie_breaker 会将其他 query 的分数，乘以 tie_breaker，然后和分数最高的 query 进行一个综合计算。 17.4 function_score query场景：例如想要搜索附近的肯德基，搜索的关键字是肯德基，但是我希望能够将评分较高的肯德基优先展示出来。但是默认的评分策略是没有办法考虑到餐厅评分的，他只是考虑相关性，这个时候可以通过 function_score query 来实现。 准备两条测试数据： 1234567891011121314151617181920212223242526PUT blog{ \"mappings\": { \"properties\": { \"title\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"votes\":{ \"type\": \"integer\" } } }}PUT blog/_doc/1{ \"title\":\"Java集合详解\", \"votes\":100}PUT blog/_doc/2{ \"title\":\"Java多线程详解，Java锁详解\", \"votes\":10} 现在搜索标题中包含 java 关键字的文档： 12345678GET blog/_search{ \"query\": { \"match\": { \"title\": \"java\" } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.22534126, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 0.22534126, \"_source\" : { \"title\" : \"Java多线程详解，Java锁详解\", \"votes\" : 10 } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.21799318, \"_source\" : { \"title\" : \"Java集合详解\", \"votes\" : 100 } } ] }} 默认情况下，id 为 2 的记录得分较高，因为他的 title 中包含两个 java。 如果我们在查询中，希望能够充分考虑 votes 字段，将 votes 较高的文档优先展示，就可以通过 function_score 来实现。 具体的思路，就是在旧的得分基础上，根据 votes 的数值进行综合运算，重新得出一个新的评分。 具体有几种不同的计算方式： weight random_score script_score field_value_factor weight weight 可以对评分设置权重，就是在旧的评分基础上乘以 weight，他其实无法解决我们上面所说的问题。具体用法如下： 1234567891011121314151617GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"weight\": 10 } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ \"took\" : 11, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 2.2534127, \"hits\" : [ { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_score\" : 2.2534127, \"_source\" : { \"title\" : \"Java多线程详解，Java锁详解\", \"votes\" : 10 } }, { \"_index\" : \"blog\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 2.1799319, \"_source\" : { \"title\" : \"Java集合详解\", \"votes\" : 100 } } ] }} 可以看到，此时的评分，在之前的评分基础上*10 random_score random_score 会根据 uid 字段进行 hash 运算，生成分数，使用 random_score 时可以配置一个种子，如果不配置，默认使用当前时间。 1234567891011121314151617GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"random_score\": {} } ] } }} script_score 自定义评分脚本。假设每个文档的最终得分是旧的分数加上votes。查询方式如下： 12345678910111213141516171819202122GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"script_score\": { \"script\": { \"lang\": \"painless\", \"source\": \"_score + doc['votes'].value\" } } } ] } }} 现在，最终得分是 (oldScore+votes)*oldScore。 多了， 都会提示错误 如果不想乘以 oldScore，查询方式如下： 1234567891011121314151617181920212223GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"script_score\": { \"script\": { \"lang\": \"painless\", \"source\": \"_score + doc['votes'].value\" } } } ], \"boost_mode\": \"replace\" } }} 通过 boost_mode 参数，可以设置最终的计算方式。该参数还有其他取值： multiply：分数相乘 sum：分数相加 avg：求平均数 max：最大分 min：最小分 replace：不进行二次计算 field_value_factor 这个的功能类似于 script_score，但是不用自己写脚本。 假设每个文档的最终得分是旧的分数乘以votes。查询方式如下： 12345678910111213141516171819GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\" } } ] } }} 默认的得分就是oldScore*votes。 还可以利用 es 内置的函数进行一些更复杂的运算： 123456789101112131415161718192021GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\", \"modifier\": \"sqrt\" } } ], \"boost_mode\": \"replace\" } }} 此时，最终的得分是（sqrt(votes)）。 modifier 中可以设置内置函数，其他的内置函数还有： 参数名 含义 none 默认的，不进行任何计算 log 对字段值取对数 log1p 字段值+1 然后取对数 log2p 字段值+2 然后取对数 ln 取字段值的自然对数 ln1p 字段值+1 然后取自然对数 ln2p 字段值+2 然后取自然对数 sqrt 字段值求平方根 square 字段值的平方 reciprocal 倒数 另外还有个参数 factor ，影响因子。字段值先乘以影响因子，然后再进行计算。以 sqrt 为例，计算方式为 sqrt(factor*votes)： 12345678910111213141516171819202122GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\", \"modifier\": \"sqrt\", \"factor\": 10 } } ], \"boost_mode\": \"replace\" } }} 还有一个参数 max_boost，控制计算结果的范围： 123456789101112131415161718192021GET blog/_search{ \"query\": { \"function_score\": { \"query\": { \"match\": { \"title\": \"java\" } }, \"functions\": [ { \"field_value_factor\": { \"field\": \"votes\" } } ], \"boost_mode\": \"sum\", \"max_boost\": 100 } }} max_boost 参数表示 functions 模块中，最终的计算结果上限。如果超过上限，就按照上线计算。 17.5 boosting queryboosting query 中包含三部分： positive：得分不变 negative：降低得分 negative_boost：降低的权重 123456789101112131415161718GET books/_search{ \"query\": { \"boosting\": { \"positive\": { \"match\": { \"name\": \"java\" } }, \"negative\": { \"match\": { \"name\": \"2008\" } }, \"negative_boost\": 0.5 } }} 查询结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647{ \"took\" : 15, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 4.5299835, \"hits\" : [ { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"549\", \"_score\" : 4.5299835, \"_source\" : { \"name\" : \"全国计算机等级考试笔试＋上机全真模拟：二级Java语言程序设计（最新版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"考试认证\", \"author\" : \"\", \"info\" : \"为了更好地服务于考生，引导考生尽快掌握考试大纲中要求的知识点和技能，顺利通过计算机等级考试，根据最新的考试大纲，高等教育出版社组织长期从事计算机等级考试命题研究和培训工作的专家编写了这套“笔试+上机考试全真模拟”，全面模拟考试真题，让考生在做题的同时全面巩固复习考点，提前熟悉考试环境，在短时间内冲刺过关。本书内容包括20套笔试模拟题和20套上机模拟题，还给出了参考答案和解析，尤其适合参加计算机等级考试的考生考前实战演练。\", \"price\" : 30 } }, { \"_index\" : \"books\", \"_type\" : \"_doc\", \"_id\" : \"86\", \"_score\" : 2.5018067, \"_source\" : { \"name\" : \"全国计算机等级考试2级教程：Java语言程序设计（2008年版）\", \"publish\" : \"高等教育出版社\", \"type\" : \"计算机考试\", \"author\" : \"\", \"info\" : \"由国家教育部考试中心推出的计算机等级考试是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国性考试，它面向社会，服务于社会。本书在教育部考试中心组织下、在全国计算机等级考试委员会指导下，由有关专家执笔编写而成。本书按照《全国计算机等级考试二级Java语言程序设计考试大纲（2007年版）》的要求编写，内容包括：Java体系结构、基本数据类型、流程控制语句、类、数组和字符串操作、输入输出及文件操作、图形用户界面编写、线程和串行化技术、A程序设计以及应用开发工具和安装使用等。本书是参加全国计算机等级考试二级Java语言程序设计的考生的良师益友，是教育部考试中心指定教材，也可作为欲学习Java编程的读者的参考书。\", \"price\" : 37 } } ] }} 可以看到，id 为 86 的文档满足条件，因此它的最终得分在旧的分数上*0.5。 18.ElasticSearch 嵌套查询关系型数据库中有表的关联关系，在 es 中，我们也有类似的需求，例如订单表和商品表，在 es 中，这样的一对多一般来说有两种方式： 嵌套文档（nested） 父子文档 18.1 嵌套文档假设：有一个电影文档，每个电影都有演员信息： 12345678910111213141516171819202122232425PUT movies{ \"mappings\": { \"properties\": { \"actors\":{ \"type\": \"nested\" } } }}PUT movies/_doc/1{ \"name\":\"霸王别姬\", \"actors\":[ { \"name\":\"张国荣\", \"gender\":\"男\" }, { \"name\":\"巩俐\", \"gender\":\"女\" } ]} 注意 actors 类型要是 nested，具体原因参考 10.2.3 小节。 缺点 查看文档数量： 1GET _cat/indices?v 查看结果如下： 添加了1个文档，显示存了3个 这是因为 nested 文档在 es 内部其实也是独立的 lucene 文档，只是在我们查询的时候，es 内部帮我们做了 join 处理，所以最终看起来就像一个独立文档一样。因此这种方案性能并不是特别好。 18.2 嵌套查询这个用来查询嵌套文档： 123456789101112131415161718192021222324GET movies/_search{ \"query\": { \"nested\": { \"path\": \"actors\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"actors.name\": \"张国荣\" } }, { \"match\": { \"actors.gender\": \"男\" } } ] } } } }} 18.3 父子文档相比于嵌套文档，父子文档主要有如下优势： 更新父文档时，不会重新索引子文档 创建、修改或者删除子文档时，不会影响父文档或者其他的子文档。 子文档可以作为搜索结果独立返回。 例如学生和班级的关系： 12345678910111213141516PUT stu_class{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"s_c\":{ \"type\": \"join\", \"relations\":{ \"class\":\"student\" } } } }} s_c 表示父子文档关系的名字，可以自定义。join 表示这是一个父子文档。relations 里边，class 这个位置是 parent，student 这个位置是 child。 接下来，插入两个父文档： 1234567891011121314PUT stu_class/_doc/1{ \"name\":\"一班\", \"s_c\":{ \"name\":\"class\" }}PUT stu_class/_doc/2{ \"name\":\"二班\", \"s_c\":{ \"name\":\"class\" }} 再来添加三个子文档： 123456789101112131415161718192021222324PUT stu_class/_doc/3?routing=1{ \"name\":\"zhangsan\", \"s_c\":{ \"name\":\"student\", \"parent\":1 }}PUT stu_class/_doc/4?routing=1{ \"name\":\"lisi\", \"s_c\":{ \"name\":\"student\", \"parent\":1 }}PUT stu_class/_doc/5?routing=2{ \"name\":\"wangwu\", \"s_c\":{ \"name\":\"student\", \"parent\":2 }} 首先大家可以看到，子文档都是独立的文档。特别需要注意的地方是，子文档需要和父文档在同一个分片上，所以 routing 关键字的值为父文档的 id。另外，name 属性表明这是一个子文档。 父子文档需要注意的地方： 每个索引只能定义一个 join filed 父子文档需要在同一个分片上（查询，修改需要routing） 可以向一个已经存在的 join filed 上新增关系 18.4 has_child query通过子文档查询父文档使用 has_child query。 12345678910111213GET stu_class/_search{ \"query\": { \"has_child\": { \"type\": \"student\", \"query\": { \"match\": { \"name\": \"wangwu\" } } } }} 查询 wangwu 所属的班级。 18.5 has_parent query通过父文档查询子文档： 12345678910111213GET stu_class/_search{ \"query\": { \"has_parent\": { \"parent_type\": \"class\", \"query\": { \"match\": { \"name\": \"二班\" } } } }} 查询二班的学生。但是大家注意，这种查询没有评分。 可以使用 parent id 查询子文档： 123456789GET stu_class/_search{ \"query\": { \"parent_id\":{ \"type\":\"student\", \"id\":1 } }} 通过 parent id 查询，默认情况下使用相关性计算分数。 18.6 小结整体上来说： 普通子对象实现一对多，会损失子文档的边界，子对象之间的属性关系丢失。 nested 可以解决第 1 点的问题，但是 nested 有两个缺点：更新主文档的时候要全部更新，不支持子文档属于多个主文档。 父子文档解决 1、2 点的问题，但是它主要适用于写多读少的场景。 19.ElasticSearch 地理位置查询19.1 数据准备创建一个索引： 12345678910111213PUT geo{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"location\":{ \"type\": \"geo_point\" } } }} 准备一个 geo.json 文件： 12345678910111213141516{\"index\":{\"_index\":\"geo\",\"_id\":1}}{\"name\":\"西安\",\"location\":\"34.288991865037524,108.9404296875\"}{\"index\":{\"_index\":\"geo\",\"_id\":2}}{\"name\":\"北京\",\"location\":\"39.926588421909436,116.43310546875\"}{\"index\":{\"_index\":\"geo\",\"_id\":3}}{\"name\":\"上海\",\"location\":\"31.240985378021307,121.53076171875\"}{\"index\":{\"_index\":\"geo\",\"_id\":4}}{\"name\":\"天津\",\"location\":\"39.13006024213511,117.20214843749999\"}{\"index\":{\"_index\":\"geo\",\"_id\":5}}{\"name\":\"杭州\",\"location\":\"30.259067203213018,120.21240234375001\"}{\"index\":{\"_index\":\"geo\",\"_id\":6}}{\"name\":\"武汉\",\"location\":\"30.581179257386985,114.3017578125\"}{\"index\":{\"_index\":\"geo\",\"_id\":7}}{\"name\":\"合肥\",\"location\":\"31.840232667909365,117.20214843749999\"}{\"index\":{\"_index\":\"geo\",\"_id\":8}}{\"name\":\"重庆\",\"location\":\"29.592565403314087,106.5673828125\"} 最后，执行如下命令，批量导入 geo.json 数据： 1curl -XPOST \"http://localhost:9200/geo/_bulk?pretty\" -H \"content-type:application/json\" --data-binary @geo.json 可能用到的工具网站： http://geojson.io/#map=6/32.741/116.521 19.2 geo_distance query给出一个中心点，查询距离该中心点指定范围内的文档： 1234567891011121314151617181920212223GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_distance\": { \"distance\": \"600km\", \"location\": { \"lat\": 34.288991865037524, \"lon\": 108.9404296875 } } } ] } }} 以(34.288991865037524,108.9404296875) 为圆心，以 600KM 为半径，这个范围内的数据。 19.3 geo_bounding_box query在某一个矩形内的点，通过两个点锁定一个矩形： 12345678910111213141516171819202122232425262728GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_bounding_box\": { \"location\": { \"top_left\": { \"lat\": 32.0639555946604, \"lon\": 118.78967285156249 }, \"bottom_right\": { \"lat\": 29.98824461550903, \"lon\": 122.20642089843749 } } } } ] } }} 以南京经纬度作为矩形的左上角，以舟山经纬度作为矩形的右下角，构造出来的矩形中，包含上海和杭州两个城市。 19.4 geo_polygon query在某一个多边形范围内的查询。 12345678910111213141516171819202122232425262728293031323334GET geo/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_polygon\": { \"location\": { \"points\": [ { \"lat\": 31.793755581217674, \"lon\": 113.8238525390625 }, { \"lat\": 30.007273923504556, \"lon\":114.224853515625 }, { \"lat\": 30.007273923504556, \"lon\":114.8345947265625 } ] } } } ] } }} 给定多个点，由多个点组成的多边形中的数据。 es7.12出现提示 #! Deprecated field [geo_polygon] used, replaced by [[geo_shape] query where polygons are defined in geojson or wkt] 19.5 geo_shape querygeo_shape 用来查询图形，针对 geo_shape，两个图形之间的关系有：相交、包含、不相交。 新建索引： 12345678910111213PUT geo_shape{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"keyword\" }, \"location\":{ \"type\": \"geo_shape\" } } }} 然后添加一条线： 1234567891011PUT geo_shape/_doc/1{ \"name\":\"西安-郑州\", \"location\":{ \"type\":\"linestring\", \"coordinates\":[ [108.9404296875,34.279914398549934], [113.66455078125,34.768691457552706] ] }} 接下来查询某一个图形中是否包含该线： 12345678910111213141516171819202122232425262728293031323334GET geo_shape/_search{ \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"geo_shape\": { \"location\": { \"shape\": { \"type\": \"envelope\", \"coordinates\": [ [ 106.5234375, 36.80928470205937 ], [ 115.33447265625, 32.24997445586331 ] ] }, \"relation\": \"within\" } } } ] } }} relation 属性表示两个图形的关系： within 包含 intersects 相交 disjoint 不相交 20.ElasticSearch 特殊查询20.1 more_like_this querymore_like_this query 可以实现基于内容的推荐，给定一篇文章，可以查询出和该文章相似的内容。 12345678910111213GET books/_search{ \"query\": { \"more_like_this\": { \"fields\": [ \"info\" ], \"like\": \"大学战略\", \"min_term_freq\": 1, \"max_query_terms\": 12 } }} fields：要匹配的字段，可以有多个 like：要匹配的文本 min_term_freq：词项的最低频率，默认是 2。特别注意，这个是指词项在要匹配的文本中的频率，而不是 es 文档中的频率 max_query_terms：query 中包含的最大词项数目 min_doc_freq：最小的文档频率，搜索的词，至少在多少个文档中出现，少于指定数目，该词会被忽略 max_doc_freq：最大文档频率 analyzer：分词器，默认使用字段的分词器 stop_words：停用词列表 minmum_should_match 20.2 script query脚本查询，例如查询所有价格大于 200 的图书： 1234567891011121314151617GET books/_search{ \"query\": { \"bool\": { \"filter\": [ { \"script\": { \"script\": { \"lang\": \"painless\", \"source\": \"if(doc['price'].size()!=0){doc['price'].value &gt; 200}\" } } } ] } }} 20.3 percolate querypercolate query 译作渗透查询或者反向查询。 正常操作：根据查询语句找到对应的文档 query-&gt;document percolate query：根据文档，返回与之匹配的查询语句，document-&gt;query 应用场景： 价格监控 库存报警 股票警告 … 例如阈值告警，假设指定字段值大于阈值，报警提示。 percolate mapping 定义： 12345678910111213141516PUT log{ \"mappings\": { \"properties\": { \"threshold\":{ \"type\": \"long\" }, \"count\":{ \"type\": \"long\" }, \"query\":{ \"type\":\"percolator\" } } }} percolator 类型相当于 keyword、long 以及 integer 等。 插入文档： 123456789101112131415PUT log/_doc/1{ \"threshold\":10, \"query\":{ \"bool\":{ \"must\":{ \"range\":{ \"count\":{ \"gt\":10 } } } } }} 最后查询： 12345678910111213141516171819202122232425GET log/_search{ \"query\": { \"percolate\": { \"field\": \"query\", \"documents\": [ { \"count\":3 }, { \"count\":6 }, { \"count\":90 }, { \"count\":12 }, { \"count\":15 } ] } }} 查询结果中会列出不满足条件的文档。 查询结果中的 _percolator_document_slot 字段表示文档的 position，从 0 开始计。 21.ElasticSearch 搜索高亮与排序21.1 搜索高亮普通高亮，默认会自动添加 em 标签： 12345678910111213GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"fields\": { \"name\": {} } }} 正常来说，我们见到的高亮可能是红色、黄色之类的。 可以自定义高亮标签： 12345678910111213141516GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"fields\": { \"name\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] } } }} 有的时候，虽然我们是在 name 字段中搜索的，但是我们希望 info 字段中，相关的关键字也能高亮： 123456789101112131415161718192021GET books/_search{ \"query\": { \"match\": { \"name\": \"大学\" } }, \"highlight\": { \"require_field_match\": \"false\", \"fields\": { \"name\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] }, \"info\": { \"pre_tags\": [\"&lt;strong&gt;\"], \"post_tags\": [\"&lt;/strong&gt;\"] } } }} require_field_match By default, only fields that contains a query match are highlighted. Set require_field_match to false to highlight all fields. Defaults to true. 21.2 排序排序很简单，默认是按照查询文档的相关度来排序的，即（_score 字段）： 12345678910GET books/_search{ \"query\": { \"term\": { \"name\": { \"value\": \"java\" } } }} 等价于： 1234567891011121314151617GET books/_search{ \"query\": { \"term\": { \"name\": { \"value\": \"java\" } } }, \"sort\": [ { \"_score\": { \"order\": \"desc\" } } ]} match_all 查询只是返回所有文档，不评分，默认按照添加顺序返回，可以通过 _doc 字段对其进行排序： 1234567891011121314GET books/_search{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"_doc\": { \"order\": \"desc\" } } ], \"size\": 20} es 同时也支持多字段排序。 12345678910111213141516171819GET books/_search{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"price\": { \"order\": \"asc\" } }, { \"_doc\": { \"order\": \"desc\" } } ], \"size\": 20} 22.ElasticSearch 指标聚合22.1 Max Aggregation统计最大值。例如查询价格最高的书： 12345678910GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"field\": \"price\" } } }} 1234567891011GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"field\": \"price\", \"missing\": 1000 } } }} 如果某个文档中缺少 price 字段，则设置该字段的值为 1000。 也可以通过脚本来查询最大值： 123456789101112GET books/_search{ \"aggs\": { \"max_price\": { \"max\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 使用脚本时，可以先通过 doc['price'].size()!=0 去判断文档是否有对应的属性。 22.2 Min Aggregation统计最小值，用法和 Max Aggregation 基本一致： 1234567891011GET books/_search{ \"aggs\": { \"min_price\": { \"min\": { \"field\": \"price\", \"missing\": 1000 } } }} 脚本： 123456789101112GET books/_search{ \"aggs\": { \"min_price\": { \"min\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.3 Avg Aggregation统计平均值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } }}GET books/_search{ \"aggs\": { \"avg_price\": { \"avg\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.4 Sum Aggregation求和： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"sum_price\": { \"sum\": { \"field\": \"price\" } } }}GET books/_search{ \"aggs\": { \"sum_price\": { \"sum\": { \"script\": { \"source\": \"if(doc['price'].size()!=0){doc.price.value}\" } } } }} 22.5 Cardinality Aggregationcardinality aggregation 用于基数统计。类似于 SQL 中的 distinct count(0)： text 类型是分析型类型，默认是不允许进行聚合操作的，如果相对 text 类型进行聚合操作，需要设置其 fielddata 属性为 true，这种方式虽然可以使 text 类型进行聚合操作，但是无法满足精准聚合，如果需要精准聚合，可以设置字段的子域为 keyword。 方式一： 重新定义 books 索引： 123456789101112131415161718192021222324252627282930PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"fielddata\": true }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 定义完成后，重新插入数据（参考之前的视频）。 接下来就可以查询出版社的总数量： 12345678910GET books/_search{ \"aggs\": { \"publish_count\": { \"cardinality\": { \"field\": \"publish\" } } }} 这种聚合方式可能会不准确。可以将 publish 设置为 keyword 类型或者设置子域为 keyword。 12345678910111213141516171819202122232425262728PUT books{ \"mappings\": { \"properties\": { \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"publish\":{ \"type\": \"keyword\" }, \"type\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"author\":{ \"type\": \"keyword\" }, \"info\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\" }, \"price\":{ \"type\": \"double\" } } }} 对比查询结果可知，使用 fileddata 的方式，查询结果不准确。 22.6 Stats Aggregation基本统计，一次性返回 count、max、min、avg、sum： 12345678910GET books/_search{ \"aggs\": { \"stats_query\": { \"stats\": { \"field\": \"price\" } } }} 22.7 Extends Stats Aggregation高级统计，比 stats 多出来：平方和、方差、标准差、平均值加减两个标准差的区间： 12345678910GET books/_search{ \"aggs\": { \"es\": { \"extended_stats\": { \"field\": \"price\" } } }} 22.8 Percentiles Aggregation百分位统计。 123456789101112131415161718192021GET books/_search{ \"aggs\": { \"p\": { \"percentiles\": { \"field\": \"price\", \"percents\": [ 1, 5, 10, 15, 25, 50, 75, 95, 99 ] } } }} 22.9 Value Count Aggregation可以按照字段统计文档数量（包含指定字段的文档数量）： 12345678910GET books/_search{ \"aggs\": { \"count\": { \"value_count\": { \"field\": \"price\" } } }} 23.ElasticSearch 桶聚合（bucket）23.1 Terms AggregationTerms Aggregation 用于分组聚合，例如，统计各个出版社出版的图书总数量: 1234567891011GET books/_search{ \"aggs\": { \"NAME\": { \"terms\": { \"field\": \"publish\", \"size\": 20 } } }} 在 terms 分桶的基础上，还可以对每个桶进行指标聚合。 统计不同出版社所出版的图书的平均价格： 123456789101112131415161718GET books/_search{ \"aggs\": { \"NAME\": { \"terms\": { \"field\": \"publish\", \"size\": 20 }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.2 Filter Aggregation过滤器聚合。可以将符合过滤器中条件的文档分到一个桶中，然后可以求其平均值。 例如查询书名中包含 java 的图书的平均价格： 12345678910111213141516171819GET books/_search{ \"aggs\": { \"NAME\": { \"filter\": { \"term\": { \"name\": \"java\" } }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.3 Filters Aggregation多过滤器聚合。过滤条件可以有多个。 例如查询书名中包含 java 或者 office 的图书的平均价格： 123456789101112131415161718192021222324252627GET books/_search{ \"aggs\": { \"NAME\": { \"filters\": { \"filters\": [ { \"term\":{ \"name\":\"java\" } },{ \"term\":{ \"name\":\"office\" } } ] }, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"price\" } } } } }} 23.4 Range Aggregation按照范围聚合，在某一个范围内的文档数统计。 例如统计图书价格在 0-50、50-100、100-150、150以上的图书数量： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"NAME\": { \"range\": { \"field\": \"price\", \"ranges\": [ { \"to\": 50 },{ \"from\": 50, \"to\": 100 },{ \"from\": 100, \"to\": 150 },{ \"from\": 150 } ] } } }} 23.5 Date Range AggregationRange Aggregation 也可以用来统计日期，但是也可以使用 Date Range Aggregation，后者的优势在于可以使用日期表达式。 造数据： 123456789101112131415PUT blog/_doc/1{ \"title\":\"java\", \"date\":\"2018-12-30\"}PUT blog/_doc/2{ \"title\":\"java\", \"date\":\"2020-12-30\"}PUT blog/_doc/3{ \"title\":\"java\", \"date\":\"2022-10-30\"} 统计一年前到一年后的博客数量： 12345678910111213141516GET blog/_search{ \"aggs\": { \"NAME\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-12M/M\", \"to\": \"now+1y/y\" } ] } } }} 12M/M 表示 12 个月。 1y/y 表示 1年。 d 表示天 23.6 Date Histogram Aggregation时间直方图聚合。 例如统计各个月份的博客数量 1234567891011GET blog/_search{ \"aggs\": { \"NAME\": { \"date_histogram\": { \"field\": \"date\", \"calendar_interval\": \"month\" } } }} 23.7 Missing Aggregation空值聚合。 统计所有没有 price 字段的文档： 12345678910GET books/_search{ \"aggs\": { \"NAME\": { \"missing\": { \"field\": \"price\" } } }} 23.8 Children Aggregation可以根据父子文档关系进行分桶。 查询子类型为 student 的文档数量： 12345678910GET stu_class/_search{ \"aggs\": { \"NAME\": { \"children\": { \"type\": \"student\" } } }} 23.9 Geo Distance Aggregation对地理位置数据做统计。 例如查询(34.288991865037524,108.9404296875)坐标方圆 600KM 和 超过 600KM 的城市数量。 12345678910111213141516171819GET geo/_search{ \"aggs\": { \"NAME\": { \"geo_distance\": { \"field\": \"location\", \"origin\": \"34.288991865037524,108.9404296875\", \"unit\": \"km\", \"ranges\": [ { \"to\": 600 },{ \"from\": 600 } ] } } }} 23.10 IP Range AggregationIP 地址范围查询。 12345678910111213141516GET blog/_search{ \"aggs\": { \"NAME\": { \"ip_range\": { \"field\": \"ip\", \"ranges\": [ { \"from\": \"127.0.0.5\", \"to\": \"127.0.0.11\" } ] } } }} 24.ElasticSearch 管道聚合 类似于linux的 | 管道符管道聚合相当于在之前聚合的基础上，再次聚合。 24.1 Avg Bucket Aggregation计算聚合平均值。例如，统计每个出版社所出版图书的平均值，然后再统计所有出版社的平均值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"avg_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.2 Max Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最大值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"max_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.3 Min Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最小值： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"min_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.4 Sum Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值之和： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"sum_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.5 Stats Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值的各种数据： 1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"stats_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.6 Extended Stats Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"extended_stats_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 24.7 Percentiles Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ \"aggs\": { \"book_count\": { \"terms\": { \"field\": \"publish\", \"size\": 3 }, \"aggs\": { \"book_avg\": { \"avg\": { \"field\": \"price\" } } } }, \"avg_book\":{ \"percentiles_bucket\": { \"buckets_path\": \"book_count&gt;book_avg\" } } }} 25.ElasticSearch Java Api 概览Java操作Es的方案: 1.直接使用Http请求以 HttpUrlConnection 为例，请求方式如下： 12345678910111213public class HttpRequestTest { public static void main(String[] args) throws IOException { URL url = new URL(\"http://localhost:9200/books/_search?pretty=true\"); HttpURLConnection con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } } }} 弊端：需要自己组装请求参数，自己去解析响应的json 2.Java Low Level REST Client从字面上来理解，这个叫做低级客户端。 它允许通过 Http 与一个 Elasticsearch 集群通信。将请求的 JSON 参数拼接和响应的 JSON 字符串解析留给用户自己处理。低级客户端最大的优势在于兼容所有的 ElasticSearch 的版本（因为它的 API 并没有封装 JSON 操作，所有的 JSON 操作还是由开发者自己完成），同时低级客户端要求 JDK 为 1.7 及以上。 低级客户端主要包括如下一些功能： 最小的依赖 跨所有可用节点的负载均衡 节点故障和特定响应代码时的故障转移 连接失败重试（是否重试失败的节点取决于它失败的连续次数；失败次数越多，客户端在再次尝试同一节点之前等待的时间越长） 持久连接 跟踪请求和响应的日志记录 可选自动发现集群节点 Java Low Level REST Client 的操作其实比较简单，松哥后面会录制一个视频和大家分享相关操作。 3.Java High Level REST Client从字面上来理解，这个叫做高级客户端，也是目前使用最多的一种客户端。它其实有点像之前的 TransportClient。 这个所谓的高级客户端它的内部其实还是基于低级客户端，只不过针对 ElasticSearch 它提供了更多的 API，将请求参数和响应参数都封装成了相应的 API，开发者只需要调用相关的方法就可以拼接参数或者解析响应结果。 Java High Level REST Client 中的每个 API 都可以同步或异步调用，同步方法返回一个响应对象，而异步方法的名称则以 Async 为后缀结尾，异步请求一般需要一个监听器参数，用来处理响应结果。 相对于低级客户端，高级客户端的兼容性就要差很多（因为 JSON 的拼接和解析它已经帮我们做好了）。高级客户端需要 JDK1.8 及以上版本并且依赖版本需要与 ElasticSearch 版本相同（主版本号需要一致，次版本号不必相同）。 举个简单例子： 7.0 客户端能够与任何 7.x ElasticSearch 节点进行通信，而 7.1 客户端肯定能够与 7.1，7.2 和任何后来的 7.x 版本进行通信，但与旧版本的 ElasticSearch 节点通信时可能会存在不兼容的问题。 4.其他ElasticSearch 的 Java 客户端 TransportClient Jest Spring Data Elasticsearch Java Low Level REST Client Java High Level REST Client TransportClient 官方已经不再推荐使用 TransportClient，并且表示会在 ElasticSearch8.0 中完全移除相关支持。 Jest Jest 提供了更流畅的 API 和更容易使用的接口，并且它的版本是遵循 ElasticSearch 的主版本号的，这样可以确保客户端和服务端之间的兼容性。 早期的 ElasticSearch 官方客户端对 RESTful 支持不够完美， Jest 在一定程度上弥补了官方客户端的不足，但是随着近两年官方客户端对 RESTful 功能的增强，Jest 早已成了明日黄花，最近的一次更新也停留在 2018 年 4 月，所以 Jest 小伙伴们也不必花时间去学了，知道曾经有过这么一个东西就行了。 Spring Data Elasticsearch Spring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点： Spring Data 项目支持 NoSQL 存储： MongoDB （文档数据库） Neo4j（图形数据库） Redis（键/值存储） Hbase（列族数据库） ElasticSearch Spring Data 项目所支持的关系数据存储技术： JDBC JPA 26.ElasticSearch普通HTTP请求以 HttpUrlConnection 为例，请求方式如下： 12345678910111213public class HttpRequestTest { public static void main(String[] args) throws IOException { URL url = new URL(\"http://localhost:9200/books/_search?pretty=true\"); HttpURLConnection con = (HttpURLConnection) url.openConnection(); if (con.getResponseCode() == 200) { BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } } }} 弊端：需要自己组装请求参数，自己去解析响应的json 27.ElasticSearch Java Low Level REST Client首先创建一个普通的 Maven 工程，添加如下依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.10.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 然后添加如下代码，发起一个简单的查询请求： 1234567891011121314151617181920212223242526272829public class LowLevelTest { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //这种请求发起方式，会阻塞后面的代码 Response response = restClient.performRequest(request); //5.解析 response，获取响应结果 BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); }} 这个查询请求，是一个同步请求，在请求的过程中，后面的代码会被阻塞，如果不希望后面的代码被阻塞，可以使用异步请求。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LowLevelTest2 { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //异步请求 restClient.performRequestAsync(request, new ResponseListener() { //请求成功的回调 @Override public void onSuccess(Response response) { //5.解析 response，获取响应结果 try { BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败的回调 @Override public void onFailure(Exception e) { } }); }} 开发者在请求时，也可以携带 JSON 参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class LowLevelTest3 { public static void main(String[] args) throws IOException { //1.构建一个 RestClient 对象 RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") ); //2.如果需要在请求头中设置认证信息等，可以通过 builder 来设置// builder.setDefaultHeaders(new Header[]{new BasicHeader(\"key\",\"value\")}); RestClient restClient = builder.build(); //3.构建请求 Request request = new Request(\"GET\", \"/books/_search\"); //添加请求参数 request.addParameter(\"pretty\",\"true\"); //添加请求体 request.setEntity(new NStringEntity(\"{\\\"query\\\": {\\\"term\\\": {\\\"name\\\": {\\\"value\\\": \\\"java\\\"}}}}\", ContentType.APPLICATION_JSON)); //4.发起请求，发起请求有两种方式，可以同步，可以异步 //异步请求 restClient.performRequestAsync(request, new ResponseListener() { //请求成功的回调 @Override public void onSuccess(Response response) { //5.解析 response，获取响应结果 try { BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); String str = null; while ((str = br.readLine()) != null) { System.out.println(str); } br.close(); //最后记得关闭 RestClient restClient.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败的回调 @Override public void onFailure(Exception e) { } }); }} 28.Java High Level REST Client28.1 索引管理28.1.1 创建索引首先创建一个普通的 Maven 项目，然后引入 high level rest client 依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.10.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意，依赖的版本和 Es 的版本要对应。 创建一个索引： 1234567891011121314151617181920212223public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式 blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} mapping 的配置，还有另外两种方式： 第一种，通过 map 构建 mapping： 12345678910111213141516171819202122232425262728293031public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式 Map&lt;String, String&gt; title = new HashMap&lt;&gt;(); title.put(\"type\", \"text\"); Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;(); properties.put(\"title\", title); Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;(); mappings.put(\"properties\", properties); blog1.mapping(mappings); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 第二种，通过 XContentBuilder 构建 mapping： 1234567891011121314151617181920212223242526272829303132333435363738394041public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式// Map&lt;String, String&gt; title = new HashMap&lt;&gt;();// title.put(\"type\", \"text\");// Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;();// properties.put(\"title\", title);// Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;();// mappings.put(\"properties\", properties);// blog1.mapping(mappings); //XContentBuilder 方式 XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); builder.startObject(\"properties\"); builder.startObject(\"title\"); builder.field(\"type\", \"text\"); builder.endObject(); builder.endObject(); builder.endObject(); blog1.mapping(builder); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 还可以给索引配置别名： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class HighLevelTest { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //配置 settings，分片、副本等信息 blog1.settings(Settings.builder().put(\"index.number_of_shards\", 3).put(\"index.number_of_replicas\", 2)); //配置字段类型，字段类型可以通过 JSON 字符串、Map 以及 XContentBuilder 三种方式来构建 //json 字符串的方式// blog1.mapping(\"{\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"text\\\"}}}\", XContentType.JSON); //map 的方式// Map&lt;String, String&gt; title = new HashMap&lt;&gt;();// title.put(\"type\", \"text\");// Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;();// properties.put(\"title\", title);// Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;();// mappings.put(\"properties\", properties);// blog1.mapping(mappings); //XContentBuilder 方式 XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); builder.startObject(\"properties\"); builder.startObject(\"title\"); builder.field(\"type\", \"text\"); builder.endObject(); builder.endObject(); builder.endObject(); blog1.mapping(builder); //配置别名 blog1.alias(new Alias(\"blog_alias\")); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 如果觉得调 API 太麻烦，也可以直接上 JSON： 1234567891011121314151617181920public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 另外还有一些其他的可选配置： 123456789101112131415161718192021222324public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //请求超时时间，连接所有节点的超时时间 blog1.setTimeout(TimeValue.timeValueMinutes(2)); //连接 master 节点的超时时间 blog1.setMasterTimeout(TimeValue.timeValueMinutes(1)); //执行请求，创建索引 client.indices().create(blog1, RequestOptions.DEFAULT); //关闭 client client.close(); }} 前面所有的请求都是同步的，会阻塞的，也可以异步： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class HighLevelTest2 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //删除已经存在的索引 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(\"blog\"); client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); //创建一个索引 CreateIndexRequest blog1 = new CreateIndexRequest(\"blog\"); //直接同构 JSON 配置索引 blog1.source(\"{\\\"settings\\\": {\\\"number_of_shards\\\": 3,\\\"number_of_replicas\\\": 2},\\\"mappings\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"type\\\": \\\"keyword\\\"}}},\\\"aliases\\\": {\\\"blog_alias_javaboy\\\": {}}}\", XContentType.JSON); //请求超时时间，连接所有节点的超时时间 blog1.setTimeout(TimeValue.timeValueMinutes(2)); //连接 master 节点的超时时间 blog1.setMasterTimeout(TimeValue.timeValueMinutes(1)); //执行请求，创建索引// client.indices().create(blog1, RequestOptions.DEFAULT); //异步创建索引 client.indices().createAsync(blog1, RequestOptions.DEFAULT, new ActionListener&lt;CreateIndexResponse&gt;() { //请求成功 @Override public void onResponse(CreateIndexResponse createIndexResponse) { //关闭 client try { client.close(); } catch (IOException e) { e.printStackTrace(); } } //请求失败 @Override public void onFailure(Exception e) { } }); //关闭 client// client.close(); }} 28.1.2 查询索引是否存在1234567891011121314public class HighLevelTest3 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetIndexRequest blog = new GetIndexRequest(\"blog2\"); boolean exists = client.indices().exists(blog, RequestOptions.DEFAULT); System.out.println(\"exists = \" + exists); //关闭 client client.close(); }} 28.1.3 关闭/打开索引关闭： 1234567891011121314151617public class HighLevelTest4 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); CloseIndexRequest blog = new CloseIndexRequest(\"blog\"); CloseIndexResponse close = client.indices().close(blog, RequestOptions.DEFAULT); List&lt;CloseIndexResponse.IndexResult&gt; indices = close.getIndices(); for (CloseIndexResponse.IndexResult index : indices) { System.out.println(\"index.getIndex() = \" + index.getIndex()); } //关闭 client client.close(); }} 触发warning[“the default value for the ?wait_for_active_shards parameter will change from ‘0’ to ‘index-setting’ in version 8; specify ‘?wait_for_active_shards=index-setting’ to adopt the future default behaviour, or ‘?wait_for_active_shards=0’ to preserve today’s behaviour”] 打开： 12345678910111213public class HighLevelTest4 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); OpenIndexRequest blog = new OpenIndexRequest(\"blog\"); client.indices().open(blog, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.4 索引修改1234567891011121314public class HighLevelTest5 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateSettingsRequest request = new UpdateSettingsRequest(\"blog\"); request.settings(Settings.builder().put(\"index.blocks.write\", true).build()); client.indices().putSettings(request, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.5 克隆索引被克隆的索引需要是只读索引，可以通过 28.1.4 小节中的方式设置索引为只读。 12345678910111213public class HighLevelTest6 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); ResizeRequest request = new ResizeRequest(\"blog2\", \"blog\"); client.indices().clone(request, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.6 查看索引12345678910111213141516171819public class HighLevelTest7 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetSettingsRequest request = new GetSettingsRequest().indices(\"blog\"); //设置需要互殴去的具体的参数，不设置则返回所有参数 request.names(\"index.blocks.write\"); GetSettingsResponse response = client.indices().getSettings(request, RequestOptions.DEFAULT); ImmutableOpenMap&lt;String, Settings&gt; indexToSettings = response.getIndexToSettings(); System.out.println(indexToSettings); String s = response.getSetting(\"blog\", \"index.number_of_replicas\"); System.out.println(s); //关闭 client client.close(); }} 28.1.7 Refresh &amp; FlushEs 底层依赖 Lucene，而 Lucene 中有 reopen 和 commit 两种操作，还有一个特殊的概念叫做 segment。 Es 中，基本的存储单元是 shard，对应到 Lucene 上，就是一个索引，Lucene 中的索引由 segment 组成，每个 segment 相当于 es 中的倒排索引。每个 es 文档创建时，都会写入到一个新的 segment 中，删除文档时，只是从属于它的 segment 处标记为删除，并没有从磁盘中删除。 Lucene 中： reopen 可以让数据搜索到，但是不保证数据被持久化到磁盘中。 commit 可以让数据持久化。 Es 中： 默认是每秒 refresh 一次（Es 中文档被索引之后，首先添加到内存缓冲区，refresh 操作将内存缓冲区中的数据拷贝到新创建的 segment 中，这里是在内存中操作的）。 flush 将内存中的数据持久化到磁盘中。一般来说，flush 的时间间隔比较久，默认 30 分钟。 123456789101112131415public class HighLevelTest8 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); RefreshRequest request = new RefreshRequest(\"blog\"); client.indices().refresh(request, RequestOptions.DEFAULT); FlushRequest flushRequest = new FlushRequest(\"blog\"); client.indices().flush(flushRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.9 索引别名索引的别名类似于 MySQL 中的视图。 28.1.9.1 添加别名添加一个普通的别名： 12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.ADD); aliasAction.index(\"books\").alias(\"books_alias\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 添加一个带 filter 的别名： 12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.ADD); aliasAction.index(\"books\").alias(\"books_alias2\").filter(\"{\\\"term\\\": {\\\"name\\\": \\\"java\\\"}}\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 现在，books 索引将存在两个别名，其中，books_alias2 自动过滤 name 中含有 java 的文档。 28.1.9.2 删除别名12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); IndicesAliasesRequest.AliasActions aliasAction = new IndicesAliasesRequest.AliasActions(IndicesAliasesRequest.AliasActions.Type.REMOVE); aliasAction.index(\"books\").alias(\"books_alias\"); indicesAliasesRequest.addAliasAction(aliasAction); client.indices().updateAliases(indicesAliasesRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 第二种移除方式： 12345678910111213public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); DeleteAliasRequest deleteAliasRequest = new DeleteAliasRequest(\"books\", \"books_alias2\"); client.indices().deleteAlias(deleteAliasRequest, RequestOptions.DEFAULT); //关闭 client client.close(); }} 28.1.9.3 判断别名是否存在12345678910111213141516public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetAliasesRequest books_alias = new GetAliasesRequest(\"books_alias\"); //指定查看某一个索引的别名，不指定，则会搜索所有的别名 books_alias.indices(\"books\"); boolean b = client.indices().existsAlias(books_alias, RequestOptions.DEFAULT); System.out.println(b); //关闭 client client.close(); }} 28.1.9.4 获取别名1234567891011121314151617public class HighLevelTest9 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetAliasesRequest books_alias = new GetAliasesRequest(\"books_alias\"); //指定查看某一个索引的别名，不指定，则会搜索所有的别名 books_alias.indices(\"books\"); GetAliasesResponse response = client.indices().getAlias(books_alias, RequestOptions.DEFAULT); Map&lt;String, Set&lt;AliasMetadata&gt;&gt; aliases = response.getAliases(); System.out.println(\"aliases = \" + aliases); //关闭 client client.close(); }} 29.1 添加文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class DocTest01 { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); //构建一个 IndexRequest 请求，参数就是索引名称 IndexRequest request = new IndexRequest(\"book\"); //给请求配置一个 id，这个就是文档 id。如果指定了 id，相当于 put book/_doc/id ，也可以不指定 id，相当于 post book/_doc// request.id(\"1\"); //构建索引文本，有三种方式：JSON 字符串、Map 对象、XContentBuilder request.source(\"{\\\"name\\\": \\\"三国演义\\\",\\\"author\\\": \\\"罗贯中\\\"}\", XContentType.JSON); //执行请求，有同步和异步两种方式 //同步 IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); //获取文档id String id = indexResponse.getId(); System.out.println(\"id = \" + id); //获取索引名称 String index = indexResponse.getIndex(); System.out.println(\"index = \" + index); //判断文档是否添加成功 if (indexResponse.getResult() == DocWriteResponse.Result.CREATED) { System.out.println(\"文档添加成功\"); } //判断文档是否更新成功（如果 id 已经存在） if (indexResponse.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"文档更新成功\"); } ReplicationResponse.ShardInfo shardInfo = indexResponse.getShardInfo(); //判断分片操作是否都成功 if (shardInfo.getTotal() != shardInfo.getSuccessful()) { System.out.println(\"有存在问题的分片\"); } //有存在失败的分片 if (shardInfo.getFailed() &gt; 0) { //打印错误信息 for (ReplicationResponse.ShardInfo.Failure failure : shardInfo.getFailures()) { System.out.println(\"failure.reason() = \" + failure.reason()); } } //异步// client.indexAsync(request, RequestOptions.DEFAULT, new ActionListener&lt;IndexResponse&gt;() {// @Override// public void onResponse(IndexResponse indexResponse) {//// }//// @Override// public void onFailure(Exception e) {//// }// }); client.close(); }} 演示分片存在问题的情况。由于我只有三个节点，但是在创建索引时，设置需要三个副本，此时的节点就不够用： 1234567PUT book{ \"settings\": { \"number_of_replicas\": 3, \"number_of_shards\": 3 }} 创建完成后，再次执行上面的添加代码，此时就会打印出 有存在问题的分片。 构建索引信息，有三种方式： 123456789101112//构建索引文本，有三种方式：JSON 字符串、Map 对象、XContentBuilder//request.source(\"{\\\"name\\\": \\\"三国演义\\\",\\\"author\\\": \\\"罗贯中\\\"}\", XContentType.JSON);//Map&lt;String, String&gt; map = new HashMap&lt;&gt;();//map.put(\"name\", \"水浒传\");//map.put(\"author\", \"施耐庵\");//request.source(map).id(\"99\");XContentBuilder jsonBuilder = XContentFactory.jsonBuilder();jsonBuilder.startObject();jsonBuilder.field(\"name\", \"西游记\");jsonBuilder.field(\"author\", \"吴承恩\");jsonBuilder.endObject();request.source(jsonBuilder); 默认情况下，如果 request 中包含有 id 属性，则相当于 PUT book/_doc/1 这样的请求，如果 request 中不包含 id 属性，则相当于 POST book/_doc，此时 id 会自动生成。对于前者，如果 id 已经存在，则会执行一个更新操作。也就是 es 的具体操作，会自动调整。 当然，也可以直接指定操作。例如，指定为添加文档的操作： 12345678910111213//构建一个 IndexRequest 请求，参数就是索引名称IndexRequest request = new IndexRequest(\"book\");XContentBuilder jsonBuilder = XContentFactory.jsonBuilder();jsonBuilder.startObject();jsonBuilder.field(\"name\", \"西游记\");jsonBuilder.field(\"author\", \"吴承恩\");jsonBuilder.endObject();request.source(jsonBuilder).id(\"99\");//这是一个添加操作，不要自动调整为更新操作request.opType(DocWriteRequest.OpType.CREATE);//执行请求，有同步和异步两种方式//同步IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); 29.2 获取文档根据 id 获取文档： 1234567891011121314151617181920212223public class GetDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetRequest request = new GetRequest(\"book\", \"98\"); GetResponse response = client.get(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); if (response.isExists()) { //如果文档存在 long version = response.getVersion(); System.out.println(\"version = \" + version); String sourceAsString = response.getSourceAsString(); System.out.println(\"sourceAsString = \" + sourceAsString); }else{ System.out.println(\"文档不存在\"); } client.close(); }} 29.3 判断文档是否存在判断文档是否存在和获取文档的 API 是一致的。只不过在判断文档是否存在时，不需要获取 source。 1234567891011121314public class ExistsDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); GetRequest request = new GetRequest(\"book\", \"99\"); request.fetchSourceContext(new FetchSourceContext(false)); boolean exists = client.exists(request, RequestOptions.DEFAULT); System.out.println(\"exists = \" + exists); client.close(); }} 29.4 删除文档删除 id 为 99 的文档： 123456789101112131415161718192021222324public class DeleteDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); DeleteRequest request = new DeleteRequest(\"book\", \"99\"); DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); ReplicationResponse.ShardInfo shardInfo = response.getShardInfo(); if (shardInfo.getTotal() != shardInfo.getSuccessful()) { System.out.println(\"有分片存在问题\"); } if (shardInfo.getFailed() &gt; 0) { for (ReplicationResponse.ShardInfo.Failure failure : shardInfo.getFailures()) { System.out.println(\"failure.reason() = \" + failure.reason()); } } client.close(); }} 删除文档的响应和添加文档成功的响应类似，可以对照着理解。 29.4 更新文档通过脚本更新： 12345678910111213141516171819202122public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); //通过脚本更新 Map&lt;String, Object&gt; params = Collections.singletonMap(\"name\", \"三国演义666\"); Script inline = new Script(ScriptType.INLINE, \"painless\", \"ctx._source.name=params.name\", params); request.script(inline); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 通过 JSON 更新： 12345678910111213141516171819public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); request.doc(\"{\\\"name\\\": \\\"三国演义\\\"}\", XContentType.JSON); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 当然，这个 JSON 字符串也可以通过 Map 或者 XContentBuilder 来构建： Map: 123456789101112131415161718192021public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); Map&lt;String, Object&gt; docMap = new HashMap&lt;&gt;(); docMap.put(\"name\", \"三国演义888\"); request.doc(docMap); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} XContentBuilder: 1234567891011121314151617181920212223public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"1\"); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); jsonBuilder.startObject(); jsonBuilder.field(\"name\", \"三国演义666\"); jsonBuilder.endObject(); request.doc(jsonBuilder); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } client.close(); }} 也可以通过 upsert 方法实现文档不存在时就添加文档： 1234567891011121314151617181920212223242526public class UpdateDoc { public static void main(String[] args) throws IOException { RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\"), new HttpHost(\"localhost\", 9202, \"http\") )); UpdateRequest request = new UpdateRequest(\"book\", \"99\"); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); jsonBuilder.startObject(); jsonBuilder.field(\"name\", \"三国演义666\"); jsonBuilder.endObject(); request.doc(jsonBuilder); request.upsert(\"{\\\"name\\\": \\\"红楼梦\\\",\\\"author\\\": \\\"曹雪芹\\\"}\", XContentType.JSON); UpdateResponse response = client.update(request, RequestOptions.DEFAULT); System.out.println(\"response.getId() = \" + response.getId()); System.out.println(\"response.getIndex() = \" + response.getIndex()); System.out.println(\"response.getVersion() = \" + response.getVersion()); if (response.getResult() == DocWriteResponse.Result.UPDATED) { System.out.println(\"更新成功!\"); } else if (response.getResult() == DocWriteResponse.Result.CREATED) { System.out.println(\"文档添加成功\"); } client.close(); }}","link":"/2021/04/13/ES%E7%AC%94%E8%AE%B0/"},{"title":"Java并发编程实战_核心篇笔记","text":"前言 Java并发编程实战_核心篇笔记 第4章4.4 基于任务的分割实现并发化 使用多线程编程的一个好的方式是从单线程程序开始，只有在单线程程序算法本身没有重大性能瓶颈但仍然无法满足要求的情况下我们才考虑使用多线程。 第5章5.3 CountDownLatch实现一个（或者多个）线程等待其他线程完成一组特定的操作之后才继续运行。这组操作也被称为 先决操作 第6章 保障线程安全的设计技术6.1 java运行时存储空间堆空间 存储对象 栈空间 存储相应方法的局部变量、返回值等私有数据 非堆空间 存储常量以及类的元数据 6.2 无状态对象一个类的同一个实例被多个线程共享并不会使这些线程存在共享状态，那么这个类及其任意一个实例就被称为无状态对象。反之，则成为有状态对象。 无状态对象不含任何实例变量，且不包含任何静态变量或常量。 清单6-2 无状态对象实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package io.renren.modules.ch3;/** * 下游部件的节点 * * @author xxp * @version 1.0 * @date 2021-09-09 9:27 **/public class Endpoint { public final String host; public final int port; public final int weight; private volatile boolean online = true; public Endpoint(String host, int port, int weight) { this.host = host; this.port = port; this.weight = weight; } public boolean isOnline() { return online; } public void setOnline(boolean online) { this.online = online; } @Override public int hashCode() { final int prime = 31; int result = 1; result = prime * result + ((host == null) ? 0 : host.hashCode()); result = prime * result + port; result = prime * result + weight; return result; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Endpoint other = (Endpoint) obj; if (host == null) { if (other.host != null) return false; } else if (!host.equals(other.host)) return false; if (port != other.port) return false; if (weight != other.weight) return false; return true; } @Override public String toString() { return \"Endpoint [host=\" + host + \", port=\" + port + \", weight=\" + weight + \", online=\" + online + \"]\\n\"; }} 123456789101112131415161718192021222324252627282930313233343536package io.renren.modules.ch6;import io.renren.modules.ch3.Endpoint;import java.util.Comparator;/** * @author xxp * @version 1.0 * @date 2021-09-09 9:23 **/public class DefaultEndpointComparator implements Comparator&lt;Endpoint&gt; { @Override public int compare(Endpoint o1, Endpoint o2) { int result = 0; boolean isOnline1 = o1.isOnline(); boolean isOnline2 = o2.isOnline(); // 优先按照服务器是否在线排序 if (isOnline1 == isOnline2) { // 被比较的两台服务器都在线（或不在线）的情况下进一步比较服务器权重 result = compareWeight(o1.weight, o2.weight); } else { // 在线的服务器排序靠前 if (isOnline1) { result = -1; } } return result; } private int compareWeight(int weight1, int weight2) { // 按权重降序排列 return Integer.compare(weight2, weight1); }} 清单6-3 对服务器节点进行排序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package io.renren.modules.ch6;import io.renren.common.util.Debug;import io.renren.modules.ch3.Endpoint;import java.util.Arrays;import java.util.Comparator;/** * @author xxp * @version 1.0 * @date 2021-09-09 9:34 **/public class EndpointView { static final Comparator&lt;Endpoint&gt; DEFAULT_COMPARATOR; static { DEFAULT_COMPARATOR = new DefaultEndpointComparator(); } // 省略其他代码 public Endpoint[] retrieveServletList(Comparator&lt;Endpoint&gt; comparator) { Endpoint[] serverList = doRetrieveServerList(); Arrays.sort(serverList, comparator); return serverList; } public Endpoint[] retrieveServerList() { return retrieveServletList(DEFAULT_COMPARATOR); } private Endpoint[] doRetrieveServerList() { // // 模拟实际代码 Endpoint[] serverList = new Endpoint[]{ new Endpoint(\"192.168.1.100\", 8080, 5), new Endpoint(\"192.168.1.101\", 8081, 3), new Endpoint(\"192.168.1.102\", 8082, 2), new Endpoint(\"192.168.1.103\", 8080, 4)}; serverList[0].setOnline(false); serverList[3].setOnline(false); return serverList; } public static void main(String[] args) { EndpointView endpointView = new EndpointView(); Endpoint[] serverList = endpointView.retrieveServerList(); Debug.info(Arrays.toString(serverList)); }} 清单6-4 多个线程访问本身不包含状态的对象也可能存在共享状态示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package io.renren.modules.ch6;import java.util.HashMap;import java.util.Map;/** * @author xxp * @version 1.0 * @date 2021-09-09 9:50 **/public class BrokenStatelessObject { public String doSomething(String s) { UnsafeSingleton us = UnsafeSingleton.INSTANCE; int i = us.doSomething(s); UnsafeStatefullObject sfo = new UnsafeStatefullObject(); String str = sfo.doSomething(s, i); return str; } public String doSomething1(String s) { UnsafeSingleton us = UnsafeSingleton.INSTANCE; UnsafeStatefullObject sfo = new UnsafeStatefullObject(); String str; // 加锁保障线程安全 synchronized (this) { str = sfo.doSomething(s, us.doSomething(s)); } return str; }}class UnsafeStatefullObject { static Map&lt;String, String&gt; cache = new HashMap&lt;&gt;(); public String doSomething(String s, int len) { String result = cache.get(s); if (null == result) { result = md5sum(result, len); cache.put(s, result); } return result; } public String md5sum(String s, int len) { // 生成md5摘要 // 省略其他代码 return s; }}enum UnsafeSingleton { INSTANCE; public int state1; public int doSomething(String s) { // 省略其他代码 // 访问state1 return 0; }} 清单6-5 非线程安全的Servlet示例123456789101112131415161718192021222324252627282930313233package io.renren.modules.ch6;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.text.ParseException;import java.text.SimpleDateFormat;/** * 该类是一个错误的Servlet类（非线程安全） * * @author xxp * @version 1.0 * @date 2021-09-09 10:02 **/public class UnsafeServlet extends HttpServlet { private static final long serialVersionUID = -2772996404655982182L; private final SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd\"); @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String strExpiryDate = req.getParameter(\"expirtyDate\"); try { sdf.parse(strExpiryDate); } catch (ParseException e) { e.printStackTrace(); } // 省略其他代码 }} 6.3 不可变对象清单6-6 减少不可变对象所占用的空间1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package io.renren.modules.ch6;import io.renren.common.util.ReadOnlyIterator;import java.util.*;import java.util.concurrent.atomic.AtomicInteger;/** * @author xxp * @version 1.0 * @date 2021-09-09 10:26 **/public class BigImmutableObject implements Iterable&lt;Map.Entry&lt;String, BigObject&gt;&gt; { private final HashMap&lt;String, BigObject&gt; registry; public BigImmutableObject(Map&lt;String, BigObject&gt; registry) { this.registry = (HashMap&lt;String, BigObject&gt;) registry; } public BigImmutableObject(BigImmutableObject prototype, String key, BigObject newValue) { this(createRegistry(prototype, key, newValue)); } @SuppressWarnings(\"unchecked\") private static HashMap&lt;String, BigObject&gt; createRegistry(BigImmutableObject prototype, String key, BigObject newValue) { // 从现有对象中复制（浅复制）字段 HashMap&lt;String, BigObject&gt; newRegistry = (HashMap&lt;String, BigObject&gt;) prototype.registry.clone(); // 仅更新需要更新的部分 newRegistry.put(key, newValue); return newRegistry; } @Override public Iterator&lt;Map.Entry&lt;String, BigObject&gt;&gt; iterator() { // 对entrySet进行防御性复制 final Set&lt;Map.Entry&lt;String, BigObject&gt;&gt; readOnlyEntries = Collections.unmodifiableSet(registry.entrySet()); // 返回一个只读的Iterator实例 return ReadOnlyIterator.with(readOnlyEntries.iterator()); } public BigObject getObject(String key) { return registry.get(key); } public BigImmutableObject update(String key, BigObject newValue) { return new BigImmutableObject(this, key, newValue); }}class BigObject { byte[] data = new byte[4 * 1024 * 1024]; private int id; private final static AtomicInteger ID_Gen = new AtomicInteger(0); public BigObject() { id = ID_Gen.incrementAndGet(); } @Override public String toString() { return new StringJoiner(\", \", BigObject.class.getSimpleName() + \"[\", \"]\") .add(\"data=\" + Arrays.toString(data)) .add(\"id=\" + id) .toString(); } // 省略其他代码} 6.4 线程特有对象清单6-7 使用ThreadLocal实现线程安全示例代码12345678910111213141516171819202122232425262728293031323334353637383940package io.renren.modules.ch6;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;import java.text.ParseException;import java.text.SimpleDateFormat;/** * @author xxp * @version 1.0 * @date 2021-09-09 10:49 **/@WebServlet(\"/threadLocalExample\")public class ServletWithThreadLocal extends HttpServlet { final static ThreadLocal&lt;SimpleDateFormat&gt; SDF = new ThreadLocal&lt;SimpleDateFormat&gt;() { @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(\"yyyy-MM-dd\"); } }; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { final SimpleDateFormat sdf = SDF.get(); String strExpiryDate = req.getParameter(\"expirtyDate\"); try (PrintWriter pwr = resp.getWriter()) { sdf.parse(strExpiryDate); // 省略其他代码 pwr.printf(\"[%s]expirtyDate:%s\", Thread.currentThread().getName(), strExpiryDate); } catch (ParseException e) { e.printStackTrace(); } // 省略其他代码 }} 清单6-8 使用ThreadLocal避免锁的争用123456789101112131415161718192021222324252627282930313233343536373839404142package io.renren.modules.ch6.case01;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;/** * @author xxp * @version 1.0 * @date 2021-09-09 10:59 **/public enum ThreadSpecificSecureRandom { INSTANCE; final static ThreadLocal&lt;SecureRandom&gt; SECURE_RANDOM = new ThreadLocal&lt;SecureRandom&gt;() { @Override protected SecureRandom initialValue() { SecureRandom srnd; try { srnd = SecureRandom.getInstance(\"SHA1PRNG\"); } catch (NoSuchAlgorithmException e) { srnd = new SecureRandom(); new RuntimeException(\"No SHA1PRNG available,defaults to new SecureRandom()\", e) .printStackTrace(); } // 通过以下调用来初始化种子 srnd.nextBytes(new byte[20]); return srnd; } }; // 生成随机数 public int nextInt(int upperBound) { SecureRandom secureRnd = SECURE_RANDOM.get(); return secureRnd.nextInt(upperBound); } public void setSeed(long seed) { SecureRandom secureRnd = SECURE_RANDOM.get(); secureRnd.setSeed(seed); }} 清单6-11 使用Filter规避ThreadLocal内存泄漏示例代码1package io.renren.modules.ch6;import javax.servlet.*;import java.io.IOException;/** * @author xxp * @version 1.0 * @date 2021-09-09 16:19 **/public class ThreadLocalCleanupFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { filterChain.doFilter(servletRequest, servletResponse); ThreadLocalMemoryLeak.counterHolder.remove(); } @Override public void init(FilterConfig filterConfig) throws ServletException { Filter.super.init(filterConfig); } @Override public void destroy() { Filter.super.destroy(); }} 第7章 线程活性故障规避死锁的常见方法 粗锁法（Coarsen-gained Lock）- 使用一个粗粒度的锁代替多个锁。 锁排序法（Lock Ordering）- 相关线程使用全局统一的顺序申请锁。 使用 ReentrantLock.tryLock(long,TimeUnit) 来申请锁。 使用开放调用(Open Call) - 在调用外部方法时不加锁。 使用锁的替代品。 终极方法-不使用锁 7.2 沉睡不醒的睡美人：锁死信号丢失锁死 常见例子 ​ CountDownLatch.countDown()调用没有放在finally块中导致CountDownLatch.await()的执行流程一直处于等待状态，从而使其任务一直无法进展 嵌套监视器锁死 第8章 线程管理","link":"/2021/08/15/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98_%E6%A0%B8%E5%BF%83%E7%AF%87_%E7%AC%94%E8%AE%B0/"},{"title":"Java并发编程实战","text":"前言 Java并发编程实战 Java并发编程实战https://time.geekbang.org/column/intro/100023901 01 | 可见性、原子性和有序性问题：并发编程Bug的源头源头之一：缓存导致的可见性问题 源头之二：线程切换带来的原子性问题 源头之三：编译优化带来的有序性问题 在 32 位的机器上对 long 型变量进行加减操作存在并发隐患? long类型64位，所以在32位的机器上，对long类型的数据操作通常需要多条指令组合出来，无法保证原子性，所以并发的时候会出问题 02 | Java内存模型：看Java如何解决可见性和有序性问题volatile 关键字并不是 Java 语言的特产，古老的 C 语言里也有，它最原始的意义就是禁用 CPU 缓存。 Happens-Before 规则前面一个操作的结果对后续操作是可见的。 1. 程序的顺序性规则这条规则是指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。 2. volatile 变量规则对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile变量的读操作。 3. 传递性这条规则是指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。 4. 管程中锁的规则对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 管程是一种通用的同步原语，在Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 5. 线程 start() 规则主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。 6. 线程 join() 规则主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。 课后题 有一个共享变量 abc，在一个线程里设置了 abc 的值 abc=3，你思考一下，有哪些办法可以让其他线程能够看到abc==3？ 1.声明共享变量abc，并使用volatile关键字修饰abc2.声明共享变量abc，在synchronized关键字对abc的赋值代码块加锁，由于Happenbefore管程锁的规则，可以使得后续的线程可以看到abc的值。 03 | 互斥锁（上）：解决原子性问题12345678910111213141516// 下面的代码用 synchronized 修饰代码块来尝试解决并发问题，你觉得这个使用方式正确吗？有哪些问题呢？能解决可见性和原子性问题吗？class SafeCalc { long value = 0L; long get() { synchronized (new Object()) { return value; } } void addOne() { synchronized (new Object()) { value += 1; } }} 加锁本质就是在锁对象的对象头中写入当前线程id，但是new object每次在内存中都是新对象，所以加锁无效。 JVM 开启逃逸分析之后，synchronized (new Object()) 这行代码在实际执行的时候会被优化掉，也就是说在真实执行的时候，这行代码压根就不存在。 04 | 互斥锁（下）：如何用一把锁保护多个资源？用Account.class 作为共享的锁 Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Account { // lock for balance private final Object balLock = new Object(); // balance private Integer balance; // lock for password private final Object pwLock = new Object(); // password private String password; // withdraw money void withdraw(Integer amt) { synchronized (balLock) { if (this.balance &gt; amt) { this.balance -= amt; } } } // show balance Integer getBalance() { synchronized (balLock) { return balance; } } // change password void updatePassword(String pw) { synchronized (pwLock) { this.password = pw; } } // show password String getPassword() { synchronized (pwLock) { return this.password; } } // transfer void transfer(Account target, int amt) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } }} 在第一个示例程序里，我们用了两把不同的锁来分别保护账户余额、账户密码，创建锁的时候，我们用的是：private final Object xxxLock = new Object();，如果账户余额用 this.balance 作为互斥锁，账户密码用 this.password 作为互斥锁，你觉得是否可以呢？ 用this.balance 和this.password 都不行。在同一个账户多线程访问时候，A线程取款进行this.balance-=amt;时候此时this.balance对应的值已经发生变换，线程B再次取款时拿到的balance对应的值并不是A线程中的，也就是说不能把可变的对象当成一把锁。this.password 虽然说是String修饰但也会改变，所以也不行。 05 | 一不小心就死锁了，怎么办？如何预防死锁 互斥，共享资源 X 和 Y 只能被一个线程占用； 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等 待。 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。 1. 破坏占用且等待条件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // allocate resource in one time synchronized boolean apply(Object from, Object to) { if (als.contains(from) || als.contains(to)) { return false; } else { als.add(from); als.add(to); } return true; } // return resource synchronized void free(Object from, Object to) { als.remove(from); als.remove(to); }}class Account { // actr should be single_instance private Allocator actr; private int balance; // transfer void transfer(Account target, int amt) { // apply transfer from or to until success while (!actr.apply(this, target)) { try { synchronized (target) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } finally { actr.free(this, target); } } } // for 3 void transfer1(Account target, int amt) { Account left = this; Account right = target; if (this.id &gt; target.id) { left = target; right = this; } // lock small number synchronized (left) { // lock big number synchronized (right) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } }} 2. 破坏不可抢占条件3. 破坏循环等待条件课后题破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环while(!actr.apply(this, target));这个方法，那它比 synchronized(Account.class)有没有性能优势呢？ synchronized(Account.class) 锁了Account类相关的所有操作。相当于文中说的包场了，只要与Account有关联，通通需要等待当前线程操作完成。while死循环的方式只锁定了当前操作的两个相关的对象。两种影响到的范围不同。 06 | 用“等待-通知”机制优化循环等待一个更好地资源分配器12345678910111213141516171819202122232425public class Allocator { private List&lt;Object&gt; als; // apply all resource at once synchronized void apply(Object from, Object to) { // classic writing while (als.contains(from) || als.contains(to)) { try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } als.add(from); als.add(to); } // return resource synchronized void free(Object from, Object to) { als.remove(from); als.remove(to); notifyAll(); }} 尽量使用 notifyAll()notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。 课后题wait() 方法和 sleep() 方法都能让当前线程挂起一段时间，那它们的区别是什么？ wait与sleep区别在于： wait会释放所有锁而sleep不会释放锁资源. wait只能在同步方法和同步块中使用，而sleep任何地方都可以. wait无需捕捉异常，而sleep需要. 两者相同点：都会让渡CPU执行时间，等待再次调度！ 07 | 安全性、活跃性以及性能问题课后思考Java 语言提供的 Vector 是一个线程安全的容器，有同学写了下面的代码，你看看是否存在并发问题呢？ 1234567void addIfNotExist(Vector v,Object o){ if(!v.contains(o)) { v.add(o); }}// contains和add之间不是原子操作，有可能重复添加。// vector是线程安全，指的是它方法单独执行的时候没有并发正确性问题，并不代表把它的操作组合在一起问木有，而这个程序显然有竞态条件问题。 08 | 管程：并发编程的万能钥匙课后思考wait() 方法，在 Hasen 模型和 Hoare 模型里面，都是没有参数的，而在 MESA 模型里面，增加了超时参数，你觉得这个参数有必要吗？ 1.由于是while 循环，所以就算超时自动唤醒也会去重新检查条件，所以不存在逻辑错误问题2.假设另外一个线程在唤醒之前因为某些原因退出了，那带参数的notify可以超时而进去就绪状态。 hasen 是执行完，再去唤醒另外一个线程。能够保证线程的执行。hoare，是中断当前线程，唤醒另外一个线程，执行完再去唤醒，也能够保证完成。而mesa是进入等待队列，不一定有机会能够执行。 09 | Java线程（上）：Java线程的生命周期课后思考下面代码的本意是当前线程被中断之后，退出while(true)，你觉得这段代码是否正确呢？ 123456789101112Thread th = Thread.currentThread(); while (true) { if (th.isInterrupted()) { break; } // 省略业务代码无数 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } 123456789101112131415//可能出现无限循环，线程在sleep期间被打断了，抛出一个InterruptedException异常，try catch捕捉此异常，应该重置一下中断标示，因为抛出异常后，中断标示会自动清除掉！Thread th = Thread.currentThread(); while (true) { if (th.isInterrupted()) { break; } // 省略业务代码无数 try { Thread.sleep(100); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); } } 10 | Java线程（中）：创建多少线程才是合适的？对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的 对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式： 最佳线程数 =1 +（I/O 耗时 / CPU 耗时） 不过上面这个公式是针对单核 CPU 的，至于多核 CPU，也很简单，只需要等比扩大就可以了，计算公式如下： 最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）] 课后思考对于最佳线程数的设置积累了一些经验值，认为对于 I/O 密集型应用，最佳线程数应该为：2 * CPU 的核数 + 1，你觉得这个经验值合理吗？ bu 11| Java线程（下）：为什么局部变量是线程安全的？课后思考常听人说，递归调用太深，可能导致栈溢出。你思考一下原因是什么？有哪些解决方案呢？ 因为调用方法时局部变量会进线程的栈帧，线程的栈内存是有限的，而递归没控制好容易造成太多层次调用，最终栈溢出。解决思路一是开源节流，即减少多余的局部变量或扩大栈内存大小设置，减少调用层次涉及具体业务逻辑，优化空间有限；二是改弦更张，即想办法消除递归，比如说能否改造成尾递归（Java会优化掉尾递归） 12 | 如何用面向对象思想写好并发程序？一、封装共享变量二、识别共享变量间的约束条件三、制定并发访问策略 避免共享：避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程。 不变模式：这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor 模式、CSP 模式以及函数式编程的基础都是不变模式。 管程及其他同步工具：Java 领域万能的解决方案是管程，但是对于很多特定场景，使用 Java并发包提供的读写锁、并发容器等同步工具会更好。 除了这些方案之外，还有一些宏观的原则需要你了解。这些宏观原则，有助于你写出“健壮”的并发程序。这些原则主要有以下三条。 优先使用成熟的工具类：Java SDK 并发包里提供了丰富的工具类，基本上能满足你日常的需要，建议你熟悉它们，用好它们，而不是自己再“发明轮子”，毕竟并发工具类不是随随便便就能发明成功的。 迫不得已时才使用低级的同步原语：低级的同步原语主要指的是 synchronized、Lock、Semaphore 等，这些虽然感觉简单，但实际上并没那么简单，一定要小心使用。 避免过早优化：安全第一，并发程序首先要保证安全，出现性能瓶颈后再优化。在设计期和开发期，很多人经常会情不自禁地预估性能的瓶颈，并对此实施优化，但残酷的现实却是：性能瓶颈不是你想预估就能预估的。 12345678910111213141516171819202122232425262728public class SafeWM { // inventory ceiling private final AtomicLong upper = new AtomicLong(0); // stock low limit private final AtomicLong lower = new AtomicLong(0); // set stock high limit void setUpper(long v) { // check param in law if (v &lt; lower.get()) { throw new IllegalArgumentException(); } upper.set(v); } // set stock low limit void setLower(long v) { // check param in law if (v &gt; upper.get()) { throw new IllegalArgumentException(); } lower.set(v); } // omit other operation} 课后思考本期示例代码中，类 SafeWM 不满足库存下限要小于库存上限这个约束条件，那你来试试修改一下，让它能够在并发条件下满足库存下限要小于库存上限这个约束条件。 13 | 理论基础模块热点问题答疑14 | Lock&amp;Condition（上）：隐藏在并发包中的管程什么是可重入锁所谓可重入锁，顾名思义，指的是线程可以重复获取同一把锁。 所谓可重入函数，指的是多个线程可以同时调用该函数，每个线程都能得到正确结果 用锁的最佳实践 永远只在更新对象的成员变量时加锁 永远只在访问可变的成员变量时加锁 永远不在调用其他对象的方法时加锁 并发问题，本来就难以诊断，所以你一定要让你的代码尽量安全，尽量简单，哪怕有一点可能会出问题，都要努力避免。 课后思考你已经知道 tryLock() 支持非阻塞方式获取锁，下面这段关于转账的程序就使用到了 tryLock()，你来看看，它是否存在死锁问题呢？ 12345678910111213141516171819202122232425class Account { private int balance; private final Lock lock = new ReentrantLock(); // 转账 void transfer(Account tar, int amt) { while (true) { if (this.lock.tryLock()) { try { if (tar.lock.tryLock()) { try { this.balance -= amt; tar.balance += amt; } finally { tar.lock.unlock(); } }//if } finally { this.lock.unlock(); } }//if }//while }//transfer} 1234567891011121314151617181920212223242526272829class Account { private int balance; private final Lock lock = new ReentrantLock(); // 转账 void transfer(Account tar, int amt) { while (true) { if (this.lock.tryLock()) { try { if (tar.lock.tryLock()) { try { this.balance -= amt; tar.balance += amt;//新增：退出循环 break; } finally { tar.lock.unlock(); } }//if } finally { this.lock.unlock(); } }//if//新增：sleep⼀个随机时间避免活锁 Thread.sleep(随机时间); }//while }//transfer} 15 | Lock和Condition（下）：Dubbo如何用管程实现异步转同步？课后思考DefaultFuture 里面唤醒等待的线程，用的是 signal()，而不是 signalAll()，你来分析一下，这样做是否合理呢？ 每个rpc请求都会占用一个线程并产生一个新的DefaultFuture实例，它们的lock&amp;condition是不同的，并没有竞争关系这里的lock&amp;condition是用来做异步转同步的，使get()方法不必等待timeout那么久，用得很巧妙 12345678910// RPC结果返回时调⽤该⽅法private void doReceived(Response res) {lock.lock();try {response = res;done.signalAll();} finally {lock.unlock();}} 16 | Semaphore：如何快速实现一个限流器？Semaphore 可以允许多个线程访问一个临界区。 课后思考在上面对象池的例子中，对象保存在了 Vector 中，Vector 是 Java 提供的线程安全的容器，如果我们把 Vector 换成 ArrayList，是否可以呢？ 答案是不可以的。Semaphore可以允许多个线程访问一个临界区，那就意味着可能存在多个线程同时访问ArrayList，而ArrayList不是线程安全的，所以对象池的例子中是不能够将Vector换成ArrayList的。Semaphore允许多个线程访问一个临界区，这也是一把双刃剑，当多个线程进入临界区时，如果需要访问共享变量就会存在并发问题，所以必须加锁，也就是说Semaphore需要锁中锁。 17 | ReadWriteLock：如何快速实现一个完备的缓存？123456789101112131415161718192021222324252627282930//快速实现一个缓存public class Cache&lt;K, V&gt; { final Map&lt;K, V&gt; m = new HashMap&lt;&gt;(); final ReadWriteLock rwl = new ReentrantReadWriteLock(); // read lock final Lock r = rwl.readLock(); // write lock final Lock w = rwl.writeLock(); // read cache V get(K key) { r.lock(); try { return m.get(key); } finally { r.unlock(); } } // write cache V put(K key, V v) { w.lock(); try { return m.put(key, v); } finally { w.unlock(); } }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 实现缓存的按需加载public class Cache&lt;K, V&gt; { Object data; volatile boolean cacheValid; final Map&lt;K, V&gt; m = new HashMap&lt;&gt;(); final ReadWriteLock rwl = new ReentrantReadWriteLock(); final Lock r = rwl.readLock(); final Lock w = rwl.writeLock(); V get(K k) { V v = null; // read cache r.lock(); try { v = m.get(k); } finally { r.unlock(); } // cache exist,return if (v != null) { return v; } // cache doesn't exist,query database w.lock(); try { // check again // other thread may already query database v = m.get(k); if (v == null) { // query database v = v; m.put(k, v); } } finally { w.unlock(); } return v; } void processCachedData() { // earn read lock r.lock(); if (!cacheValid) { // release read lock because can't allow upgrade read lock r.unlock(); // earn write lock w.lock(); try { // check status again if (!cacheValid) { data = \"1\"; cacheValid = true; } // before release write lock,downgrade lock to read lock // downgrade is allow r.lock(); } finally { w.unlock(); } } // in this place,still own read lock try { // do something. use(data) } finally { r.unlock(); } }} 课后思考有同学反映线上系统停止响应了，CPU 利用率很低，你怀疑有同学一不小心写出了读锁升级写锁的方案，那你该如何验证自己的怀疑呢？ 18 | StampedLock：有没有比读写锁更快的锁？StampedLock 读模板：1234567891011121314151617final StampedLock sl = new StampedLock(); // 乐观读 long stamp = sl.tryOptimisticRead(); // 读⼊⽅法局部变量 ... // 校验 stamp if (!sl.validate(stamp)) { // 升级为悲观读锁 stamp = sl.readLock(); try { // 读⼊⽅法局部变量 ... } finally { // 释放悲观读锁 sl.unlockRead(stamp); } } // 使⽤⽅法局部变量执⾏业务操作 .. StampedLock 写模板：1234567long stamp = sl.writeLock();try {// 写共享变量 ......} finally { sl.unlockWrite(stamp);} 课后思考StampedLock 支持锁的降级（通过 tryConvertToReadLock() 方法实现）和升级（通过tryConvertToWriteLock() 方法实现），但是建议你要慎重使用。下面的代码也源自 Java 的官方示例，我仅仅做了一点修改，隐藏了一个 Bug，你来看看 Bug 出在哪里吧。 12345678910111213141516171819202122232425private double x, y;final StampedLock sl = new StampedLock();// 存在问题的⽅法void moveIfAtOrigin(double newX, double newY) { long stamp = sl.readLock(); try { while (x == 0.0 &amp;&amp; y == 0.0) { long ws = sl.tryConvertToWriteLock(stamp); if (ws != 0L) { //问题出在没有对stamp重新赋值//新增下⾯⼀⾏ stamp = ws; x = newX; y = newY; break; } else { sl.unlockRead(stamp); stamp = sl.writeLock(); } } } finally { sl.unlock(stamp); }} bug是tryConvertToWriteLock返回的write stamp没有重新赋值给stamp 19 | CountDownLatch和CyclicBarrier：如何让多线程步调一致？123456789101112131415161718192021222324252627282930313233// CountDownLatchpublic class CountDownLatchTest { public static void main(String[] args) throws InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(2); while (true) { // count init 2 CountDownLatch latch = new CountDownLatch(2); // query 1 executorService.execute(() -&gt; { // do somethings latch.countDown(); }); // query 2 executorService.execute(() -&gt; { latch.countDown(); }); latch.await(); boolean check = check(); save(); } } private static void save() { } private static boolean check() { // check something return true; }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// CyclicBarrierpublic class CyclicBarrierTest { // queue 1 static Vector&lt;String&gt; pos; // queue 2 static Vector&lt;String&gt; dos; // threadpool static Executor executor = Executors.newFixedThreadPool(1); final static CyclicBarrier barrier = new CyclicBarrier(2, () -&gt; { executor.execute(() -&gt; check()); }); public static void main(String[] args) { } private static void check() { String p = pos.remove(0); String d = dos.remove(0); // check(p, d); save(); } private static void save() { } private static void check(String p, String d) { } void checkAll() { Thread t1 = new Thread(() -&gt; { while (true) { // exist pos.add(\"\"); // try { barrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } }); t1.start(); Thread t2 = new Thread(() -&gt; { while (true) { // exist dos.add(\"\"); // try { barrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } }); t2.start(); }} CyclicBarrier 的计数器有自动重置的功能，当减到 0 的时候，会自动重置你设置的初始值。 课后思考本章最后的示例代码中，CyclicBarrier 的回调函数我们使用了一个固定大小的线程池，你觉得是否有必要呢？ 设置线程池为单个线程可以保证对账的操作按顺序执行 其实线程池改成多线程也可以，要把两个remove(0)放到一个同步块中 第二个是使用了线程池，如果不使用，直接在回调函数里调用check()方法是否可以呢？绝对不可以。为什么呢？这个要分析一下回调函数和唤醒等待线程之间的关系。下面是CyclicBarrier相关的源码，通过源码你会发现CyclicBarrier是同步调用回调函数之后才唤醒等待的线程，如果我们在回调函数里直接调用check()方法，那就意味着在执行check()的时候，是不能同时执行getPOrders()和getDOrders()的，这样就起不到提升性能的作用。 1234567891011121314try {//barrierCommand是回调函数final Runnable command = barrierCommand;//调⽤回调函数if (command != null)command.run();ranAction = true;//唤醒等待的线程nextGeneration();return 0;} finally {if (!ranAction)breakBarrier();} 所以，当遇到回调函数的时候，你应该本能地问自己：执行回调函数的线程是哪一个？这个在多线程场景下非常重要。因为不同线程ThreadLocal里的数据是不同的，有些框架比如Spring就用ThreadLocal来管理事务，如果不清楚回调函数用的是哪个线程，很可能会导致错误的事务管理，并最终导致数据不一致。 CyclicBarrier的回调函数究竟是哪个线程执行的呢？如果你分析源码，你会发现执行回调函数的线程是将CyclicBarrier内部计数器减到 0 的那个线程。所以我们前面讲执行check()的时候，是不能同时执行getPOrders()和getDOrders()，因为执行这两个方法的线程一个在等待，一个正在忙着执行check()。 再次强调一下：当看到回调函数的时候，一定问一问执行回调函数的线程是谁。 20 | 并发容器：都有哪些“坑”需要我们填？（一）ListCopyOnWriteArrayList 使用 CopyOnWriteArrayList 需要注意的“坑”主要有两个方面。一个是应用场景，CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致。写入的新元素并不能立刻被遍历到。另一个需要注意的是，CopyOnWriteArrayList迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的。 （二）MapConcurrentHashMap 和 ConcurrentSkipListMap ConcurrentHashMap 的 key 是无序的 ConcurrentSkipListMap 的 key 是有序的 使用 ConcurrentHashMap 和 ConcurrentSkipListMap 需要注意的地方是，它们的 key 和 value都不能为空，否则会抛出NullPointerException （三）SetCopyOnWriteArraySet 和 ConcurrentSkipListSet （四）Queue1.单端阻塞队列 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、LinkedTransferQueue、PriorityBlockingQueue 和 DelayQueue。 2.双端阻塞队列：其实现是 LinkedBlockingDeque。 3.单端非阻塞队列：其实现是 ConcurrentLinkedQueue。 4.双端非阻塞队列：其实现是 ConcurrentLinkedDeque。 需要格外注意队列是否支持有界 在使用其他无界队列时，一定要充分考虑是否存在导致 OOM 的隐患。 课后思考线上系统 CPU 突然飙升，你怀疑有同学在并发场景里使用了 HashMap，因为在 1.8 之前的版本里并发执行 HashMap.put() 可能会导致 CPU 飙升到 100%，你觉得该如何验证你的猜测呢？ Java7中的HashMap在执行put操作时会涉及到扩容，由于扩容时链表并发操作会造成链表成环，所以可能导致cpu飙升100%。 21 | 原子类：无锁工具类的典范无锁方案的实现原理 其实原子类性能高的秘密很简单，硬件支持而已。CPU 为了解决并发问题，提供了 CAS 指令（CAS，全称是 Compare And Swap，即“比较并交换”）。CAS 指令包含 3 个参数：共享变量的内存地址 A、用于比较的值 B 和共享变量的新值 C；并且只有当内存中地址 A 处的值等于 B时，才能将内存中地址 A 处的值更新为新值 C。作为一条 CPU 指令，CAS 指令本身是能够保证原子性的。 1. 原子化的基本数据类型2. 原子化的对象引用类型不过需要注意的是，对象引用的更新需要重点关注 ABA 问题，AtomicStampedReference 和 AtomicMarkableReference 这两个原子类可以解决 ABA 问题。 3. 原子化数组4. 原子化对象属性更新器5. 原子化的累加器课后思考下面的示例代码是合理库存的原子化实现，仅实现了设置库存上限 setUpper() 方法，你觉得setUpper() 方法的实现是否正确呢？ 1234567891011121314151617181920212223242526272829public class SafeWM { class WMRange { final int upper; final int lower; WMRange(int upper, int lower) { // 省略构造函数实现 } } final AtomicReference&lt;WMRange&gt; rf = new AtomicReference&lt;&gt;( new WMRange(0, 0) ); // 设置库存上限 void setUpper(int v) { WMRange nr; WMRange or = rf.get(); do { // 检查参数合法性 if (v &lt; or.lower) { throw new IllegalArgumentException(); } nr = new WMRange(v, or.lower); } while (!rf.compareAndSet(or, nr)); }} or是原始的 nr是new出来的 指向不同的内存地址 compareandset的结果永远返回false 结果是死循环？ 12345678910111213141516171819202122232425262728293031public class SafeWM {class WMRange{final int upper;final int lower;WMRange(int upper,int lower){//省略构造函数实现}}final AtomicReference&lt;WMRange&gt;rf = new AtomicReference&lt;&gt;(new WMRange(0,0));// 设置库存上限void setUpper(int v){WMRange nr;WMRange or;//原代码在这⾥//WMRange or=rf.get();do{//移动到此处//每个回合都需要重新获取旧值or = rf.get();// 检查参数合法性if(v &lt; or.lower){throw new IllegalArgumentException();}nr = newWMRange(v, or.lower);}while(!rf.compareAndSet(or, nr));}} 22-Executor与线程池：如何创建正确的线程池？1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//简化的线程池，仅⽤来说明⼯作原理public class MyThreadPool { // BlockingQueue&lt;Runnable&gt; workQueue; // List&lt;WorkerThread&gt; threads = new ArrayList&lt;&gt;(); // public MyThreadPool(int poolSize, BlockingQueue&lt;Runnable&gt; workQueue) { this.workQueue = workQueue; // for (int i = 0; i &lt; poolSize; i++) { WorkerThread workerThread = new WorkerThread(); workerThread.start(); threads.add(workerThread); } } void execute(Runnable command) throws InterruptedException { workQueue.put(command); } // workthread class WorkerThread extends Thread { @SneakyThrows @Override public void run() { // while (true) { Runnable task = workQueue.take(); task.run(); } } } public static void main(String[] args) throws InterruptedException { // BlockingQueue&lt;Runnable&gt; workQueue = new LinkedBlockingQueue&lt;&gt;(2); // MyThreadPool myThreadPool = new MyThreadPool(10, workQueue); myThreadPool.execute(() -&gt; { System.out.println(\"fuck\"); }); }} 课后思考使用线程池，默认情况下创建的线程名字都类似pool-1-thread-2这样，没有业务含义。而很多情况下为了便于诊断问题，都需要给线程赋予一个有意义的名字，那你知道有哪些办法可以给线程池里的线程指定名字吗？ 23-Future：如何用多线程实现最优的“烧水泡茶”程序？那如何使用FutureTask呢？其实很简单，FutureTask实现了Runnable和Future接口，由于实现了Runnable接口，所以可以将FutureTask对象作为任务提交给ThreadPoolExecutor去执行，也可以直接被Thread执行；又因为实现了Future接口，所以也能用来获得任务的执行结果。下面的示例代码是将FutureTask对象提交给ThreadPoolExecutor去执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Slf4jpublic class PaoCha { public static void main(String[] args) throws ExecutionException, InterruptedException {// BasicConfigurator.configure(); } @Test public void name() throws ExecutionException, InterruptedException { FutureTask&lt;String&gt; ft2 = new FutureTask&lt;&gt;(new T2Task()); FutureTask&lt;String&gt; ft1 = new FutureTask&lt;&gt;(new T1Task(ft2)); Thread t1 = new Thread(ft1); t1.start(); Thread t2 = new Thread(ft2); t2.start(); log.info(\"fuck\"); log.info(ft1.get()); } static class T1Task implements Callable&lt;String&gt; { FutureTask&lt;String&gt; ft2; T1Task(FutureTask&lt;String&gt; ft2) { this.ft2 = ft2; } @Override public String call() throws Exception { log.info(\"T2.洗水壶...\"); TimeUnit.SECONDS.sleep(1); log.info(\"T2.烧开水...\"); TimeUnit.SECONDS.sleep(1); // 获取T2线程的茶叶 String tf = ft2.get(); log.info(\"T1.拿到茶叶...{}\", tf); log.info(\"T1 泡茶\"); TimeUnit.SECONDS.sleep(1); return \"上茶\" + tf; } } static class T2Task implements Callable&lt;String&gt; { @Override public String call() throws Exception { log.info(\"T2.洗茶壶...\"); TimeUnit.SECONDS.sleep(1); log.info(\"T2.洗茶杯...\"); TimeUnit.SECONDS.sleep(1); log.info(\"T2.拿茶叶...\"); TimeUnit.SECONDS.sleep(1); return \"龙井\"; } }} 课后思考不久前听说小明要做一个询价应用，这个应用需要从三个电商询价，然后保存在自己的数据库里。核心示例代码如下所示，由于是串行的，所以性能很慢，你来试着优化一下吧。 1234567// 向电商S1询价，并保存r1 = getPriceByS1();save(r1);// 向电商S2询价，并保存r2 = getPriceByS2();// 向电商S3询价，并保存r3 = getPriceByS3(); 现在是在主线程串行完成3个询价的任务，执行第一个任务，其它2个任务只能等待执行，如果要提高效率，这个地方需要改进，可以用老师今天讲的futuretask，三个询价任务改成futuretask并行执行，效率会提高 24-CompletableFuture：异步编程没那么难CompletableFuture的核心优势 分工、协作和互斥 123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class PaoCha { @Test public void Test1() { // task 1 spoil CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(() -&gt; { log.info(\"T2.洗水壶...\"); sleep(1, TimeUnit.SECONDS); log.info(\"T2.烧开水...\"); sleep(1, TimeUnit.SECONDS); }); // task 2 get tea CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(() -&gt; { log.info(\"T2.洗茶壶...\"); sleep(1,TimeUnit.SECONDS); log.info(\"T2.洗茶杯...\"); sleep(1,TimeUnit.SECONDS); log.info(\"T2.拿茶叶...\"); sleep(1,TimeUnit.SECONDS); return \"龙井\"; }); // task 3 after 1 and 2 spoil tea CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (unused, tf) -&gt; { log.info(\"T1,拿到茶叶{}\", tf); log.info(\"T1.泡茶\"); return \"上茶:\" + tf; }); log.info(f3.join()); } void sleep(int t, TimeUnit u) { try { u.sleep(t); }catch(InterruptedException e){} }} 根据不同的业务类型创建不同的线程池，以避免互相干扰。 如何理解CompletionStage接口1. 描述串行关系thenApply、thenAccept、thenRun和thenCompose 123456789@Testpublic void Test2() { CompletableFuture&lt;String&gt; f0 = CompletableFuture.supplyAsync(() -&gt; { return \"Hello World\"; }) .thenApply(s -&gt; s + \"QQ\") .thenApply(String::toUpperCase); log.info(f0.join());} 2. 描述AND汇聚关系3. 描述OR汇聚关系1234567891011121314151617181920212223@Testpublic void Test3() { CompletableFuture&lt;String&gt; f1 = CompletableFuture.supplyAsync(() -&gt; { int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t); }); CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(() -&gt; { int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t); }); CompletableFuture&lt;String&gt; f3 = f1.applyToEither(f2, s -&gt; s); log.info(f3.join());}private int getRandom(int i, int i1) { ThreadLocalRandom random1 = RandomUtil.getRandom(); return random1.nextInt(i, i1);} 4. 异常处理课后思考创建采购订单的时候，需要校验一些规则，例如最大金额是和采购员级别相关的。有同学利用CompletableFuture实现了这个校验的功能，逻辑很简单，首先是从数据库中把相关规则查出来，然后执行规则校验。你觉得他的实现是否有问题呢？ 1234567891011//采购订单PurchersOrder po;CompletableFuture&lt;Boolean&gt; cf =CompletableFuture.supplyAsync(()-&gt;{//在数据库中查询规则return findRuleByJdbc();}).thenApply(r -&gt; {//规则校验return check(po, r);});Boolean isOk = cf.join(); findRuleByJdbc()这个方法隐藏着一个阻塞式I/O，这意味着会阻塞调用线程。默认情况下所有的CompletableFuture共享一个ForkJoinPool，当有阻塞式I/O时，可能导致所有的ForkJoinPool线程都阻塞，进而影响整个系统的性能。 利用共享，往往能让我们快速实现功能，所谓是有福同享，但是代价就是有难要同当。在强调高可用的今天，大多数人更倾向于使用隔离的方案。 25-CompletionService：如何批量执行异步任务？课后思考本章使用CompletionService实现了一个询价应用的核心功能，后来又有了新的需求，需要计算出最低报价并返回，下面的示例代码尝试实现这个需求，你看看是否存在问题呢？ 123456789101112131415161718192021222324252627// 创建线程池ExecutorService executor =Executors.newFixedThreadPool(3);// 创建CompletionServiceCompletionService&lt;Integer&gt; cs = newExecutorCompletionService&lt;&gt;(executor);// 异步向电商S1询价cs.submit(()-&gt;getPriceByS1());// 异步向电商S2询价cs.submit(()-&gt;getPriceByS2());// 异步向电商S3询价cs.submit(()-&gt;getPriceByS3());// 将询价结果异步保存到数据库// 并计算最低报价AtomicReference&lt;Integer&gt; m =new AtomicReference&lt;&gt;(Integer.MAX_VALUE);for (int i=0; i&lt;3; i++) {executor.execute(()-&gt;{Integer r = null;try {r = cs.take().get();} catch (Exception e) {}save(r);m.set(Integer.min(m.get(), r));});}return m; 1.AtomicReference的get方法应该改成使用cas方法 2.最后筛选最小结果的任务是异步执行的，应该在return之前做同步，所以最好使用sumit提交该任务便于判断任务的完成 26-ForkJoin：单机版的MapReduce对于简单的并行任务，你可以通过“线程池+Future”的方案来解决；如果任务之间有聚合关系，无论是AND聚合还是OR聚合，都可以通过CompletableFuture来解决；而批量的并行任务，则可以通过CompletionService来解决。 分治任务模型Fork/Join的使用1234567891011121314151617181920212223242526272829303132333435@Slf4jpublic class ForkJoinTest { public static void main(String[] args) { // create Divide and conquer task thread pool ForkJoinPool fjp = new ForkJoinPool(4); // create Divide and conquer task Fibonacci fib = new Fibonacci(30); // start task Integer result = fjp.invoke(fib); log.info(String.valueOf(result)); } static class Fibonacci extends RecursiveTask&lt;Integer&gt; { final int n; public Fibonacci(int n) { this.n = n; } @Override protected Integer compute() { if (n &lt;= 1) { return n; } Fibonacci f1 = new Fibonacci(n - 1); // create child task f1.fork(); Fibonacci f2 = new Fibonacci(n - 2); // wait the result of child task ,and combine result return f2.compute() + f1.join(); } }} 模拟MapReduce统计单词数量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Slf4jpublic class CountWord { public static void main(String[] args) { String[] fc = {\"hello world\", \"hello me\", \"hello fork\", \"hello join\", \"fork join in world\"}; // create fork join thread pool ForkJoinPool fjp = new ForkJoinPool(3); // create task MR mr = new MR(fc, 0, fc.length); // start task Map&lt;String, Long&gt; result = fjp.invoke(mr); // sout result result.forEach((k, v) -&gt; { log.info(\"{}:{}\",k, v); }); } static class MR extends RecursiveTask&lt;Map&lt;String, Long&gt;&gt; { private String[] fc; private int start, end; public MR(String[] fc, int start, int end) { this.fc = fc; this.start = start; this.end = end; } @Override protected Map&lt;String, Long&gt; compute() { if (end - start == 1) { return calc(fc[start]); } else { int mid = (start + end) / 2; MR mr1 = new MR(fc, start, mid); mr1.fork(); MR mr2 = new MR(fc, mid, end); // compute child task and return combine count return merge(mr2.compute(), mr1.join()); } } private Map&lt;String, Long&gt; merge(Map&lt;String, Long&gt; r1, Map&lt;String, Long&gt; r2) { Map&lt;String, Long&gt; result = new HashMap&lt;&gt;(r1); // merge result r2.forEach((k, v) -&gt; result.merge(k, v, Long::sum)); return result; } private Map&lt;String, Long&gt; calc(String line) { Map&lt;String, Long&gt; result = new HashMap&lt;&gt;(); // divide word String[] words = line.split(\"\\\\s+\"); // count words for (String word : words) { Long v = result.get(word); if (v != null) { result.put(word, v + 1); } else { result.put(word, 1L); } } return result; } }} 总结Fork/Join并行计算框架主要解决的是分治任务。分治的核心思想是“分而治之”：将一个大的任务拆分成小的子任务去解决，然后再把子任务的结果聚合起来从而得到最终结果。这个过程非常类似于大数据处理中的MapReduce，所以你可以把Fork/Join看作单机版的MapReduce。 Fork/Join并行计算框架的核心组件是ForkJoinPool。ForkJoinPool支持任务窃取机制，能够让所有线程的工作量基本均衡，不会出现有的线程很忙，而有的线程很闲的状况，所以性能很好。Java 1.8提供的StreamAPI里面并行流也是以ForkJoinPool为基础的。不过需要你注意的是，默认情况下所有的并行流计算都共享一个ForkJoinPool，这个共享的ForkJoinPool默认的线程数是CPU的核数；如果所有的并行流计算都是CPU密集型计算的话，完全没有问题，但是如果存在I/O密集型的并行流计算，那么很可能会因为一个很慢的I/O计算而拖慢整个系统的性能。所以建议用不同的ForkJoinPool执行不同类型的计算任务。 27-并发工具类模块热点问题答疑","link":"/2021/09/15/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"db2","slug":"db2","link":"/tags/db2/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"vue","slug":"vue","link":"/tags/vue/"},{"name":"githubpage","slug":"githubpage","link":"/tags/githubpage/"},{"name":"eladmin","slug":"eladmin","link":"/tags/eladmin/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"小学生作文-我的生活","slug":"小学生作文-我的生活","link":"/tags/%E5%B0%8F%E5%AD%A6%E7%94%9F%E4%BD%9C%E6%96%87-%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/tags/SpringCloud/"},{"name":"es","slug":"es","link":"/tags/es/"},{"name":"Java并发","slug":"Java并发","link":"/tags/Java%E5%B9%B6%E5%8F%91/"}],"categories":[{"name":"db2","slug":"db2","link":"/categories/db2/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/categories/DeepLearning/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"vue","slug":"vue","link":"/categories/vue/"},{"name":"githubpage","slug":"githubpage","link":"/categories/githubpage/"},{"name":"eladmin","slug":"eladmin","link":"/categories/eladmin/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"小学生作文-我的生活","slug":"小学生作文-我的生活","link":"/categories/%E5%B0%8F%E5%AD%A6%E7%94%9F%E4%BD%9C%E6%96%87-%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/categories/SpringCloud/"},{"name":"es","slug":"es","link":"/categories/es/"},{"name":"Java并发","slug":"Java并发","link":"/categories/Java%E5%B9%B6%E5%8F%91/"}]}